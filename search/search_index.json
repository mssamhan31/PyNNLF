{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to PyNNLF","text":"<p>PyNNLF (Python for Network Net Load Forecast) is a tool to evaluate net load forecasting model performance in a reliable and reproducible way.</p> <p>You can access the GitHub repository here.</p>"},{"location":"#objective","title":"Objective","text":"<p>This tool evaluates net load forecasting models reliably and reproducibly. It includes a library of public net load datasets and common forecasting models, including simple benchmark models. Users input the forecast problem and model specification, and the tool outputs evaluation results. </p> <p>It also allows users to add datasets, models, and modify hyperparameters. Researchers claiming a new or superior model can compare their model with existing ones on public datasets. The target audience includes researchers in academia or industry focused on evaluating and optimizing net load forecasting models. </p> <p>A visual illustration of the tool workflow is shown below. </p>"},{"location":"#input","title":"Input","text":"<ol> <li>Forecast Target: dataset &amp; forecast horizon. List of possible forecast problem values is in <code>notebooks/config/config.ipynb</code>.</li> <li>Model Specification: model &amp; hyperparameters. List of possible model specifications is in <code>notebooks/config/model_hyperparameters.ipynb</code>.</li> </ol>"},{"location":"#output","title":"Output","text":"<ol> <li><code>a1_experiment_result.csv</code> \u2013 contains accuracy (cross-validated test n-RMSE), stability (accuracy stddev), and training time.</li> <li><code>a2_hyperparameter.csv</code> \u2013 lists hyperparameters used for each model.</li> <li><code>a3_cross_validation_result.csv</code> \u2013 detailed results for each cross-validation split.</li> <li><code>cv_plots/</code> \u2013 folder with plots including:</li> <li>Observation vs forecast (time plot)</li> <li>Observation vs forecast (scatter plot)</li> <li>Residual time plot</li> <li>Residual histogram</li> <li><code>cv_test/</code> and <code>cv_train/</code> \u2013 folders containing time series of observation, forecast, and residuals for each cross-validation split.</li> </ol>"},{"location":"#tool-output-naming-convention","title":"Tool Output Naming Convention","text":"<p>Format: <code>[experiment_no]_[experiment_date]_[dataset]_[forecast_horizon]_[model]_[hyperparameter]</code></p> <p>Example: <code>E00001_250915_ds0_fh30_m6_lr_hp1</code></p>"},{"location":"ack_license/","title":"Acknowledgements","text":"<p>This project is part of Samhan's PhD study, supported by the University International Postgraduate Award (UIPA) Scholarship from UNSW, the Industry Collaboration Project Scholarship from Ausgrid, and the RACE for 2030 Industry PhD Scholarship. We also acknowledge Solcast and the Australian Bureau of Meteorology (BOM) for providing access to historical weather datasets for this research. We further acknowledge the use of Python libraries including Pandas, NumPy, PyTorch, Scikit-learn, XGBoost, Prophet, Statsmodels, and Matplotlib. Finally, we thank the reviewers and editor of the Journal of Open Source Software for their valuable feedback and guidance.</p>"},{"location":"ack_license/#potential-conflict-disclosure","title":"Potential Conflict Disclosure","text":"<p>The authors declare that they have no competing financial, personal, or professional interests related to this work.</p>"},{"location":"ack_license/#license","title":"License","text":"<p>MIT License.</p>"},{"location":"add_dataset/","title":"How to add a dataset","text":""},{"location":"add_dataset/#dataset-format","title":"Dataset Format","text":"<p>All datasets are stored in the <code>data/</code> folder in <code>.csv</code> format. Each file is named using the pattern <code>[dataset_id]_[dataset_name].csv</code>, e.g., <code>ds4_ashd_with_weather.csv</code>.</p> <p>Some datasets may share the same net load data but differ in the availability of exogenous variables. For instance, <code>ds1_ashd.csv</code> is equivalent to <code>ds4_ashd_with_weather.csv</code> but without weather data.</p> <p>Each CSV file must include two required columns: <code>datetime</code> and <code>netload_kW</code>. PyNNLF uses <code>netload_kW</code> as the target variable for forecasting, and automatically generates lag features based on it.</p> <p>Any additional columns are treated as exogenous variables. These are also processed into lag features based on the forecast horizon, but are not used as targets.</p> <p>Calendar features are excluded from the CSV files, as PyNNLF generates them dynamically during each experiment.</p>"},{"location":"add_dataset/#how-to-add-a-dataset","title":"How to Add a Dataset","text":"<p>To add a new dataset, simply create a <code>.csv</code> file in the <code>data/</code> folder following the naming convention above. Make sure to update <code>data/metadata.xlsx</code> to document the new dataset.</p>"},{"location":"add_model/","title":"How to add a model","text":""},{"location":"add_model/#model-structure","title":"Model Structure","text":"<p>All models are stored in the <code>model</code> folder. Each model has a dedicated <code>.ipynb</code> file, making it easy to navigate and understand its implementation.</p> <p>Each model typically includes two functions: one for training and one for generating forecasts. Tasks such as train-test splitting, 10-fold cross-validation, evaluation, and plotting are handled in <code>general_functions.ipynb</code>, keeping model files focused solely on model-specific code.</p>"},{"location":"add_model/#training-function","title":"Training Function","text":"<p>The training function trains the model using the training set predictors and target values. It returns a single object, <code>model</code>, which contains the trained model.</p>"},{"location":"add_model/#testing-function","title":"Testing Function","text":"<p>The testing function uses the trained model along with the training and testing predictors. It returns two DataFrames: forecasted values for both the training and testing sets.</p> <p>Evaluation and plotting are managed by <code>notebooks/config/general_functions.ipynb</code>.</p>"},{"location":"add_model/#how-to-add-a-model","title":"How to Add a Model","text":"<p>To add a new model, follow these three steps:</p>"},{"location":"add_model/#1-create-a-new-model-file-in-the-notebooksmodel-folder","title":"1. Create a new model file in the <code>notebooks/model/</code> folder","text":"<p>For example, to add a model named <code>new_model</code>, create a file called <code>m19_new_model.ipynb</code>. Define the training and testing functions in this file, named <code>train_model_m19_new_model</code> and <code>produce_forecast_model_m19_new_model</code>.</p> <p>You can refer to existing models for examples, such as the ANN model:</p> <pre><code>def train_model_m7_ann(hyperparameter, train_df_X, train_df_y):\n    ''' Train and test an ANN model for point forecasting. \n\n    Args:\n        hyperparameter (df) : hyperparameter value of the model consisting of number of features\n        train_df_X (df) : features matrix for training\n        train_df_y (df) : target matrix for training\n\n\n    Returns:\n        model (model) : trained model with all features\n    '''\n\n    #UNPACK HYPERPARAMETER\n\n    # Set random seed for reproducibility\n    def set_seed(seed):\n        random.seed(seed)\n        os.environ[\"PYTHONHASHSEED\"] = str(seed)\n        np.random.seed(seed)\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed(seed)\n        torch.backends.cudnn.deterministic = True\n\n    seed = int(hyperparameter['seed'])\n\n    hidden_size = hyperparameter['hidden_size']\n    activation_function = hyperparameter['activation_function']\n    learning_rate = hyperparameter['learning_rate']\n    # learning_rate = 0.001\n    solver = hyperparameter['solver']\n    epochs = hyperparameter['epochs']\n\n    # Use proper format for X and y\n    X = torch.tensor(train_df_X.values, dtype=torch.float32)\n    y = torch.tensor(train_df_y.values, dtype=torch.float32).view(-1, 1) \n\n    # Define the ANN model\n    class ANNModel(nn.Module):\n        def __init__(self, input_size, hidden_size, output_size):\n            super(ANNModel, self).__init__()\n            self.fc1 = nn.Linear(input_size, hidden_size)\n            self.fc2 = nn.Linear(hidden_size, output_size)\n            self.relu = nn.ReLU()  # Activation function\n\n        def forward(self, x):\n            x = self.fc1(x)\n            if activation_function == 'relu':\n                x = self.relu(x)\n            elif activation_function == 'sigmoid':\n                x = torch.sigmoid(x)\n            else:\n                x = torch.tanh(x)\n            x = self.fc2(x)\n            return x\n\n    # Model initialization\n    input_size = X.shape[1]\n    output_size = y.shape[1]\n\n    set_seed(seed)\n\n    model_ann = ANNModel(input_size, hidden_size, output_size)\n    if solver == 'adam':\n        optimizer = optim.Adam(model_ann.parameters(), lr=learning_rate)\n    elif solver == 'sgd':\n        optimizer = optim.SGD(model_ann.parameters(), lr=learning_rate)\n    else:\n        raise ValueError('Solver not found')\n\n    # Loss function\n    criterion = nn.MSELoss()  # Mean Squared Error loss for regression\n\n    #TRAIN MODEL\n    # Training loop\n    for epoch in range(epochs):\n        model_ann.train()\n\n        # Forward pass\n        output = model_ann(X)\n        loss = criterion(output, y)\n\n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n\n        # Update weights\n        optimizer.step()\n\n        if epoch % 10 == 0:\n            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n\n    # PACK MODEL\n    model = {\"model_ann\": model_ann}\n\n\n    return model\n</code></pre> <pre><code>def produce_forecast_m7_ann(model, train_df_X, test_df_X):\n    \"\"\"Create forecast at the train and test set using the trained model\n\n    Args:\n        model (dictionary): all parameters of the trained model\n        train_df_X (df): predictors of train set\n        test_df_X (df): predictors of test set\n\n    Returns:\n        train_df_y_hat (df) : forecast result at train set\n        test_df_y_hat (df) : forecast result at test set\n\n    \"\"\"\n\n    # UNPACK MODEL\n    model_ann = model[\"model_ann\"]\n\n    # PREPARE FORMAT\n    train_df_X_tensor = torch.tensor(train_df_X.values, dtype=torch.float32)\n    test_df_X_tensor = torch.tensor(test_df_X.values, dtype=torch.float32)\n\n    # PRODUCE FORECAST\n    # Switch model to evaluation mode for inference\n    model_ann.eval()\n\n    # TRAIN SET FORECAST\n    with torch.no_grad():  # Disable gradient calculation to save memory\n        train_df_y_hat_tensor = model_ann(train_df_X_tensor)\n\n    # TEST SET FORECAST\n    with torch.no_grad():  # Disable gradient calculation to save memory\n        test_df_y_hat_tensor = model_ann(test_df_X_tensor)\n\n    # Create DataFrames of result\n    train_df_y_hat = pd.DataFrame(train_df_y_hat_tensor, index=train_df_X.index, columns=['y_hat'])\n    test_df_y_hat = pd.DataFrame(test_df_y_hat_tensor, index=test_df_X.index, columns=['y_hat'])\n\n    return train_df_y_hat, test_df_y_hat\n</code></pre> <p>You can add any model you\u2019ve developed or proposed, as long as it can be trained using the training set features and target values. The next steps are straightforward.</p>"},{"location":"add_model/#2-update-train_model-function-in-notebooksconfiggeneral_functionsipynb-file","title":"2. Update <code>train_model</code> function in <code>notebooks/config/general_functions.ipynb</code> file","text":"<p>This file contains utility functions, including train_model, which dispatches training based on the selected model.</p> <p>Add a new condition like:</p> <pre><code>elif model_name == 'm19_new_model':\n        model = train_model_m19_new_model(hyperparameter, train_df_X, train_df_y)\n</code></pre>"},{"location":"add_model/#3-update-produce_forecast-function-in-notebooksconfiggeneral_functionsipynb-file","title":"3. Update <code>produce_forecast</code> function in <code>notebooks/config/general_functions.ipynb</code> file","text":"<p>Similarly, update the <code>produce_forecast</code> function by adding:</p> <pre><code>elif model_name == 'm19_new_model':\n        train_df_y_hat, test_df_y_hat = produce_forecast_m19_new_model(model, train_df_X, test_df_X)\n</code></pre>"},{"location":"api_reference/","title":"API Reference","text":""},{"location":"api_reference/#folder-config","title":"Folder <code>Config/</code>","text":""},{"location":"api_reference/#docs.notebooks.config.general_functions.add_lag_features","title":"<code>add_lag_features(df, forecast_horizon, max_lag_day)</code>","text":"<p>Adds a lagged column to the dataframe based on the given horizon in minutes and max lag in days.</p> <p>Args: df (pd.DataFrame): The input dataframe with a datetime index and a column 'y'. forecast_horizon (int): The horizon in minutes for the lag. max_lag_day (int): the number of days until the longest lag</p> <p>Returns: pd.DataFrame: The dataframe with additional columns for the lags.</p> Source code in <code>docs\\notebooks\\config\\general_functions.py</code> <pre><code>def add_lag_features(df, forecast_horizon, max_lag_day):\n    \"\"\"\n    Adds a lagged column to the dataframe based on the given horizon in minutes and max lag in days.\n\n    Args:\n    df (pd.DataFrame): The input dataframe with a datetime index and a column 'y'.\n    forecast_horizon (int): The horizon in minutes for the lag.\n    max_lag_day (int): the number of days until the longest lag\n\n    Returns:\n    pd.DataFrame: The dataframe with additional columns for the lags.\n    \"\"\"\n\n    # Convert the horizon to a timedelta object\n    horizon_timedelta = pd.Timedelta(minutes=forecast_horizon)\n    consecutive_timedelta = df.index[1] - df.index[0]\n\n    # Calculate the number of new columns\n    n_new_cols = len(df[df.index &lt; df.index[0] + pd.DateOffset(days=max_lag_day)])\n\n    # List to hold all the new lagged columns\n    new_cols = []\n\n    # Generate lagged columns based on the horizon and max lag\n\n    #Generate lagged columns not only based on net load but also based on weather data if available\n    for column in df.columns:\n    # Generate lagged columns for the current column\n        for i in range(n_new_cols):\n            shift_timedelta = horizon_timedelta + i * consecutive_timedelta\n            new_col_name = f'{column}_lag_{shift_timedelta}m'\n            new_cols.append(df[column].shift(freq=shift_timedelta).rename(new_col_name))\n\n\n    # Concatenate the new lagged columns with the original dataframe\n    df = pd.concat([df] + new_cols, axis=1)\n\n    df.dropna(inplace=True)\n\n    return df\n</code></pre>"},{"location":"api_reference/#docs.notebooks.config.general_functions.compute_MAE","title":"<code>compute_MAE(forecast, observation)</code>","text":"<p>As the name suggest.</p> <p>Parameters:</p> Name Type Description Default <code>forecast</code> <code>Series</code> <p>Forecasted values from the model.</p> required <code>observation</code> <code>Series</code> <p>Observed (actual) values.</p> required <p>Returns:</p> Name Type Description <code>MAE</code> <code>float</code> <p>Mean Absolute Error rounded to three decimals.</p> Source code in <code>docs\\notebooks\\config\\general_functions.py</code> <pre><code>def compute_MAE(forecast, observation):\n    \"\"\"As the name suggest.\n\n    Args:\n        forecast (pd.Series): Forecasted values from the model.\n        observation (pd.Series): Observed (actual) values.\n\n    Returns:\n        MAE (float): Mean Absolute Error rounded to three decimals.\n    \"\"\"\n    return round((abs(forecast - observation)).mean(), 3)\n</code></pre>"},{"location":"api_reference/#docs.notebooks.config.general_functions.compute_MAPE","title":"<code>compute_MAPE(forecast, observation)</code>","text":"<p>Compute the Mean Absolute Percentage Error (MAPE).</p> Note <p>MAPE can approach infinity if any observed value is zero.</p> <p>Parameters:</p> Name Type Description Default <code>forecast</code> <code>Series</code> <p>Forecasted values from the model.</p> required <code>observation</code> <code>Series</code> <p>Observed (actual) values.</p> required <p>Returns:</p> Name Type Description <code>MAPE</code> <code>float</code> <p>Mean Absolute Percentage Error rounded to three decimals.</p> Source code in <code>docs\\notebooks\\config\\general_functions.py</code> <pre><code>def compute_MAPE(forecast, observation):\n    \"\"\"Compute the Mean Absolute Percentage Error (MAPE).\n\n    Note:\n        MAPE can approach infinity if any observed value is zero.\n\n    Args:\n        forecast (pd.Series): Forecasted values from the model.\n        observation (pd.Series): Observed (actual) values.\n\n    Returns:\n        MAPE (float): Mean Absolute Percentage Error rounded to three decimals.\n    \"\"\"\n    return round((abs((forecast - observation) / observation) * 100).mean(), 3)\n</code></pre>"},{"location":"api_reference/#docs.notebooks.config.general_functions.compute_MASE","title":"<code>compute_MASE(forecast, observation, train_result)</code>","text":"<p>As the name suggest. MASE is first introduced by Rob Hyndman, used to handle MAPE problem being infinity.  Instead of using observed value as denominator, MASE uses MAE of the naive forecast at the train set for denominator. </p> <p>Parameters:</p> Name Type Description Default <code>forecast</code> <code>df</code> <p>series of the forecast result from the model</p> required <code>observation</code> <code>df</code> <p>series of the observed value (actual value)</p> required <p>Returns:</p> Name Type Description <code>MASE</code> <code>float</code> <p>as the name suggest</p> Source code in <code>docs\\notebooks\\config\\general_functions.py</code> <pre><code>def compute_MASE(forecast, observation, train_result):\n    \"\"\"As the name suggest. MASE is first introduced by Rob Hyndman, used to handle MAPE problem being infinity. \n    Instead of using observed value as denominator,\n    MASE uses MAE of the naive forecast at the train set for denominator. \n\n    Args:\n        forecast (df): series of the forecast result from the model\n        observation (df): series of the observed value (actual value)\n\n    Returns:\n        MASE (float): as the name suggest\n    \"\"\"\n    errors = abs(forecast - observation)\n    MAE_naive = compute_MAE(train_result['naive'], train_result['observation'])\n\n    MASE = errors.mean() / MAE_naive\n    return round(MASE, 3)\n</code></pre>"},{"location":"api_reference/#docs.notebooks.config.general_functions.compute_MBE","title":"<code>compute_MBE(forecast, observation)</code>","text":"<p>As the name suggest.</p> <p>Parameters:</p> Name Type Description Default <code>forecast</code> <code>df</code> <p>series of the forecast result from the model</p> required <code>observation</code> <code>df</code> <p>series of the observed value (actual value)</p> required <p>Returns:</p> Name Type Description <code>MBE</code> <code>float</code> <p>as the name suggest</p> Source code in <code>docs\\notebooks\\config\\general_functions.py</code> <pre><code>def compute_MBE(forecast, observation):\n    \"\"\"As the name suggest.\n\n    Args:\n        forecast (df): series of the forecast result from the model\n        observation (df): series of the observed value (actual value)\n\n    Returns:\n        MBE (float): as the name suggest\n    \"\"\"\n    return round(((forecast - observation).sum()) / len(observation), 5)\n</code></pre>"},{"location":"api_reference/#docs.notebooks.config.general_functions.compute_R2","title":"<code>compute_R2(forecast, observation)</code>","text":"<p>As the name suggest. Be careful with R2 though because it is not a forecast evaluation.  It is just used to show linearity on the scatter plot of forecast and observed value. </p> <p>Parameters:</p> Name Type Description Default <code>forecast</code> <code>df</code> <p>series of the forecast result from the model</p> required <code>observation</code> <code>df</code> <p>series of the observed value (actual value)</p> required <p>Returns:</p> Name Type Description <code>R2</code> <code>float</code> <p>as the name suggest</p> Source code in <code>docs\\notebooks\\config\\general_functions.py</code> <pre><code>def compute_R2(forecast, observation):\n    \"\"\"As the name suggest. Be careful with R2 though because it is not a forecast evaluation. \n    It is just used to show linearity on the scatter plot of forecast and observed value. \n\n    Args:\n        forecast (df): series of the forecast result from the model\n        observation (df): series of the observed value (actual value)\n\n    Returns:\n        R2 (float): as the name suggest\n    \"\"\"\n    return round(forecast.corr(observation)**2, 3)\n</code></pre>"},{"location":"api_reference/#docs.notebooks.config.general_functions.compute_RMSE","title":"<code>compute_RMSE(forecast, observation)</code>","text":"<p>As the name suggest.</p> <p>Parameters:</p> Name Type Description Default <code>forecast</code> <code>df</code> <p>series of the forecast result from the model</p> required <code>observation</code> <code>df</code> <p>series of the observed value (actual value)</p> required <p>Returns:</p> Name Type Description <code>RMSE</code> <code>float</code> <p>as the name suggest</p> Source code in <code>docs\\notebooks\\config\\general_functions.py</code> <pre><code>def compute_RMSE(forecast, observation):\n    \"\"\"As the name suggest.\n\n    Args:\n        forecast (df): series of the forecast result from the model\n        observation (df): series of the observed value (actual value)\n\n    Returns:\n        RMSE (float): as the name suggest\n    \"\"\"\n    return round(np.sqrt(((forecast - observation) ** 2).mean()), 3)\n</code></pre>"},{"location":"api_reference/#docs.notebooks.config.general_functions.compute_exp_no","title":"<code>compute_exp_no(path_result)</code>","text":"<p>Compute experiment number for folder and file naming.</p> <p>This function determines the experiment number based on the count of  existing experiment folders. For example, if the folder already contains  5 experiment folders, the new experiment number will be <code>E00006</code>.</p> <p>Parameters:</p> Name Type Description Default <code>path_result</code> <code>str</code> <p>Relative path of the experiment folder, stored in config.</p> required <p>Returns:</p> Name Type Description <code>experiment_no</code> <code>int</code> <p>Experiment number as an integer.</p> <code>experiment_no_str</code> <code>str</code> <p>Experiment number as a zero-padded string.</p> Source code in <code>docs\\notebooks\\config\\general_functions.py</code> <pre><code>def compute_exp_no(path_result):\n    \"\"\"Compute experiment number for folder and file naming.\n\n    This function determines the experiment number based on the count of \n    existing experiment folders. For example, if the folder already contains \n    5 experiment folders, the new experiment number will be `E00006`.\n\n    Args:\n        path_result (str): Relative path of the experiment folder, stored in config.\n\n    Returns:\n        experiment_no (int): Experiment number as an integer.\n        experiment_no_str (str): Experiment number as a zero-padded string.\n    \"\"\"\n    subfolders = os.listdir(path_result)\n    number_of_folders = len(subfolders)\n    experiment_no = number_of_folders - 1 # there is one archive folder\n    experiment_no_str = f\"E{str(experiment_no).zfill(5)}\"\n\n    return experiment_no, experiment_no_str\n</code></pre>"},{"location":"api_reference/#docs.notebooks.config.general_functions.compute_folder_name","title":"<code>compute_folder_name(experiment_no_str, forecast_horizon, model_name, hyperparameter_no)</code>","text":"<p>Folder name in the format of [exp number][exp date][dataset][forecast horizon][model]_[hyperparameter]</p> <p>Parameters:</p> Name Type Description Default <code>experiment_no_str</code> <code>str</code> <p>exp number</p> required <code>forecast_horizon</code> <code>int</code> <p>forecast horizon in minutes</p> required <code>model_name</code> <code>str</code> <p>for example, m1_naive</p> required <code>hyperparameter_no</code> <code>str</code> <p>for example, hp1</p> required <p>Returns:</p> Name Type Description <code>folder_name</code> <code>str</code> <p>folder name</p> Source code in <code>docs\\notebooks\\config\\general_functions.py</code> <pre><code>def compute_folder_name(experiment_no_str, forecast_horizon, model_name, hyperparameter_no):\n    \"\"\"Folder name in the format of [exp number]_[exp date]_[dataset]_[forecast horizon]_[model]_[hyperparameter]\n\n    Args:\n        experiment_no_str (str): exp number\n        forecast_horizon (int): forecast horizon in minutes\n        model_name (str): for example, m1_naive\n        hyperparameter_no (str): for example, hp1\n\n    Returns:\n        folder_name (str): folder name\n    \"\"\"\n    folder_name = \\\n        experiment_no_str + '_' +\\\n        datetime.today().date().strftime(\"%y%m%d\") + '_' +\\\n        dataset.split('_')[0] + '_' +\\\n        'fh' + str(forecast_horizon) + '_' +\\\n        model_name + '_' +\\\n        hyperparameter_no\n    return folder_name\n</code></pre>"},{"location":"api_reference/#docs.notebooks.config.general_functions.compute_fskill","title":"<code>compute_fskill(forecast, observation, naive)</code>","text":"<p>As the name suggest. Forecast Skill is a relative measure seeing the improvement  of the model performance over naive model. </p> <p>Parameters:</p> Name Type Description Default <code>forecast</code> <code>df</code> <p>series of the forecast result from the model</p> required <code>observation</code> <code>df</code> <p>series of the observed value (actual value)</p> required <p>Returns:</p> Name Type Description <code>fskill</code> <code>float</code> <p>as the name suggest</p> Source code in <code>docs\\notebooks\\config\\general_functions.py</code> <pre><code>def compute_fskill(forecast, observation, naive):\n    \"\"\"As the name suggest. Forecast Skill is a relative measure seeing the improvement \n    of the model performance over naive model. \n\n    Args:\n        forecast (df): series of the forecast result from the model\n        observation (df): series of the observed value (actual value)\n\n    Returns:\n        fskill (float): as the name suggest\n    \"\"\"\n    return round((1 - compute_RMSE(forecast, observation) / compute_RMSE(naive, observation)) * 100, 3)\n</code></pre>"},{"location":"api_reference/#docs.notebooks.config.general_functions.export_result","title":"<code>export_result(filepath, df_a1_result, cross_val_result_df, hyperparameter)</code>","text":"<p>Export experiment summary results.</p> This function exports <ol> <li>Experiment result.</li> <li>Hyperparameters.</li> <li>Cross-validation detailed results.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>dict</code> <p>Dictionary of file paths for exporting results.</p> required <code>df_a1_result</code> <code>DataFrame</code> <p>DataFrame containing the experiment results.</p> required <code>cross_val_result_df</code> <code>DataFrame</code> <p>DataFrame containing cross-validation results.</p> required <code>hyperparameter</code> <code>dict</code> <p>Dictionary of hyperparameters used in the experiment.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>docs\\notebooks\\config\\general_functions.py</code> <pre><code>def export_result(filepath, df_a1_result, cross_val_result_df, hyperparameter):\n    \"\"\"Export experiment summary results.\n\n    This function exports:\n        1. Experiment result.\n        2. Hyperparameters.\n        3. Cross-validation detailed results.\n\n    Args:\n        filepath (dict): Dictionary of file paths for exporting results.\n        df_a1_result (pd.DataFrame): DataFrame containing the experiment results.\n        cross_val_result_df (pd.DataFrame): DataFrame containing cross-validation results.\n        hyperparameter (dict): Dictionary of hyperparameters used in the experiment.\n\n    Returns:\n        None\n    \"\"\"\n    # Create a df of hyperparameter being used\n    df_a2 = pd.DataFrame(hyperparameter)\n\n    # EXPORT IT\n    df_a1_result.to_csv(filepath['a1'], index=False)\n    df_a2.to_csv(filepath['a2'])\n    cross_val_result_df.to_csv(filepath['a3'])\n</code></pre>"},{"location":"api_reference/#docs.notebooks.config.general_functions.histogram_residual","title":"<code>histogram_residual(residual, df, pathname)</code>","text":"<p>Produce histogiram of residual value and save it on the designated folder</p> <p>Parameters:</p> Name Type Description Default <code>residual</code> <code>df</code> <p>forecast - observation</p> required <code>pathname</code> <code>str</code> <p>filepath to save the figure</p> required Source code in <code>docs\\notebooks\\config\\general_functions.py</code> <pre><code>def histogram_residual(residual, df, pathname):\n    \"\"\"Produce histogiram of residual value and save it on the designated folder\n\n    Args:\n        residual (df): forecast - observation\n        pathname (str): filepath to save the figure\n    \"\"\"\n    # Create the figure with specified size\n    plt.figure(figsize=(9, 9))\n\n    # Set background color\n    # plt.gcf().patch.set_facecolor(platinum)\n\n    # Compute the range\n    dataset_range = df['y'].max() - df['y'].min()\n    bin_min = -dataset_range/7\n    bin_max = dataset_range/7\n\n    # Plot the actual and forecast data\n    plt.hist(residual, bins=31, range=(bin_min, bin_max), color=dark_blue, edgecolor=dark_blue, alpha=0.7)\n\n\n    # Set the font to Arial\n    plt.rcParams['font.family'] = 'Arial'\n\n    # Remove grid lines\n    plt.grid(False)\n\n    # Set tick marks for x and y axis\n    plt.xticks(fontsize=12, color=dark_blue, alpha=0.5, rotation=0)\n    plt.yticks(fontsize=12, color=dark_blue, alpha=0.5)\n\n    # Add borders to the plot\n    plt.gca().spines['top'].set_color(dark_blue)\n    plt.gca().spines['right'].set_color(dark_blue)\n    plt.gca().spines['bottom'].set_color(dark_blue)\n    plt.gca().spines['left'].set_color(dark_blue)\n\n    # Remove the tick markers (the small lines)\n    plt.tick_params(axis='x', which='both', length=0)  # Remove x-axis tick markers\n    plt.tick_params(axis='y', which='both', length=0)  # Remove y-axis tick markers\n\n    # Set axis titles\n    plt.xlabel('Forecast Residual (kW)', fontsize=14, color=dark_blue)\n    plt.ylabel('Count', fontsize=14, color=dark_blue)\n\n    # Remove title\n    plt.title('')\n\n\n    plt.savefig(pathname, format='png', bbox_inches='tight')\n    plt.close()\n</code></pre>"},{"location":"api_reference/#docs.notebooks.config.general_functions.input_and_process","title":"<code>input_and_process(path_data_cleaned, forecast_horizon, max_lag_day, n_block, hyperparameter)</code>","text":"<p>read dataset, add calendar features, add lag features (which depends on the forecast horizon).</p> <p>Parameters:</p> Name Type Description Default <code>path_data_cleaned</code> <code>str</code> <p>path to the dataset chosen</p> required <code>forecast_horizon</code> <code>int</code> <p>forecast horizon in minutes</p> required <code>max_lag_day</code> <code>int</code> <p>how much lag data will be used, written in days. For example, 7 means lag data until d-7 is used. </p> required <code>n_block</code> <code>int</code> <p>number of blocks to divide the original df. This includes the block for hold_out_df, so if k=10, this n_block = k+1 = 11</p> required <code>hyperparameter</code> <code>dict</code> <p>hyperparameters for the model</p> required <p>Returns:</p> Name Type Description <code>block_length</code> <code>int</code> <p>number of weeks per block</p> <code>holdout_df</code> <code>df</code> <p>unused df, can be used later for unbiased estimate of final model performance</p> <code>df</code> <code>df</code> <p>df that will be used for training and validation (test) set</p> Source code in <code>docs\\notebooks\\config\\general_functions.py</code> <pre><code>def input_and_process(path_data_cleaned, forecast_horizon, max_lag_day, n_block, hyperparameter):\n    \"\"\"read dataset, add calendar features, add lag features (which depends on the forecast horizon).\n\n    Args:\n        path_data_cleaned (str): path to the dataset chosen\n        forecast_horizon (int): forecast horizon in minutes\n        max_lag_day (int): how much lag data will be used, written in days. For example, 7 means lag data until d-7 is used. \n        n_block (int): number of blocks to divide the original df. This includes the block for hold_out_df, so if k=10, this n_block = k+1 = 11\n        hyperparameter (dict): hyperparameters for the model\n\n    Returns:\n        block_length (int): number of weeks per block\n        holdout_df (df): unused df, can be used later for unbiased estimate of final model performance\n        df (df): df that will be used for training and validation (test) set\n    \"\"\"\n    # MAKE THIS AS FUNCTION\n    # ADD CALENDAR DATA (holiday to add)\n    # columns_to_use = ['datetime', 'netload_kW']\n    df = pd.read_csv(path_data_cleaned + dataset, index_col=0, parse_dates=True)\n    df.rename(columns={'netload_kW': 'y'}, inplace=True)\n\n    # 1. Check if forecast horizon is &gt;= dataset frequency\n    # for example, if dataset is daily, forecast horizon should be at least 1 day\n    # compute dataset frequency in minutes based on the datetime index\n    dataset_freq = (df.index[1] - df.index[0]).seconds / 60\n    if forecast_horizon &lt; dataset_freq:\n        raise ValueError('Forecast horizon should be &gt;= dataset frequency')\n    else:\n        print('Pass Test 1 - Forecast horizon is &gt;= dataset frequency')\n\n    # 2. Check if hyperparameter choice is possible given the forecast horizon\n    # for example, with forecast horizon of 2 days, we cannot use 1 day as the hyperparameter of seasonal naive forecast.\n\n\n    if model_name == 'm2_snaive':\n        if int(hyperparameter['days'] * 24 * 60) &lt; forecast_horizon:\n            raise ValueError('Choice of seasonal naive hyperparameter needs to be &gt;= forecast horizon! Please change the hyperparameter.')\n    # if model_name == 'm4_sarima':\n    #     if int(hyperparameter['seasonal_period_days'] * 24 * 60) &lt; forecast_horizon:\n    #         raise ValueError('Choice of seasonal_period_days in SARIMA hyperparameter &gt;= forecast horizon! Please change the hyperparameter.')\n    print('Pass Test 2 - Hyperparameter choice is possible given the forecast horizon')\n\n\n# ADD LAG FEATURES\n    df = add_lag_features(df, forecast_horizon, max_lag_day)\n\n# ADD CALENDAR FEATURES    \n    # 1. Numerical representation of the datetime (Excel-style)\n    numeric_datetime = pd.Series((df.index - pd.Timestamp(\"1970-01-01\")) / pd.Timedelta(days=1), index=df.index)\n\n    # 2. Year\n    year = pd.Series(df.index.year, index=df.index)\n\n    # 3. One-hot encoding of month (is_jan, is_feb, ..., is_nov, excluding December)\n    month_dummies = pd.get_dummies(df.index.month, prefix='is', drop_first=False)\n\n    # Custom column names for months: is_jan, is_feb, ..., is_nov\n    month_names = ['is_jan', 'is_feb', 'is_mar', 'is_apr', 'is_may', 'is_jun', \n                'is_jul', 'is_aug', 'is_sep', 'is_oct', 'is_nov', 'is_dec']  \n\n    # Drop the last column (December) to avoid redundancy and rename the columns\n    month_dummies = month_dummies.iloc[:, :-1]  # Exclude December column\n    month_dummies.columns = month_names[:month_dummies.shape[1]]  # Apply custom column names\n    month_dummies = month_dummies.astype(int)  # Convert to 1 and 0\n    month_dummies.index = df.index\n\n    # 4. One-hot encoding of hour (hour_0, hour_1, ..., hour_22, excluding hour_23)\n    hour_dummies = pd.get_dummies(df.index.hour, prefix='hour', drop_first=False).iloc[:, :-1]\n    hour_dummies = hour_dummies.astype(int)  # Convert to 1 and 0\n    hour_dummies.index = df.index\n\n    # 5. One-hot encoding of day of week (is_mon, is_tue, ..., is_sat, excluding Sunday)\n    # Mapping day of week (0=Mon, 1=Tue, ..., 6=Sun)\n    dayofweek_dummies = pd.get_dummies(df.index.dayofweek, prefix='is', drop_first=False).iloc[:, :-1]\n\n    # Custom mapping for days of the week: is_mon, is_tue, ..., is_sat\n    dayofweek_names = ['is_mon', 'is_tue', 'is_wed', 'is_thu', 'is_fri', 'is_sat']  # Custom day names\n    dayofweek_dummies.columns = dayofweek_names[:dayofweek_dummies.shape[1]]  # Apply custom column names\n    dayofweek_dummies = dayofweek_dummies.astype(int)  # Convert to 1 and 0\n    dayofweek_dummies.index = df.index\n\n    # 6. Is weekday (1 if Monday to Friday, 0 if Saturday/Sunday)\n    is_weekday = pd.Series((df.index.dayofweek &lt; 5).astype(int), index=df.index)\n\n\n    # Concatenate all new features into the original dataframe at once\n    df = pd.concat([df, \n                    numeric_datetime.rename('numeric_datetime'), \n                    year.rename('year'),\n                    month_dummies, \n                    hour_dummies, \n                    dayofweek_dummies, \n                    is_weekday.rename('is_weekday')], axis=1)\n\n    block_length, holdout_df, df = separate_holdout(df, n_block)\n\n    return block_length, holdout_df, df\n</code></pre>"},{"location":"api_reference/#docs.notebooks.config.general_functions.prepare_directory","title":"<code>prepare_directory(path_result, forecast_horizon, model_name, hyperparameter_no)</code>","text":"<p>Prepare experiment result directories and file paths.</p> This function <ol> <li>Creates required folders inside the experiment result folder.</li> <li>Generates file paths to be used when exporting results.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>path_result</code> <code>str</code> <p>Relative path to the experiment result folder.</p> required <code>forecast_horizon</code> <code>int</code> <p>Forecast horizon in minutes.</p> required <code>model_name</code> <code>str</code> <p>Model name (e.g., \"m1_naive\").</p> required <code>hyperparameter_no</code> <code>str</code> <p>Index or identifier of the chosen hyperparameter.</p> required <p>Returns:</p> Name Type Description <code>hyperparameter</code> <code>Series</code> <p>Selected hyperparameter configuration.</p> <code>experiment_no_str</code> <code>str</code> <p>Experiment number as a zero-padded string.</p> <code>filepath</code> <code>dict</code> <p>Dictionary of file paths for results, plots, CV splits, and models.</p> Source code in <code>docs\\notebooks\\config\\general_functions.py</code> <pre><code>def prepare_directory(path_result, forecast_horizon, model_name, hyperparameter_no):\n    \"\"\"Prepare experiment result directories and file paths.\n\n    This function:\n        1. Creates required folders inside the experiment result folder.\n        2. Generates file paths to be used when exporting results.\n\n    Args:\n        path_result (str): Relative path to the experiment result folder.\n        forecast_horizon (int): Forecast horizon in minutes.\n        model_name (str): Model name (e.g., \"m1_naive\").\n        hyperparameter_no (str): Index or identifier of the chosen hyperparameter.\n\n    Returns:\n        hyperparameter (pd.Series): Selected hyperparameter configuration.\n        experiment_no_str (str): Experiment number as a zero-padded string.\n        filepath (dict): Dictionary of file paths for results, plots, CV splits, and models.\n    \"\"\"\n\n    hyperparameter_table = globals()[f\"{model_name.split('_')[0]}_hp_table\"]\n    hyperparameter = hyperparameter_table.loc[hyperparameter_no]\n\n    experiment_no, experiment_no_str = compute_exp_no(path_result)\n    folder_name = compute_folder_name(experiment_no_str, forecast_horizon, model_name, hyperparameter_no)\n\n    # CREATE FOLDER\n    cv_folder_train = experiment_no_str + '_cv_train'\n    cv_folder_test = experiment_no_str + '_cv_test'\n    cv1_plot_folder = experiment_no_str + '_cv1_plots'\n    folder_model = experiment_no_str + '_models'\n\n    path_result2 = path_result + folder_name +'/'\n    path_result_train = path_result2 + cv_folder_train +'/'\n    path_result_test = path_result2 + cv_folder_test +'/'\n    path_result_plot = path_result2 + cv1_plot_folder +'/'\n    path_model = path_result2 + folder_model +'/'\n\n    # MAKE FOLDERS\n    os.mkdir(path_result2)\n    os.mkdir(path_result_train)\n    os.mkdir(path_result_test)\n    os.mkdir(path_result_plot)\n    os.mkdir(path_model)\n\n    # MAKE FILE PATH\n    filepath = {\n        'a1' : path_result2 + experiment_no_str + '_a1_experiment_result.csv',\n        'a2' : path_result2 + experiment_no_str + '_a2_hyperparameter.csv',\n        'a3' : path_result2 + experiment_no_str + '_a3_cross_validation_result.csv',\n        'b1' : path_result_plot + experiment_no_str + '_b1_train_timeplot.png', # Time Plot of Forecast vs Observation\n        'b2' : path_result_plot + experiment_no_str + '_b2_train_scatterplot.png', # Scatter Plot of Forecast vs Observation\n        'b3' : path_result_plot + experiment_no_str + '_b3_train_residual_timeplot.png', # Time Plot of Residual\n        'b4' : path_result_plot + experiment_no_str + '_b4_train_residual_histogram.png', # Histogram of Residual\n        'b5' : path_result_plot + experiment_no_str + '_b5_train_learningcurve.png', # Learning Curve vs Epoch\n        'c1' : path_result_plot + experiment_no_str + '_c1_test_timeplot.png',  # Time Plot of Forecast vs Observation\n        'c2' : path_result_plot + experiment_no_str + '_c2_test_scatterplot.png',  # Scatter Plot of Forecast vs Observation\n        'c3' : path_result_plot + experiment_no_str + '_c3_test_residual_timeplot.png',  # Time Plot of Residual\n        'c4' : path_result_plot + experiment_no_str + '_c4_test_residual_histogram.png',  # Histogram of Residual\n        'c5' : path_result_plot + experiment_no_str + '_c5_test_learningcurve.png',  # Learning Curve vs Epoch\n\n        # B. FOLDER FOR CROSS VALIDATION TIME SERIES\n        'train_cv' : {\n            1 : path_result_train + experiment_no_str + '_cv1_train_result.csv',\n            2 : path_result_train + experiment_no_str + '_cv2_train_result.csv',\n            3 : path_result_train + experiment_no_str + '_cv3_train_result.csv',\n            4 : path_result_train + experiment_no_str + '_cv4_train_result.csv',\n            5 : path_result_train + experiment_no_str + '_cv5_train_result.csv',\n            6 : path_result_train + experiment_no_str + '_cv6_train_result.csv',\n            7 : path_result_train + experiment_no_str + '_cv7_train_result.csv',\n            8 : path_result_train + experiment_no_str + '_cv8_train_result.csv',\n            9 : path_result_train + experiment_no_str + '_cv9_train_result.csv',\n            10 : path_result_train + experiment_no_str + '_cv10_train_result.csv'\n        },\n\n        'test_cv' : {\n            1 : path_result_test + experiment_no_str + '_cv1_test_result.csv',\n            2 : path_result_test + experiment_no_str + '_cv2_test_result.csv',\n            3 : path_result_test + experiment_no_str + '_cv3_test_result.csv',\n            4 : path_result_test + experiment_no_str + '_cv4_test_result.csv',\n            5 : path_result_test + experiment_no_str + '_cv5_test_result.csv',\n            6 : path_result_test + experiment_no_str + '_cv6_test_result.csv',\n            7 : path_result_test + experiment_no_str + '_cv7_test_result.csv',\n            8 : path_result_test + experiment_no_str + '_cv8_test_result.csv',\n            9 : path_result_test + experiment_no_str + '_cv9_test_result.csv',\n            10 : path_result_test + experiment_no_str + '_cv10_test_result.csv'\n        },\n\n        'model' : {\n            1 : path_model + experiment_no_str + '_cv1_model.pkl',\n            2 : path_model + experiment_no_str + '_cv2_model.pkl',\n            3 : path_model + experiment_no_str + '_cv3_model.pkl',\n            4 : path_model + experiment_no_str + '_cv4_model.pkl',\n            5 : path_model + experiment_no_str + '_cv5_model.pkl',\n            6 : path_model + experiment_no_str + '_cv6_model.pkl',\n            7 : path_model + experiment_no_str + '_cv7_model.pkl',\n            8 : path_model + experiment_no_str + '_cv8_model.pkl',\n            9 : path_model + experiment_no_str + '_cv9_model.pkl',\n            10 : path_model + experiment_no_str + '_cv10_model.pkl'\n        }\n    }\n    return hyperparameter,experiment_no_str, filepath\n</code></pre>"},{"location":"api_reference/#docs.notebooks.config.general_functions.produce_forecast","title":"<code>produce_forecast(model_name, model, train_df_X, test_df_X, train_df_y, forecast_horizon)</code>","text":"<p>Generate forecasts based on the model and its name.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>string</code> <p>Model identifier (e.g., 'm1_naive').</p> required <code>model</code> <code>dict</code> <p>Trained model object containing all relevant features.</p> required <code>train_df_X</code> <code>DataFrame</code> <p>Matrix of predictors for training set.</p> required <code>test_df_X</code> <code>DataFrame</code> <p>Matrix of predictors for test set.</p> required <code>train_df_y</code> <code>DataFrame</code> <p>Target forecast y for training set.</p> required <code>forecast_horizon</code> <code>int</code> <p>Forecast horizon in minutes.</p> required <p>Returns:</p> Name Type Description <code>train_df_y_hat</code> <code>DataFrame</code> <p>Forecasted values for training set.</p> <code>test_df_y_hat</code> <code>DataFrame</code> <p>Forecasted values for test set.</p> Source code in <code>docs\\notebooks\\config\\general_functions.py</code> <pre><code>def produce_forecast(model_name, model, train_df_X, test_df_X, train_df_y, forecast_horizon):\n    \"\"\"Generate forecasts based on the model and its name.\n\n    Args:\n        model_name (string): Model identifier (e.g., 'm1_naive').\n        model (dict): Trained model object containing all relevant features.\n        train_df_X (DataFrame): Matrix of predictors for training set.\n        test_df_X (DataFrame): Matrix of predictors for test set.\n        train_df_y (DataFrame): Target forecast y for training set.\n        forecast_horizon (int): Forecast horizon in minutes.\n\n    Returns:\n        train_df_y_hat (DataFrame): Forecasted values for training set.\n        test_df_y_hat (DataFrame): Forecasted values for test set.\n    \"\"\"\n\n    if model_name == 'm1_naive':\n        train_df_y_hat, test_df_y_hat = produce_forecast_m1_naive(model, train_df_X, test_df_X)\n    elif model_name == 'm2_snaive':\n        train_df_y_hat, test_df_y_hat = produce_forecast_m2_snaive(model, train_df_X, test_df_X)\n    elif model_name == 'm3_ets':\n        train_df_y_hat, test_df_y_hat = produce_forecast_m3_ets(model, train_df_X, test_df_X, forecast_horizon)\n    elif model_name == 'm4_arima':\n        train_df_y_hat, test_df_y_hat = produce_forecast_m4_arima(model, train_df_X, test_df_X, forecast_horizon)\n    elif model_name == 'm5_sarima':\n        train_df_y_hat, test_df_y_hat = produce_forecast_m5_sarima(model, train_df_X, test_df_X, forecast_horizon)\n    elif model_name == 'm6_lr':\n        train_df_y_hat, test_df_y_hat = produce_forecast_m6_lr(model, train_df_X, test_df_X)\n    elif model_name == 'm7_ann':\n        train_df_y_hat, test_df_y_hat = produce_forecast_m7_ann(model, train_df_X, test_df_X)\n    elif model_name == 'm8_dnn':\n        train_df_y_hat, test_df_y_hat = produce_forecast_m8_dnn(model, train_df_X, test_df_X)\n    elif model_name == 'm9_rt':\n        train_df_y_hat, test_df_y_hat = produce_forecast_m9_rt(model, train_df_X, test_df_X)\n    elif model_name == 'm10_rf':\n        train_df_y_hat, test_df_y_hat = produce_forecast_m10_rf(model, train_df_X, test_df_X)\n    elif model_name == 'm11_svr':\n        train_df_y_hat, test_df_y_hat = produce_forecast_m11_svr(model, train_df_X, test_df_X)\n    elif model_name == 'm12_rnn':\n        train_df_y_hat, test_df_y_hat = produce_forecast_m12_rnn(model, train_df_X, test_df_X)\n    elif model_name == 'm13_lstm':\n        train_df_y_hat, test_df_y_hat = produce_forecast_m13_lstm(model, train_df_X, test_df_X)\n    elif model_name == 'm14_gru':\n        train_df_y_hat, test_df_y_hat = produce_forecast_m14_gru(model, train_df_X, test_df_X)\n    elif model_name == 'm15_transformer':\n        train_df_y_hat, test_df_y_hat = produce_forecast_m15_transformer(model, train_df_X, test_df_X)\n    elif model_name == 'm16_prophet':\n        train_df_y_hat, test_df_y_hat = produce_forecast_m16_prophet(model, train_df_X, test_df_X, train_df_y, forecast_horizon)\n    elif model_name == 'm17_xgb':\n        train_df_y_hat, test_df_y_hat = produce_forecast_m17_xgb(model, train_df_X, test_df_X)\n    elif model_name == 'm18_nbeats':\n        train_df_y_hat, test_df_y_hat = produce_forecast_m18_nbeats(model, train_df_X, test_df_X)\n    else:\n        raise ValueError(\n            \"Wrong Model Choice! Available models are: m1_naive, m2_snaive, m3_ets, m4_arima, m5_sarima, m6_lr, m7_ann, m8_dnn, m9_rt, m10_rf, m11_svr, m12_rnn, m13_lstm, m14_gru, m15_transformer, m16_prophet, m17_xgb, m18_nbeats\"\n        )\n\n    return train_df_y_hat, test_df_y_hat\n</code></pre>"},{"location":"api_reference/#docs.notebooks.config.general_functions.remove_jump_df","title":"<code>remove_jump_df(train_df_y)</code>","text":"<p>Remove jump in the time series data Parameters:     train_df_y (pd.Series): Time series data</p> <p>Returns:</p> Name Type Description <code>train_df_y_updated</code> <code>Series</code> <p>Time series data with jump removed</p> Source code in <code>docs\\notebooks\\config\\general_functions.py</code> <pre><code>def remove_jump_df(train_df_y):\n    #make docstring with the same format like other cells\n    \"\"\"\n    Remove jump in the time series data\n    Parameters:\n        train_df_y (pd.Series): Time series data\n\n    Returns:\n        train_df_y_updated (pd.Series): Time series data with jump removed\n    \"\"\"\n\n    time_diff = train_df_y.index.to_series().diff().dt.total_seconds()\n    initial_freq = time_diff.iloc[1]\n    jump_indices = time_diff[time_diff &gt; initial_freq].index\n    if not jump_indices.empty:\n        jump_index = jump_indices[0]\n        jump_pos = train_df_y.index.get_loc(jump_index)\n        train_df_y_updated = train_df_y.iloc[:jump_pos]\n    else:\n        train_df_y_updated = train_df_y\n    return train_df_y_updated\n</code></pre>"},{"location":"api_reference/#docs.notebooks.config.general_functions.run_experiment","title":"<code>run_experiment(dataset, forecast_horizon, model_name, hyperparameter_no)</code>","text":"<p>Run the experiment with the specified parameters.</p> <p>This function prepares directories, processes input data,  and runs the chosen model. Results and models are saved to disk.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>str</code> <p>Name of the dataset file.</p> required <code>forecast_horizon</code> <code>int</code> <p>Forecast horizon in minutes.</p> required <code>model_name</code> <code>str</code> <p>Model identifier (e.g., \"m6_lr\").</p> required <code>hyperparameter_no</code> <code>str</code> <p>Hyperparameter set identifier.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>docs\\notebooks\\config\\general_functions.py</code> <pre><code>def run_experiment(dataset, forecast_horizon, model_name, hyperparameter_no):\n    \"\"\"Run the experiment with the specified parameters.\n\n    This function prepares directories, processes input data, \n    and runs the chosen model. Results and models are saved to disk.\n\n    Args:\n        dataset (str): Name of the dataset file.\n        forecast_horizon (int): Forecast horizon in minutes.\n        model_name (str): Model identifier (e.g., \"m6_lr\").\n        hyperparameter_no (str): Hyperparameter set identifier.\n\n    Returns:\n        None\n    \"\"\"\n    # PREPARE FOLDER\n    hyperparameter, experiment_no_str, filepath = prepare_directory(path_result, forecast_horizon, model_name, hyperparameter_no)\n    # INPUT DATA\n    block_length, holdout_df, df = input_and_process(path_data_cleaned, forecast_horizon, max_lag_day, n_block, hyperparameter)\n    # RUN MODEL\n    run_model(df, model_name, hyperparameter, filepath, forecast_horizon, experiment_no_str, block_length)\n</code></pre>"},{"location":"api_reference/#docs.notebooks.config.general_functions.run_model","title":"<code>run_model(df, model_name, hyperparameter, filepath, forecast_horizon, experiment_no_str, block_length)</code>","text":"<p>Run model training, validation, and evaluation with cross-validation.</p> This function performs <ol> <li>Cross-validation over multiple folds.</li> <li>Train-test split for each fold.</li> <li>Model training and saving.</li> <li>Naive forecast benchmark.</li> <li>Forecast generation for train and test sets.</li> <li>Residual computation.</li> <li>Export of results (forecasts, residuals, plots).</li> <li>Forecast evaluation with multiple metrics (e.g., RMSE, MAE, MAPE, R\u00b2).</li> <li>Aggregation of performance metrics (mean and stddev).</li> <li>Export of experiment summary and results.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input data with features (X) and target (y).</p> required <code>model_name</code> <code>str</code> <p>Model identifier (e.g., \"m06_lr\").</p> required <code>hyperparameter</code> <code>Series</code> <p>Hyperparameter configuration for the model.</p> required <code>filepath</code> <code>dict</code> <p>Dictionary of file paths for saving outputs.</p> required <code>forecast_horizon</code> <code>int</code> <p>Forecast horizon in minutes.</p> required <code>experiment_no_str</code> <code>str</code> <p>Experiment number as a zero-padded string.</p> required <code>block_length</code> <code>int</code> <p>Block length for one cross-validation set.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>docs\\notebooks\\config\\general_functions.py</code> <pre><code>def run_model(df, model_name, hyperparameter, filepath, forecast_horizon, experiment_no_str, block_length):\n    \"\"\"Run model training, validation, and evaluation with cross-validation.\n\n    This function performs:\n        1. Cross-validation over multiple folds.\n        2. Train-test split for each fold.\n        3. Model training and saving.\n        4. Naive forecast benchmark.\n        5. Forecast generation for train and test sets.\n        6. Residual computation.\n        7. Export of results (forecasts, residuals, plots).\n        8. Forecast evaluation with multiple metrics (e.g., RMSE, MAE, MAPE, R\u00b2).\n        9. Aggregation of performance metrics (mean and stddev).\n        10. Export of experiment summary and results.\n\n    Args:\n        df (pd.DataFrame): Input data with features (X) and target (y).\n        model_name (str): Model identifier (e.g., \"m06_lr\").\n        hyperparameter (pd.Series): Hyperparameter configuration for the model.\n        filepath (dict): Dictionary of file paths for saving outputs.\n        forecast_horizon (int): Forecast horizon in minutes.\n        experiment_no_str (str): Experiment number as a zero-padded string.\n        block_length (int): Block length for one cross-validation set.\n\n    Returns:\n        None\n    \"\"\"\n\n    import warnings\n    warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\n    cross_val_result_df = pd.DataFrame()\n\n    # Compute max_y for normalization later\n    max_y = df['y'].max()\n\n    # DO CROSS VALIDATION\n    for cv_no in range(1, k+1):\n        print(f'Processing CV {cv_no} / {k}....')\n\n        # SPLIT INTO TRAIN AND TEST X AND Y\n        train_df, test_df = split_time_series(df, cv_no)\n        train_df_X, train_df_y = split_xy(train_df)\n        test_df_X, test_df_y = split_xy(test_df)\n\n        # INITIALISE RESULT DF   \n        train_result = train_df_y.copy()\n        train_result = train_result.rename(columns={'y': 'observation'})\n\n        test_result = test_df_y.copy()\n        test_result = test_result.rename(columns={'y': 'observation'})\n\n        # PRODUCE NAIVE FORECAST\n        horizon_timedelta = pd.Timedelta(minutes=forecast_horizon)\n        last_observation = f'y_lag_{horizon_timedelta}m'\n        train_result['naive'] = train_df[last_observation]\n        test_result['naive'] = test_df[last_observation]\n\n        # TRAIN MODEL\n        start_time = time.time()\n        model = train_model(model_name, hyperparameter, train_df_X, train_df_y, forecast_horizon)\n        save_model(filepath, cv_no, model)\n        end_time = time.time()\n        runtime_ms = (end_time - start_time) * 1000  # Convert to milliseconds\n\n        # PRODUCE FORECAST\n        train_df_y_hat, test_df_y_hat = produce_forecast(model_name, model, train_df_X, test_df_X, train_df_y, forecast_horizon)\n        train_result['forecast'] = train_df_y_hat\n        test_result['forecast'] = test_df_y_hat\n\n        # EVALUATE FORECAST\n        train_result['residual'] = train_result['forecast'] - train_result['observation']\n        test_result['residual'] = test_result['forecast'] - test_result['observation']\n        train_R2 = compute_R2(train_result['forecast'], train_result['observation'])\n        test_R2 = compute_R2(test_result['forecast'], test_result['observation'])\n\n        train_RMSE = compute_RMSE(train_result['forecast'], train_result['observation'])\n        test_RMSE = compute_RMSE(test_result['forecast'], test_result['observation'])\n\n        train_nRMSE = 100*train_RMSE / max_y # in percent\n        test_nRMSE = 100*test_RMSE / max_y # in percent\n\n        cross_val_result = pd.DataFrame(\n        {\n            \"runtime_ms\": runtime_ms,\n            \"train_MBE\": compute_MBE(train_result['forecast'], train_result['observation']), \n            \"train_MAE\": compute_MAE(train_result['forecast'], train_result['observation']),\n            \"train_RMSE\": train_RMSE,\n            \"train_MAPE\": compute_MAPE(train_result['forecast'], train_result['observation']),\n            \"train_MASE\": compute_MASE(train_result['forecast'], train_result['observation'], train_result),\n            \"train_fskill\": compute_fskill(train_result['forecast'], train_result['observation'], train_result['naive']),\n            \"train_R2\": train_R2,\n            \"test_MBE\": compute_MBE(test_result['forecast'], test_result['observation']),\n            \"test_MAE\": compute_MAE(test_result['forecast'], test_result['observation']),\n            \"test_RMSE\": test_RMSE,\n            \"test_MAPE\": compute_MAPE(test_result['forecast'], test_result['observation']),\n            \"test_MASE\": compute_MASE(test_result['forecast'], test_result['observation'], train_result),\n            \"test_fskill\": compute_fskill(test_result['forecast'], test_result['observation'], test_result['naive']),\n            \"test_R2\": test_R2,\n            \"train_nRMSE\": train_nRMSE,\n            \"test_nRMSE\": test_nRMSE\n        }, \n        index=[cv_no]\n        )\n\n        if cross_val_result_df.empty:\n            cross_val_result_df = cross_val_result\n        else:\n            cross_val_result_df = pd.concat([cross_val_result_df, cross_val_result], ignore_index=False)\n        cross_val_result_df.index.name = 'cv_no'\n\n        # EXPORT RESULTS DF TO CSV\n        train_result.to_csv(filepath['train_cv'][cv_no])\n        test_result.to_csv(filepath['test_cv'][cv_no])\n\n        # IF CV_NO = 1, ALSO EXPORT SOME PLOTS\n        if cv_no == 1:\n            timeplot_forecast(train_result['observation'], train_result['forecast'], filepath['b1'])\n            timeplot_forecast(test_result['observation'], test_result['forecast'], filepath['c1'])\n            scatterplot_forecast(train_result['observation'], train_result['forecast'], train_R2, filepath['b2'])\n            scatterplot_forecast(test_result['observation'], test_result['forecast'], test_R2, filepath['c2'])\n            timeplot_residual(train_result['residual'], filepath['b3'])\n            timeplot_residual(test_result['residual'], filepath['c3'])\n            histogram_residual(train_result['residual'], df, filepath['b4'])\n            histogram_residual(test_result['residual'], df, filepath['c4'])\n        print()\n\n\n    cross_val_result = pd.DataFrame(\n        {\n            \"runtime_ms\": [cross_val_result_df['runtime_ms'].mean(), cross_val_result_df['runtime_ms'].std()],\n            \"train_MBE\": [cross_val_result_df['train_MBE'].mean(), cross_val_result_df['train_MBE'].std()], \n            \"train_MAE\": [cross_val_result_df['train_MAE'].mean(), cross_val_result_df['train_MAE'].std()],\n            \"train_RMSE\": [cross_val_result_df['train_RMSE'].mean(), cross_val_result_df['train_RMSE'].std()],\n            \"train_MAPE\": [cross_val_result_df['train_MAPE'].mean(), cross_val_result_df['train_MAPE'].std()],\n            \"train_MASE\": [cross_val_result_df['train_MASE'].mean(), cross_val_result_df['train_MASE'].std()],\n            \"train_fskill\": [cross_val_result_df['train_fskill'].mean(), cross_val_result_df['train_fskill'].std()],\n            \"train_R2\": [cross_val_result_df['train_R2'].mean(), cross_val_result_df['train_R2'].std()],\n            \"test_MBE\": [cross_val_result_df['test_MBE'].mean(), cross_val_result_df['test_MBE'].std()],\n            \"test_MAE\": [cross_val_result_df['test_MAE'].mean(), cross_val_result_df['test_MAE'].std()],\n            \"test_RMSE\": [cross_val_result_df['test_RMSE'].mean(), cross_val_result_df['test_RMSE'].std()],\n            \"test_MAPE\": [cross_val_result_df['test_MAPE'].mean(), cross_val_result_df['test_MAPE'].std()],\n            \"test_MASE\": [cross_val_result_df['test_MASE'].mean(), cross_val_result_df['test_MASE'].std()],\n            \"test_fskill\": [cross_val_result_df['test_fskill'].mean(), cross_val_result_df['test_fskill'].std()],\n            \"test_R2\": [cross_val_result_df['test_R2'].mean(), cross_val_result_df['test_R2'].std()],\n            \"train_nRMSE\": [cross_val_result_df['train_nRMSE'].mean(), cross_val_result_df['train_nRMSE'].std()],\n            \"test_nRMSE\": [cross_val_result_df['test_nRMSE'].mean(), cross_val_result_df['test_nRMSE'].std()]\n        }, \n        index=['mean', 'stddev']\n        )\n\n    cross_val_result_df = pd.concat([cross_val_result_df, cross_val_result], ignore_index=False)\n\n    data_a1 = {\n        \"experiment_no\": experiment_no_str,\n        \"exp_date\": datetime.today().strftime('%Y-%m-%d'), #today date in YYYY-MM-DD format\n        \"dataset_no\": dataset.split('_')[0],\n        \"dataset\": dataset.split('_')[1].split('.')[0],\n        \"dataset_freq_min\": int((df.index[1] - df.index[0]).total_seconds() / 60),\n        \"dataset_length_week\": block_length * (n_block - 1),\n        \"forecast_horizon_min\": forecast_horizon,\n        \"train_pct\": train_pct,\n        \"test_pct\": test_pct,\n        \"model_no\": model_name.split('_')[0],\n        \"hyperparameter_no\": hyperparameter_no,\n        \"model_name\": model_name + '_' + hyperparameter_no,\n        \"hyperparamter\": ', '.join(f\"{k}: {v}\" for k, v in hyperparameter.items()),  \n        \"runtime_ms\": cross_val_result_df.loc['mean', 'runtime_ms'],\n        \"train_RMSE\": cross_val_result_df.loc['mean', 'train_RMSE'],\n        \"train_RMSE_stddev\": cross_val_result_df.loc['stddev', 'train_RMSE'],\n        \"test_RMSE\": cross_val_result_df.loc['mean', 'test_RMSE'],\n        \"test_RMSE_stddev\": cross_val_result_df.loc['stddev', 'test_RMSE'],\n        \"train_nRMSE\": cross_val_result_df.loc['mean', 'train_nRMSE'],\n        \"train_nRMSE_stddev\": cross_val_result_df.loc['stddev', 'train_nRMSE'],\n        \"test_nRMSE\": cross_val_result_df.loc['mean', 'test_nRMSE'],\n        \"test_nRMSE_stddev\": cross_val_result_df.loc['stddev', 'test_nRMSE']\n    }\n\n    # Create a df of experiment result\n    df_a1_result = pd.DataFrame([data_a1])\n\n    export_result(filepath, df_a1_result, cross_val_result_df, hyperparameter)\n</code></pre>"},{"location":"api_reference/#docs.notebooks.config.general_functions.save_model","title":"<code>save_model(filepath, cv_no, model)</code>","text":"<p>Save a trained model to a binary file using pickle/dill.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>dict</code> <p>Dictionary containing file paths, including model paths.</p> required <code>cv_no</code> <code>int</code> <p>Cross-validation fold number.</p> required <code>model</code> <code>object</code> <p>Trained model object to be serialized.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>docs\\notebooks\\config\\general_functions.py</code> <pre><code>def save_model(filepath, cv_no, model):\n    \"\"\"Save a trained model to a binary file using pickle/dill.\n\n    Args:\n        filepath (dict): Dictionary containing file paths, including model paths.\n        cv_no (int): Cross-validation fold number.\n        model (object): Trained model object to be serialized.\n\n    Returns:\n        None\n    \"\"\"\n\n    with open(filepath['model'][cv_no], \"wb\") as model_file:\n        # pickle.dump(model, model_file)\n        dill.dump(model, model_file)\n</code></pre>"},{"location":"api_reference/#docs.notebooks.config.general_functions.scatterplot_forecast","title":"<code>scatterplot_forecast(observation, forecast, R2, pathname)</code>","text":"<p>Produce scatterplot observation vs forecast value and save it on the designated folder</p> <p>Parameters:</p> Name Type Description Default <code>observation</code> <code>df</code> <p>observed value</p> required <code>forecast</code> <code>df</code> <p>forecast value</p> required <code>pathname</code> <code>str</code> <p>filepath to save the figure</p> required Source code in <code>docs\\notebooks\\config\\general_functions.py</code> <pre><code>def scatterplot_forecast(observation, forecast, R2, pathname):\n    \"\"\"Produce scatterplot observation vs forecast value and save it on the designated folder\n\n    Args:\n        observation (df): observed value\n        forecast (df): forecast value\n        pathname (str): filepath to save the figure\n    \"\"\"\n    # Create the figure with specified size\n    plt.figure(figsize=(9, 9))\n\n    # Set background color\n    # plt.gcf().patch.set_facecolor(platinum)\n\n    # Plot the actual and forecast data\n    plt.scatter(forecast, observation, color=dark_blue, label='Actual', s=40, alpha=0.7)  # 's' sets the size of the points\n\n\n    # Set the font to Arial\n    plt.rcParams['font.family'] = 'Arial'\n\n    # Remove grid lines\n    plt.grid(False)\n\n    # Set tick marks for x and y axis\n    plt.xticks(fontsize=12, color=dark_blue, alpha=0.5, rotation=0)\n    plt.yticks(fontsize=12, color=dark_blue, alpha=0.5)\n\n    # Add borders to the plot\n    plt.gca().spines['top'].set_color(dark_blue)\n    plt.gca().spines['right'].set_color(dark_blue)\n    plt.gca().spines['bottom'].set_color(dark_blue)\n    plt.gca().spines['left'].set_color(dark_blue)\n\n    # Remove the tick markers (the small lines)\n    plt.tick_params(axis='x', which='both', length=0)  # Remove x-axis tick markers\n    plt.tick_params(axis='y', which='both', length=0)  # Remove y-axis tick markers\n\n    # Set axis titles\n    plt.xlabel('Net Load Forecast (kW)', fontsize=14, color=dark_blue)\n    plt.ylabel('Net Load Observation (kW)', fontsize=14, color=dark_blue)\n\n    # Remove title\n    plt.title('')\n\n    # Add R\u00b2 value at the top-left corner\n    plt.text(0.95, 0.05, f'R\u00b2 = {R2:.3f}', transform=plt.gca().transAxes, \n         fontsize=14, color=dark_blue, verticalalignment='bottom', horizontalalignment='right',\n         bbox=dict(facecolor='white', edgecolor=dark_blue, boxstyle='round,pad=0.5', linewidth=1))\n\n\n    plt.savefig(pathname, format='png', bbox_inches='tight')\n    plt.close()\n</code></pre>"},{"location":"api_reference/#docs.notebooks.config.general_functions.separate_holdout","title":"<code>separate_holdout(df, n_block)</code>","text":"<p>Separate dataset into training/validation blocks and holdout set.</p> <p>The dataset is divided into weekly blocks due to weekly seasonality  in net load data. The last block (or more) is reserved as a holdout set,  while the remaining blocks are used for blocked k-fold cross-validation.</p> Example <p>If the dataset has 12 weeks of data and n_block=12: - Training/validation df: weeks 1\u201310. - Holdout df: weeks 11\u201312. With k=10, cross-validation folds are built from the first 10 weeks.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Cleaned DataFrame with target <code>y</code> and predictors.</p> required <code>n_block</code> <code>int</code> <p>Total number of blocks. Includes one block for the holdout set  (e.g., for k=10, n_block=11).</p> required <p>Returns:</p> Name Type Description <code>block_length</code> <code>int</code> <p>Number of weeks per block.</p> <code>holdout_df</code> <code>DataFrame</code> <p>DataFrame reserved as holdout set, unused in CV.</p> <code>df</code> <code>DataFrame</code> <p>DataFrame for training and cross-validation.</p> Source code in <code>docs\\notebooks\\config\\general_functions.py</code> <pre><code>def separate_holdout(df, n_block):    \n    \"\"\"Separate dataset into training/validation blocks and holdout set.\n\n    The dataset is divided into weekly blocks due to weekly seasonality \n    in net load data. The last block (or more) is reserved as a holdout set, \n    while the remaining blocks are used for blocked k-fold cross-validation.\n\n    Example:\n        If the dataset has 12 weeks of data and n_block=12:\n        - Training/validation df: weeks 1\u201310.\n        - Holdout df: weeks 11\u201312.\n        With k=10, cross-validation folds are built from the first 10 weeks.\n\n    Args:\n        df (pd.DataFrame): Cleaned DataFrame with target `y` and predictors.\n        n_block (int): Total number of blocks. Includes one block for the holdout set \n            (e.g., for k=10, n_block=11).\n\n    Returns:\n        block_length (int): Number of weeks per block.\n        holdout_df (pd.DataFrame): DataFrame reserved as holdout set, unused in CV.\n        df (pd.DataFrame): DataFrame for training and cross-validation.\n    \"\"\"\n\n    dataset_length_week= ((df.index[-1] - df.index[0]).total_seconds() / 86400/7)\n    block_length = int(dataset_length_week / n_block)\n    consecutive_timedelta = df.index[1] - df.index[0]\n    n_timestep_per_week = int(one_week / consecutive_timedelta)\n    holdout_start = (n_block - 1)* block_length * n_timestep_per_week\n    holdout_df = df.iloc[holdout_start:]\n    df = df.drop(df.index[holdout_start:])\n\n    return block_length, holdout_df, df\n</code></pre>"},{"location":"api_reference/#docs.notebooks.config.general_functions.split_time_series","title":"<code>split_time_series(df, cv_no)</code>","text":"<p>Split dataset into training and validation sets using blocked cross-validation.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame containing features (X) and target (y).</p> required <code>cv_no</code> <code>int</code> <p>Cross-validation fold number. - cv_no=1 \u2192 test set is the last block. - cv_no=k \u2192 test set is the first block.  </p> required <p>Returns:</p> Name Type Description <code>train_df</code> <code>DataFrame</code> <p>Subset used for training.</p> <code>test_df</code> <code>DataFrame</code> <p>Subset used for validation (dev/test set).</p> Source code in <code>docs\\notebooks\\config\\general_functions.py</code> <pre><code>def split_time_series(df, cv_no):\n    \"\"\"Split dataset into training and validation sets using blocked cross-validation.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame containing features (X) and target (y).\n        cv_no (int): Cross-validation fold number.\n            - cv_no=1 \u2192 test set is the last block.  \n            - cv_no=k \u2192 test set is the first block.  \n\n    Returns:\n        train_df (pd.DataFrame): Subset used for training.\n        test_df (pd.DataFrame): Subset used for validation (dev/test set).\n    \"\"\"\n\n    n = len(df)\n    test_start = int(n*(1 - cv_no*test_pct))\n    test_end = int(n*(1 - (cv_no-1)*test_pct))\n\n    test_df = df.iloc[test_start:test_end]\n    train_df = df.drop(df.index[test_start:test_end])\n\n    return train_df, test_df\n</code></pre>"},{"location":"api_reference/#docs.notebooks.config.general_functions.split_xy","title":"<code>split_xy(df)</code>","text":"<p>Separate target variable y and predictors X into two DataFrames.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing target y and predictors X.</p> required <p>Returns:</p> Name Type Description <code>df_X</code> <code>DataFrame</code> <p>DataFrame of predictors X.</p> <code>df_y</code> <code>DataFrame</code> <p>DataFrame of target variable y.</p> Source code in <code>docs\\notebooks\\config\\general_functions.py</code> <pre><code>def split_xy(df):\n    \"\"\"Separate target variable y and predictors X into two DataFrames.\n\n    Args:\n        df (pd.DataFrame): DataFrame containing target y and predictors X.\n\n    Returns:\n        df_X (pd.DataFrame): DataFrame of predictors X.\n        df_y (pd.DataFrame): DataFrame of target variable y.\n    \"\"\"\n\n    df_y = df[['y']]\n    df_X = df.drop(\"y\", axis=1)\n\n    return df_X, df_y\n</code></pre>"},{"location":"api_reference/#docs.notebooks.config.general_functions.timeplot_forecast","title":"<code>timeplot_forecast(observation, forecast, pathname)</code>","text":"<p>Generate a time plot of observed vs. forecast values.</p> <p>The function plots the last week of data (based on the observation  index frequency) and saves the figure as a PNG.</p> <p>Parameters:</p> Name Type Description Default <code>observation</code> <code>Series or DataFrame</code> <p>Observed values with datetime index.</p> required <code>forecast</code> <code>Series or DataFrame</code> <p>Forecasted values with datetime index.</p> required <code>pathname</code> <code>str</code> <p>File path to save the plot as PNG.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>docs\\notebooks\\config\\general_functions.py</code> <pre><code>def timeplot_forecast(observation, forecast, pathname):\n    \"\"\"Generate a time plot of observed vs. forecast values.\n\n    The function plots the last week of data (based on the observation \n    index frequency) and saves the figure as a PNG.\n\n    Args:\n        observation (pd.Series or pd.DataFrame): Observed values with datetime index.\n        forecast (pd.Series or pd.DataFrame): Forecasted values with datetime index.\n        pathname (str): File path to save the plot as PNG.\n\n    Returns:\n        None\n    \"\"\"\n    consecutive_timedelta = observation.index[-1] - observation.index[-2]\n    # Calculate total minutes in a week\n    minutes_per_week = 7 * 24 * 60  # 7 days * 24 hours * 60 minutes\n\n    # Calculate the number of minutes per timestep\n    minutes_per_timestep = consecutive_timedelta.total_seconds() / 60  # convert seconds to minutes\n\n    # Compute the number of timesteps in a week\n    timesteps_per_week = int(minutes_per_week / minutes_per_timestep)\n\n    # Create the figure with specified size\n    plt.figure(figsize=(9, 9))\n\n    # Set background color\n    # plt.gcf().patch.set_facecolor(platinum)\n\n    # Plot the actual and forecast data\n    plt.plot(observation[-timesteps_per_week:], color=dark_blue, label='Actual')\n    plt.plot(forecast[-timesteps_per_week:], color=orange, label='Forecast')\n\n    # Set the font to Arial\n    plt.rcParams['font.family'] = 'Arial'\n\n    # Remove grid lines\n    plt.grid(False)\n\n    # Set tick marks for x and y axis\n    plt.xticks(fontsize=12, color=dark_blue, alpha=0.5, rotation=30)\n    plt.yticks(fontsize=12, color=dark_blue, alpha=0.5)\n\n    # Add borders to the plot\n    plt.gca().spines['top'].set_color(dark_blue)\n    plt.gca().spines['right'].set_color(dark_blue)\n    plt.gca().spines['bottom'].set_color(dark_blue)\n    plt.gca().spines['left'].set_color(dark_blue)\n\n    # Remove the tick markers (the small lines)\n    plt.tick_params(axis='x', which='both', length=0)  # Remove x-axis tick markers\n    plt.tick_params(axis='y', which='both', length=0)  # Remove y-axis tick markers\n\n    # Set axis titles\n    plt.xlabel('Time', fontsize=14, color=dark_blue)\n    plt.ylabel('Net Load (kW)', fontsize=14, color=dark_blue)\n\n    # Remove title\n    plt.title('')\n\n    plt.legend(loc='upper left', fontsize=12, frameon=False, labelspacing=1, bbox_to_anchor=(1, 1))\n\n    plt.savefig(pathname, format='png', bbox_inches='tight')\n    plt.close()\n</code></pre>"},{"location":"api_reference/#docs.notebooks.config.general_functions.timeplot_residual","title":"<code>timeplot_residual(residual, pathname)</code>","text":"<p>Produce time plot of residual; value and save it on the designated folder</p> <p>Parameters:</p> Name Type Description Default <code>residual</code> <code>df</code> <p>forecast - observation</p> required <code>pathname</code> <code>str</code> <p>filepath to save the figure</p> required Source code in <code>docs\\notebooks\\config\\general_functions.py</code> <pre><code>def timeplot_residual(residual, pathname):\n    \"\"\"Produce time plot of residual; value and save it on the designated folder\n\n    Args:\n        residual (df): forecast - observation\n        pathname (str): filepath to save the figure\n    \"\"\"\n    consecutive_timedelta = residual.index[-1] - residual.index[-2]\n    # Calculate total minutes in a week\n    minutes_per_week = 7 * 24 * 60  # 7 days * 24 hours * 60 minutes\n\n    # Calculate the number of minutes per timestep\n    minutes_per_timestep = consecutive_timedelta.total_seconds() / 60  # convert seconds to minutes\n\n    # Compute the number of timesteps in a week\n    timesteps_per_week = int(minutes_per_week / minutes_per_timestep)\n\n    # Create the figure with specified size\n    plt.figure(figsize=(9, 9))\n\n    # Set background color\n    # plt.gcf().patch.set_facecolor(platinum)\n\n    # Plot the actual and forecast data\n    plt.plot(residual[-timesteps_per_week:], color=dark_blue, label='Actual')\n\n    # Set the font to Arial\n    plt.rcParams['font.family'] = 'Arial'\n\n    # Remove grid lines\n    plt.grid(False)\n\n    # Set tick marks for x and y axis\n    plt.xticks(fontsize=12, color=dark_blue, alpha=0.5, rotation=30)\n    plt.yticks(fontsize=12, color=dark_blue, alpha=0.5)\n\n    # Add borders to the plot\n    plt.gca().spines['top'].set_color(dark_blue)\n    plt.gca().spines['right'].set_color(dark_blue)\n    plt.gca().spines['bottom'].set_color(dark_blue)\n    plt.gca().spines['left'].set_color(dark_blue)\n\n    # Remove the tick markers (the small lines)\n    plt.tick_params(axis='x', which='both', length=0)  # Remove x-axis tick markers\n    plt.tick_params(axis='y', which='both', length=0)  # Remove y-axis tick markers\n\n    # Set axis titles\n    plt.xlabel('Time', fontsize=14, color=dark_blue)\n    plt.ylabel('Forecast Residual (kW)', fontsize=14, color=dark_blue)\n\n    # Remove title\n    plt.title('')\n\n    plt.savefig(pathname, format='png', bbox_inches='tight')\n    plt.close()\n</code></pre>"},{"location":"api_reference/#docs.notebooks.config.general_functions.train_model","title":"<code>train_model(model_name, hyperparameter, train_df_X, train_df_y, forecast_horizon)</code>","text":"<p>Train a forecasting model given its identifier and data.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Model identifier (e.g., 'm6_lr').</p> required <code>hyperparameter</code> <code>Series</code> <p>Hyperparameters for the model.</p> required <code>train_df_X</code> <code>DataFrame</code> <p>Predictor matrix.</p> required <code>train_df_y</code> <code>DataFrame</code> <p>Target variable (y).</p> required <code>forecast_horizon</code> <code>int</code> <p>Forecast horizon in minutes.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Trained model object containing predictors, settings, and metadata.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported model_name is provided.</p> Source code in <code>docs\\notebooks\\config\\general_functions.py</code> <pre><code>def train_model(model_name, hyperparameter, train_df_X, train_df_y, forecast_horizon):\n    \"\"\"Train a forecasting model given its identifier and data.\n\n    Args:\n        model_name (str): Model identifier (e.g., 'm6_lr').\n        hyperparameter (pd.Series): Hyperparameters for the model.\n        train_df_X (pd.DataFrame): Predictor matrix.\n        train_df_y (pd.DataFrame): Target variable (y).\n        forecast_horizon (int): Forecast horizon in minutes.\n\n    Returns:\n        dict: Trained model object containing predictors, settings, and metadata.\n\n    Raises:\n        ValueError: If an unsupported model_name is provided.\n    \"\"\"\n\n    if model_name == 'm1_naive':\n        model = train_model_m1_naive(hyperparameter, train_df_X, train_df_y)\n    elif model_name == 'm2_snaive':\n        model = train_model_m2_snaive(hyperparameter, train_df_X, train_df_y)\n    elif model_name == 'm3_ets':\n        model = train_model_m3_ets(hyperparameter, train_df_X, train_df_y, forecast_horizon)\n    elif model_name == 'm4_arima':\n        model = train_model_m4_arima(hyperparameter, train_df_X, train_df_y, forecast_horizon)\n    elif model_name == 'm5_sarima':\n        model = train_model_m5_sarima(hyperparameter, train_df_X, train_df_y, forecast_horizon)\n    elif model_name == 'm6_lr':\n        model = train_model_m6_lr(hyperparameter, train_df_X, train_df_y)\n    elif model_name == 'm7_ann':\n        model = train_model_m7_ann(hyperparameter, train_df_X, train_df_y)\n    elif model_name == 'm8_dnn':\n        model = train_model_m8_dnn(hyperparameter, train_df_X, train_df_y)\n    elif model_name == 'm9_rt':\n        model = train_model_m9_rt(hyperparameter, train_df_X, train_df_y)\n    elif model_name == 'm10_rf':\n        model = train_model_m10_rf(hyperparameter, train_df_X, train_df_y)\n    elif model_name == 'm11_svr':\n        model = train_model_m11_svr(hyperparameter, train_df_X, train_df_y)\n    elif model_name == 'm12_rnn':\n        model = train_model_m12_rnn(hyperparameter, train_df_X, train_df_y)\n    elif model_name == 'm13_lstm':\n        model = train_model_m13_lstm(hyperparameter, train_df_X, train_df_y)\n    elif model_name == 'm14_gru':\n        model = train_model_m14_gru(hyperparameter, train_df_X, train_df_y)\n    elif model_name == 'm15_transformer':\n        model = train_model_m15_transformer(hyperparameter, train_df_X, train_df_y)\n    elif model_name == 'm16_prophet':\n        model = train_model_m16_prophet(hyperparameter, train_df_X, train_df_y, forecast_horizon)\n    elif model_name == 'm17_xgb':\n        model = train_model_m17_xgb(hyperparameter, train_df_X, train_df_y)\n    elif model_name == 'm18_nbeats':\n        model = train_model_m18_nbeats(hyperparameter, train_df_X, train_df_y)\n    else:\n        raise ValueError(\n            \"Wrong Model Choice! Available models are: m1_naive, m2_snaive, m3_ets, m4_arima, m5_sarima, m6_lr, m7_ann, m8_dnn, m9_rt, m10_rf, m11_svr, m12_rnn, m13_lstm, m14_gru, m15_transformer, m16_prophet, m17_xgb, m18_nbeats\"\n        )\n\n    return model\n</code></pre>"},{"location":"api_reference/#folder-model","title":"Folder <code>Model/</code>","text":""},{"location":"api_reference/#docs.notebooks.model.m1_naive.produce_forecast_m1_naive","title":"<code>produce_forecast_m1_naive(model, train_df_X, test_df_X)</code>","text":"<p>Generate naive forecasts for training and test sets using lagged values.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>dict</code> <p>Parameters of the trained model.</p> required <code>train_df_X</code> <code>DataFrame</code> <p>Predictor data for the training set.</p> required <code>test_df_X</code> <code>DataFrame</code> <p>Predictor data for the test set.</p> required <p>Returns:</p> Name Type Description <code>train_df_y_hat</code> <code>Series</code> <p>Forecast results for the training set.</p> <code>test_df_y_hat</code> <code>Series</code> <p>Forecast results for the test set.</p> Source code in <code>docs\\notebooks\\model\\m1_naive.py</code> <pre><code>def produce_forecast_m1_naive(model, train_df_X, test_df_X):\n    \"\"\"Generate naive forecasts for training and test sets using lagged values.\n\n    Args:\n        model (dict): Parameters of the trained model.\n        train_df_X (pd.DataFrame): Predictor data for the training set.\n        test_df_X (pd.DataFrame): Predictor data for the test set.\n\n    Returns:\n        train_df_y_hat (pd.Series): Forecast results for the training set.\n        test_df_y_hat (pd.Series): Forecast results for the test set.\n    \"\"\"\n\n    # PRODUCE FORECAST\n    horizon_timedelta = pd.Timedelta(minutes=forecast_horizon)\n    last_observation = f'y_lag_{horizon_timedelta}m'\n    train_df_y_hat = train_df_X[last_observation]\n    test_df_y_hat = test_df_X[last_observation]\n\n    return train_df_y_hat, test_df_y_hat\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m1_naive.train_model_m1_naive","title":"<code>train_model_m1_naive(hyperparameter, train_df_X, train_df_y)</code>","text":"<p>Train a naive model for point forecasting.</p> <p>For the naive model, no actual training is required. The model  simply stores configuration (if any) and will produce lagged forecasts.</p> <p>Parameters:</p> Name Type Description Default <code>hyperparameter</code> <code>DataFrame</code> <p>Hyperparameter values (e.g., number of features).</p> required <code>train_df_X</code> <code>DataFrame</code> <p>Features matrix for training.</p> required <code>train_df_y</code> <code>DataFrame</code> <p>Target values for training.</p> required <p>Returns:</p> Name Type Description <code>model</code> <code>dict</code> <p>Trained naive model object containing all features.</p> Source code in <code>docs\\notebooks\\model\\m1_naive.py</code> <pre><code>def train_model_m1_naive(hyperparameter, train_df_X, train_df_y):\n    \"\"\"Train a naive model for point forecasting.\n\n    For the naive model, no actual training is required. The model \n    simply stores configuration (if any) and will produce lagged forecasts.\n\n    Args:\n        hyperparameter (pd.DataFrame): Hyperparameter values (e.g., number of features).\n        train_df_X (pd.DataFrame): Features matrix for training.\n        train_df_y (pd.DataFrame): Target values for training.\n\n    Returns:\n        model (dict): Trained naive model object containing all features.\n    \"\"\"\n\n    #UNPACK HYPERPARAMETER\n    #no hyperparameter for naive model\n\n    #TRAIN MODEL\n    #no training is required for naive model\n\n    # PACK MODEL\n    model = {}\n\n\n    return model\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m2_snaive.produce_forecast_m2_snaive","title":"<code>produce_forecast_m2_snaive(model, train_df_X, test_df_X)</code>","text":"<p>Generate seasonal naive forecasts for training and test sets.</p> <p>The seasonal naive model forecasts using the value from the same period  in the previous season (e.g., same day in prior week).</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>dict</code> <p>Trained seasonal naive model containing lagged column info.</p> required <code>train_df_X</code> <code>DataFrame</code> <p>Predictor data for the training set.</p> required <code>test_df_X</code> <code>DataFrame</code> <p>Predictor data for the test set.</p> required <p>Returns:</p> Name Type Description <code>train_df_y_hat</code> <code>Series</code> <p>Forecast results for the training set.</p> <code>test_df_y_hat</code> <code>Series</code> <p>Forecast results for the test set.</p> Source code in <code>docs\\notebooks\\model\\m2_snaive.py</code> <pre><code>def produce_forecast_m2_snaive(model, train_df_X, test_df_X):\n    \"\"\"Generate seasonal naive forecasts for training and test sets.\n\n    The seasonal naive model forecasts using the value from the same period \n    in the previous season (e.g., same day in prior week).\n\n    Args:\n        model (dict): Trained seasonal naive model containing lagged column info.\n        train_df_X (pd.DataFrame): Predictor data for the training set.\n        test_df_X (pd.DataFrame): Predictor data for the test set.\n\n    Returns:\n        train_df_y_hat (pd.Series): Forecast results for the training set.\n        test_df_y_hat (pd.Series): Forecast results for the test set.\n    \"\"\"\n\n    # UNPACK MODEL\n    col_name = model['col_name']  #this depends on the lag day\n\n    # PRODUCE FORECAST\n    train_df_y_hat = train_df_X[col_name]\n    test_df_y_hat = test_df_X[col_name]\n\n    return train_df_y_hat, test_df_y_hat\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m2_snaive.train_model_m2_snaive","title":"<code>train_model_m2_snaive(hyperparameter, train_df_X, train_df_y)</code>","text":"<p>Train a seasonal naive model for point forecasting.</p> <p>The seasonal naive model does not require training; it forecasts  using lagged values based on seasonal periods.</p> <p>Parameters:</p> Name Type Description Default <code>hyperparameter</code> <code>DataFrame</code> <p>Hyperparameter values, e.g., number of days for seasonality.</p> required <code>train_df_X</code> <code>DataFrame</code> <p>Features matrix for training.</p> required <code>train_df_y</code> <code>DataFrame</code> <p>Target values for training.</p> required <p>Returns:</p> Name Type Description <code>model</code> <code>dict</code> <p>Trained seasonal naive model object containing the lagged column name.</p> Source code in <code>docs\\notebooks\\model\\m2_snaive.py</code> <pre><code>def train_model_m2_snaive(hyperparameter, train_df_X, train_df_y):\n    \"\"\"Train a seasonal naive model for point forecasting.\n\n    The seasonal naive model does not require training; it forecasts \n    using lagged values based on seasonal periods.\n\n    Args:\n        hyperparameter (pd.DataFrame): Hyperparameter values, e.g., number of days for seasonality.\n        train_df_X (pd.DataFrame): Features matrix for training.\n        train_df_y (pd.DataFrame): Target values for training.\n\n    Returns:\n        model (dict): Trained seasonal naive model object containing the lagged column name.\n    \"\"\"\n\n    #UNPACK HYPERPARAMETER\n    days = hyperparameter['days']\n    col_name = f'y_lag_{days} days 00:00:00m'\n\n    #TRAIN MODEL\n    #no training is required for seasonal naive model\n\n    # PACK MODEL\n    model = {\"col_name\": col_name }\n\n\n    return model\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m3_ets.produce_forecast_m3_ets","title":"<code>produce_forecast_m3_ets(model, train_df_X, test_df_X, forecast_horizon)</code>","text":"<p>Generate forecasts for training and test sets using a fitted ETS model.</p> <p>The function produces fitted values for the training set and  horizon-based forecasts for the test set, handling gaps and  timestep frequency. Test forecasts are skipped if the test set  precedes the training set (e.g., in certain CV folds).</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>dict</code> <p>Trained ETS model containing the fitted model object.</p> required <code>train_df_X</code> <code>DataFrame</code> <p>Predictor data for the training set.</p> required <code>test_df_X</code> <code>DataFrame</code> <p>Predictor data for the test set.</p> required <code>forecast_horizon</code> <code>int</code> <p>Forecast horizon in minutes.</p> required <p>Returns:</p> Name Type Description <code>train_df_y_hat</code> <code>DataFrame</code> <p>Forecasted values for the training set.</p> <code>test_df_y_hat</code> <code>DataFrame</code> <p>Forecasted values for the test set.</p> Source code in <code>docs\\notebooks\\model\\m3_ets.py</code> <pre><code>def produce_forecast_m3_ets(model, train_df_X, test_df_X, forecast_horizon):\n    \"\"\"Generate forecasts for training and test sets using a fitted ETS model.\n\n    The function produces fitted values for the training set and \n    horizon-based forecasts for the test set, handling gaps and \n    timestep frequency. Test forecasts are skipped if the test set \n    precedes the training set (e.g., in certain CV folds).\n\n    Args:\n        model (dict): Trained ETS model containing the fitted model object.\n        train_df_X (pd.DataFrame): Predictor data for the training set.\n        test_df_X (pd.DataFrame): Predictor data for the test set.\n        forecast_horizon (int): Forecast horizon in minutes.\n\n    Returns:\n        train_df_y_hat (pd.DataFrame): Forecasted values for the training set.\n        test_df_y_hat (pd.DataFrame): Forecasted values for the test set.\n    \"\"\"\n\n    timestep_frequency = test_df_X.index[1] - test_df_X.index[0]\n    n_timestep_forecast_horizon = int(forecast_horizon / (timestep_frequency.total_seconds() / 60))\n\n\n    train_df_X_updated = remove_jump_df(train_df_X)\n    test_df_X_updated = remove_jump_df(test_df_X)\n\n    # UNPACK MODEL\n    model_fitted = model['model_fitted']\n\n    # PRODUCE FORECAST FOR TRAIN SET\n    train_df_y_hat = pd.DataFrame(model_fitted.fittedvalues)\n    train_df_y_hat.columns = ['y']\n\n    # train_df_y_hat_2 = pd.DataFrame(model_fitted.forecast(n_timestep_forecast_horizon-1))\n    # train_df_y_hat_2.columns = ['y']\n    # train_df_y_hat = pd.concat([train_df_y_hat, train_df_y_hat_2])\n\n    train_df_y_hat.index.name = 'datetime'\n\n    # TRANSFORM test_df_X to a series with only the last lag\n    horizon_timedelta = pd.Timedelta(minutes=forecast_horizon)\n    last_observation = f'y_lag_{horizon_timedelta}m'\n    test_df_y_last = test_df_X[last_observation]\n\n\n    # REFIT THE MODEL AND PRODUCE NEW FORECAST FOR TEST SET\n    # THIS CODE RESULTS IN 2 MINS\n    test_df_y_hat = pd.DataFrame(index = test_df_X.index)\n    test_df_y_hat['y_hat'] = np.nan\n\n\n    # in the case of CV 10, which is when test df &lt; train df\n    # don't compute the test forecast\n    if (test_df_X.index[-1] &lt; train_df_X.index[0]):\n    # this is the case when we use CV10, where the test set is before the train set\n        print(\"Test set is before train set / CV 10, no test forecast can be made\")\n        return train_df_y_hat, test_df_y_hat\n\n    for i in range(len(test_df_y_last)):\n    # for i in range(2): #for test only\n        print('Processing i = ', i + 1, ' out of ', len(test_df_y_last)),\n        if i == 0:\n            test_df_y_hat.iloc[i, 0] = model_fitted.forecast(steps=n_timestep_forecast_horizon).iloc[-1]\n        else:\n            new_row = pd.DataFrame([test_df_y_last.values[i]], columns=['y'], index=[test_df_y_last.index[i] - dt.timedelta(minutes=forecast_horizon)])\n            new_row = new_row.asfreq(test_df_X_updated.index.freq)\n\n            model_fitted = model_fitted.append(new_row)\n            test_df_y_hat.iloc[i, 0] = model_fitted.forecast(steps=n_timestep_forecast_horizon).iloc[-1] # to update based on the forecast horizon\n\n    # test_df_y_hat = m06_lr.predict(test_df_X)\n\n    return train_df_y_hat, test_df_y_hat\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m3_ets.train_model_m3_ets","title":"<code>train_model_m3_ets(hyperparameter, train_df_X, train_df_y, forecast_horizon)</code>","text":"<p>Train an Exponential Smoothing (ETS) model for point forecasting.</p> <p>Uses statsmodels' ExponentialSmoothing for trend forecasting. Handles  timestep frequency, removes sudden jumps, and introduces a gap to avoid data leakage.</p> <p>Parameters:</p> Name Type Description Default <code>hyperparameter</code> <code>DataFrame</code> <p>Model hyperparameters including trend, damped_trend, and seasonal_periods_days.</p> required <code>train_df_X</code> <code>DataFrame</code> <p>Features matrix for training (unused for ETS).</p> required <code>train_df_y</code> <code>DataFrame</code> <p>Target series for training.</p> required <code>forecast_horizon</code> <code>int</code> <p>Forecast horizon in minutes.</p> required <p>Returns:</p> Name Type Description <code>model</code> <code>dict</code> <p>Trained ETS model object containing fitted model.</p> Source code in <code>docs\\notebooks\\model\\m3_ets.py</code> <pre><code>def train_model_m3_ets(hyperparameter, train_df_X, train_df_y, forecast_horizon):\n    \"\"\"Train an Exponential Smoothing (ETS) model for point forecasting.\n\n    Uses statsmodels' ExponentialSmoothing for trend forecasting. Handles \n    timestep frequency, removes sudden jumps, and introduces a gap to avoid data leakage.\n\n    Args:\n        hyperparameter (pd.DataFrame): Model hyperparameters including trend, damped_trend, and seasonal_periods_days.\n        train_df_X (pd.DataFrame): Features matrix for training (unused for ETS).\n        train_df_y (pd.DataFrame): Target series for training.\n        forecast_horizon (int): Forecast horizon in minutes.\n\n    Returns:\n        model (dict): Trained ETS model object containing fitted model.\n    \"\"\"\n\n    #UNPACK HYPERPARAMETER\n    trend = hyperparameter['trend']\n    damped_trend = hyperparameter['damped_trend']\n    seasonal_periods_days = hyperparameter['seasonal_periods_days']\n\n    # UPDATE train_df_y to exclude all rows after a sudden jump in the timestep\n    train_df_y_updated = remove_jump_df(train_df_y)\n\n    # TRAIN MODEL\n    # Calculate the frequency of the timesteps using the first and second index values\n    timestep_frequency = train_df_y_updated.index[1] - train_df_y_updated.index[0]\n    inferred_frequency = pd.infer_freq(train_df_y_updated.index)\n    train_df_y_updated = train_df_y_updated.asfreq(inferred_frequency) \n\n    # INTRODUCE GAP BETWEEN TRAIN AND TEST SET TO AVOID DATA LEAKAGE\n    n_timestep_forecast_horizon = int(forecast_horizon / (timestep_frequency.total_seconds() / 60))\n    if n_timestep_forecast_horizon == 1:\n        pass\n    else:\n        train_df_y_updated = train_df_y_updated[:-(n_timestep_forecast_horizon - 1)]\n\n    # Assuming train_df_y_updated is your dataframe and 'y' is the column with the training series\n    y = train_df_y_updated['y']\n\n   # Build and fit the state-space Exponential Smoothing model\n    model_fitted = ExponentialSmoothing(\n        y,\n        trend=trend,\n        seasonal=None, #can be updated later\n        damped_trend=damped_trend\n    ).fit()\n\n\n    # Print the model summary\n    # print(model_fitted.summary())\n\n    # PACK MODEL\n    model = {\"model_fitted\": model_fitted}\n\n\n    return model\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m4_arima.produce_forecast_m4_arima","title":"<code>produce_forecast_m4_arima(model, train_df_X, test_df_X, forecast_horizon)</code>","text":"<p>Generate forecasts for training and test sets using a fitted ARIMA model.</p> <p>Produces fitted values for the training set and horizon-based forecasts for the test set. Handles timestep frequency, gaps, and skips test forecasts if the test set precedes the training set (e.g., in certain CV folds).</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>dict</code> <p>Trained ARIMA model containing the fitted model object.</p> required <code>train_df_X</code> <code>DataFrame</code> <p>Predictor data for the training set.</p> required <code>test_df_X</code> <code>DataFrame</code> <p>Predictor data for the test set.</p> required <code>forecast_horizon</code> <code>int</code> <p>Forecast horizon in minutes.</p> required <p>Returns:</p> Name Type Description <code>train_df_y_hat</code> <code>DataFrame</code> <p>Forecasted values for the training set.</p> <code>test_df_y_hat</code> <code>DataFrame</code> <p>Forecasted values for the test set.</p> Source code in <code>docs\\notebooks\\model\\m4_arima.py</code> <pre><code>def produce_forecast_m4_arima(model, train_df_X, test_df_X, forecast_horizon):\n    \"\"\"Generate forecasts for training and test sets using a fitted ARIMA model.\n\n    Produces fitted values for the training set and horizon-based forecasts\n    for the test set. Handles timestep frequency, gaps, and skips test forecasts\n    if the test set precedes the training set (e.g., in certain CV folds).\n\n    Args:\n        model (dict): Trained ARIMA model containing the fitted model object.\n        train_df_X (pd.DataFrame): Predictor data for the training set.\n        test_df_X (pd.DataFrame): Predictor data for the test set.\n        forecast_horizon (int): Forecast horizon in minutes.\n\n    Returns:\n        train_df_y_hat (pd.DataFrame): Forecasted values for the training set.\n        test_df_y_hat (pd.DataFrame): Forecasted values for the test set.\n    \"\"\"\n    timestep_frequency = test_df_X.index[1] - test_df_X.index[0]\n    n_timestep_forecast_horizon = int(forecast_horizon / (timestep_frequency.total_seconds() / 60))\n\n    train_df_X_updated = remove_jump_df(train_df_X)\n    test_df_X_updated = remove_jump_df(test_df_X)\n\n    # UNPACK MODEL\n    model_fitted = model['model_fitted']\n\n    # PRODUCE FORECAST FOR TRAIN SET\n    train_df_y_hat = pd.DataFrame(model_fitted.fittedvalues)\n    train_df_y_hat.columns = ['y']\n\n    # train_df_y_hat_2 = pd.DataFrame(model_fitted.forecast(n_timestep_forecast_horizon-1))\n    # train_df_y_hat_2.columns = ['y']\n    # train_df_y_hat = pd.concat([train_df_y_hat, train_df_y_hat_2])\n\n    train_df_y_hat.index.name = 'datetime'\n\n    # TRANSFORM test_df_X to a series with only the last lag\n    horizon_timedelta = pd.Timedelta(minutes=forecast_horizon)\n    last_observation = f'y_lag_{horizon_timedelta}m'\n    test_df_y_last = test_df_X[last_observation]\n\n\n    # REFIT THE MODEL AND PRODUCE NEW FORECAST FOR TEST SET\n    # THIS CODE RESULTS IN 2 MINS\n    test_df_y_hat = pd.DataFrame(index = test_df_X.index)\n    test_df_y_hat['y_hat'] = np.nan\n\n    # in the case of CV 10, which is when test df &lt; train df\n    # don't compute the test forecast\n    if (test_df_X.index[-1] &lt; train_df_X.index[0]):\n    # this is the case when we use CV10, where the test set is before the train set\n        print(\"Test set is before train set / CV 10, no test forecast can be made\")\n        return train_df_y_hat, test_df_y_hat\n\n    for i in range(len(test_df_y_last)):\n    # for i in range(2): #for test only\n        print('Processing i = ', i + 1, ' out of ', len(test_df_y_last)),\n        if i == 0:\n            test_df_y_hat.iloc[i, 0] = model_fitted.forecast(steps=n_timestep_forecast_horizon).iloc[-1]\n        else:\n            new_row = pd.DataFrame([test_df_y_last.values[i]], columns=['y'], index=[test_df_y_last.index[i] - dt.timedelta(minutes=forecast_horizon)])\n            new_row = new_row.asfreq(test_df_X_updated.index.freq)\n\n            model_fitted = model_fitted.append(new_row)\n            test_df_y_hat.iloc[i, 0] = model_fitted.forecast(steps=n_timestep_forecast_horizon).iloc[-1] # to update based on the forecast horizon\n\n\n    # test_df_y_hat = m06_lr.predict(test_df_X)\n\n    return train_df_y_hat, test_df_y_hat\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m4_arima.train_model_m4_arima","title":"<code>train_model_m4_arima(hyperparameter, train_df_X, train_df_y, forecast_horizon)</code>","text":"<p>Train an ARIMA model for point forecasting.</p> <p>Handles timestep frequency, removes sudden jumps, and introduces  a gap to avoid data leakage. Fits an ARIMA(p, d, q) model on the  target series.</p> <p>Parameters:</p> Name Type Description Default <code>hyperparameter</code> <code>DataFrame</code> <p>Hyperparameters including 'p', 'd', and 'q' for ARIMA.</p> required <code>train_df_X</code> <code>DataFrame</code> <p>Features matrix for training (unused for ARIMA).</p> required <code>train_df_y</code> <code>DataFrame</code> <p>Target series for training.</p> required <code>forecast_horizon</code> <code>int</code> <p>Forecast horizon in minutes.</p> required <p>Returns:</p> Name Type Description <code>model</code> <code>dict</code> <p>Trained ARIMA model object containing the fitted model.</p> Source code in <code>docs\\notebooks\\model\\m4_arima.py</code> <pre><code>def train_model_m4_arima(hyperparameter, train_df_X, train_df_y, forecast_horizon):\n    \"\"\"Train an ARIMA model for point forecasting.\n\n    Handles timestep frequency, removes sudden jumps, and introduces \n    a gap to avoid data leakage. Fits an ARIMA(p, d, q) model on the \n    target series.\n\n    Args:\n        hyperparameter (pd.DataFrame): Hyperparameters including 'p', 'd', and 'q' for ARIMA.\n        train_df_X (pd.DataFrame): Features matrix for training (unused for ARIMA).\n        train_df_y (pd.DataFrame): Target series for training.\n        forecast_horizon (int): Forecast horizon in minutes.\n\n    Returns:\n        model (dict): Trained ARIMA model object containing the fitted model.\n    \"\"\"\n\n    #UNPACK HYPERPARAMETER\n    p = hyperparameter['p']\n    d = hyperparameter['d']\n    q = hyperparameter['q']\n\n\n    # UPDATE train_df_y to exclude all rows after a sudden jump in the timestep\n    train_df_y_updated = remove_jump_df(train_df_y)\n\n    # TRAIN MODEL\n    # Calculate the frequency of the timesteps using the first and second index values\n    timestep_frequency = train_df_y_updated.index[1] - train_df_y_updated.index[0]\n    inferred_frequency = pd.infer_freq(train_df_y_updated.index)\n    train_df_y_updated = train_df_y_updated.asfreq(inferred_frequency)\n\n    # INTRODUCE GAP BETWEEN TRAIN AND TEST SET TO AVOID DATA LEAKAGE\n    n_timestep_forecast_horizon = int(forecast_horizon / (timestep_frequency.total_seconds() / 60))\n    if n_timestep_forecast_horizon == 1:\n        pass\n    else:\n        train_df_y_updated = train_df_y_updated[:-(n_timestep_forecast_horizon - 1)]\n\n    # Assuming train_df_y_updated is your dataframe and 'y' is the column with the training series\n    y = train_df_y_updated['y']\n\n    # Build and fit the state-space ARIMA model\n    model_fitted = ARIMA(y, order=(p, d, q), freq=inferred_frequency).fit()\n\n    # PACK MODEL\n    model = {\"model_fitted\": model_fitted}\n\n\n    return model\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m5_sarima.produce_forecast_m5_sarima","title":"<code>produce_forecast_m5_sarima(model, train_df_X, test_df_X, forecast_horizon)</code>","text":"<p>Generate forecasts for train and test sets using a trained SARIMA model.</p> <p>Handles timestep adjustments, sudden jumps, and gaps to avoid data leakage. Produces fitted values for the training set and step-wise forecasts for the test set.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>dict</code> <p>Trained SARIMA model object containing the fitted model.</p> required <code>train_df_X</code> <code>DataFrame</code> <p>Predictors of the training set (used to align timesteps).</p> required <code>test_df_X</code> <code>DataFrame</code> <p>Predictors of the test set (used to align timesteps).</p> required <code>forecast_horizon</code> <code>int</code> <p>Forecast horizon in minutes.</p> required <p>Returns:</p> Name Type Description <code>train_df_y_hat</code> <code>DataFrame</code> <p>Forecasted values for the training set.</p> <code>test_df_y_hat</code> <code>DataFrame</code> <p>Forecasted values for the test set.</p> Source code in <code>docs\\notebooks\\model\\m5_sarima.py</code> <pre><code>def produce_forecast_m5_sarima(model, train_df_X, test_df_X, forecast_horizon):\n    \"\"\"Generate forecasts for train and test sets using a trained SARIMA model.\n\n    Handles timestep adjustments, sudden jumps, and gaps to avoid data leakage.\n    Produces fitted values for the training set and step-wise forecasts for the test set.\n\n    Args:\n        model (dict): Trained SARIMA model object containing the fitted model.\n        train_df_X (pd.DataFrame): Predictors of the training set (used to align timesteps).\n        test_df_X (pd.DataFrame): Predictors of the test set (used to align timesteps).\n        forecast_horizon (int): Forecast horizon in minutes.\n\n    Returns:\n        train_df_y_hat (pd.DataFrame): Forecasted values for the training set.\n        test_df_y_hat (pd.DataFrame): Forecasted values for the test set.\n    \"\"\"\n    timestep_frequency = test_df_X.index[1] - test_df_X.index[0]\n    n_timestep_forecast_horizon = int(forecast_horizon / (timestep_frequency.total_seconds() / 60))\n\n    train_df_X_updated = remove_jump_df(train_df_X)\n    test_df_X_updated = remove_jump_df(test_df_X)\n\n    # UNPACK MODEL\n    model_fitted = model['model_fitted']\n\n    # PRODUCE FORECAST FOR TRAIN SET\n    train_df_y_hat = pd.DataFrame(model_fitted.fittedvalues)\n    train_df_y_hat.columns = ['y']\n\n    # train_df_y_hat_2 = pd.DataFrame(model_fitted.forecast(n_timestep_forecast_horizon-1))\n    # train_df_y_hat_2.columns = ['y']\n    # train_df_y_hat = pd.concat([train_df_y_hat, train_df_y_hat_2])\n\n    train_df_y_hat.index.name = 'datetime'\n\n    # TRANSFORM test_df_X to a series with only the last lag\n    horizon_timedelta = pd.Timedelta(minutes=forecast_horizon)\n    last_observation = f'y_lag_{horizon_timedelta}m'\n    test_df_y_last = test_df_X[last_observation]\n\n\n    # REFIT THE MODEL AND PRODUCE NEW FORECAST FOR TEST SET\n    # THIS CODE RESULTS IN 2 MINS\n    test_df_y_hat = pd.DataFrame(index = test_df_X.index)\n    test_df_y_hat['y_hat'] = np.nan\n\n    # in the case of CV 10, which is when test df &lt; train df\n    # don't compute the test forecast\n    if (test_df_X.index[-1] &lt; train_df_X.index[0]):\n    # this is the case when we use CV10, where the test set is before the train set\n        print(\"Test set is before train set / CV 10, no test forecast can be made\")\n        return train_df_y_hat, test_df_y_hat\n\n    for i in range(len(test_df_y_last)):\n    # for i in range(2): #for test only\n        print('Processing i = ', i + 1, ' out of ', len(test_df_y_last)),\n        if i == 0:\n            test_df_y_hat.iloc[i, 0] = model_fitted.forecast(steps=n_timestep_forecast_horizon).iloc[-1]\n        else:\n            new_row = pd.DataFrame([test_df_y_last.values[i]], columns=['y'], index=[test_df_y_last.index[i] - dt.timedelta(minutes=forecast_horizon)])\n            new_row = new_row.asfreq(test_df_X_updated.index.freq)\n\n            model_fitted = model_fitted.append(new_row)\n            test_df_y_hat.iloc[i, 0] = model_fitted.forecast(steps=n_timestep_forecast_horizon).iloc[-1] # to update based on the forecast horizon\n\n\n    # test_df_y_hat = m06_lr.predict(test_df_X)\n\n    return train_df_y_hat, test_df_y_hat\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m5_sarima.train_model_m5_sarima","title":"<code>train_model_m5_sarima(hyperparameter, train_df_X, train_df_y, forecast_horizon)</code>","text":"<p>Train a SARIMA model for point forecasting.</p> <p>Handles timestep frequency, removes sudden jumps, and introduces  a gap to avoid data leakage. Fits a seasonal ARIMA (SARIMA) model  with specified non-seasonal and seasonal orders.</p> <p>Parameters:</p> Name Type Description Default <code>hyperparameter</code> <code>DataFrame</code> <p>Hyperparameters including 'p', 'd', 'q', 'P', 'D', 'Q', and seasonal_period_days.</p> required <code>train_df_X</code> <code>DataFrame</code> <p>Features matrix for training (unused for SARIMA).</p> required <code>train_df_y</code> <code>DataFrame</code> <p>Target series for training.</p> required <code>forecast_horizon</code> <code>int</code> <p>Forecast horizon in minutes.</p> required <p>Returns:</p> Name Type Description <code>model</code> <code>dict</code> <p>Trained SARIMA model object containing the fitted model.</p> Source code in <code>docs\\notebooks\\model\\m5_sarima.py</code> <pre><code>def train_model_m5_sarima(hyperparameter, train_df_X, train_df_y, forecast_horizon):\n    \"\"\"Train a SARIMA model for point forecasting.\n\n    Handles timestep frequency, removes sudden jumps, and introduces \n    a gap to avoid data leakage. Fits a seasonal ARIMA (SARIMA) model \n    with specified non-seasonal and seasonal orders.\n\n    Args:\n        hyperparameter (pd.DataFrame): Hyperparameters including 'p', 'd', 'q', 'P', 'D', 'Q', and seasonal_period_days.\n        train_df_X (pd.DataFrame): Features matrix for training (unused for SARIMA).\n        train_df_y (pd.DataFrame): Target series for training.\n        forecast_horizon (int): Forecast horizon in minutes.\n\n    Returns:\n        model (dict): Trained SARIMA model object containing the fitted model.\n    \"\"\"\n\n    #UNPACK HYPERPARAMETER\n    p = hyperparameter['p']\n    d = hyperparameter['d']\n    q = hyperparameter['q']\n    P = hyperparameter['P']\n    D = hyperparameter['D']\n    Q = hyperparameter['Q']\n    seasonal_period_days = hyperparameter['seasonal_period_days']\n\n\n    # UPDATE train_df_y to exclude all rows after a sudden jump in the timestep\n    train_df_y_updated = remove_jump_df(train_df_y)\n\n    # TRAIN MODEL\n    # Calculate the frequency of the timesteps using the first and second index values\n    timestep_frequency = train_df_y_updated.index[1] - train_df_y_updated.index[0]\n    s = int(seasonal_period_days * 24 * 60 / (timestep_frequency.seconds / 60))\n    inferred_frequency = pd.infer_freq(train_df_y_updated.index)\n    train_df_y_updated = train_df_y_updated.asfreq(inferred_frequency)\n\n    # INTRODUCE GAP BETWEEN TRAIN AND TEST SET TO AVOID DATA LEAKAGE\n    n_timestep_forecast_horizon = int(forecast_horizon / (timestep_frequency.total_seconds() / 60))\n    if n_timestep_forecast_horizon == 1:\n        pass\n    else:\n        train_df_y_updated = train_df_y_updated[:-(n_timestep_forecast_horizon - 1)]\n\n    # Assuming train_df_y_updated is your dataframe and 'y' is the column with the training series\n    y = train_df_y_updated['y']\n\n    # Build and fit the state-space ARIMA model\n    model_fitted = SARIMAX(y, order=(p, d, q), seasonal_order = (P, D, Q, s), freq=inferred_frequency).fit()\n\n    # PACK MODEL\n    model = {\"model_fitted\": model_fitted}\n\n\n    return model\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m6_lr.produce_forecast_m6_lr","title":"<code>produce_forecast_m6_lr(model, train_df_X, test_df_X)</code>","text":"<p>Generate forecasts for train and test sets using a trained linear regression model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>dict</code> <p>Contains the trained feature selector and linear regression model: - 'feature_selector': SelectKBest object for feature selection. - 'regression_model': Fitted LinearRegression object.</p> required <code>train_df_X</code> <code>DataFrame</code> <p>Predictor variables for the training set.</p> required <code>test_df_X</code> <code>DataFrame</code> <p>Predictor variables for the test set.</p> required <p>Returns:</p> Name Type Description <code>train_df_y_hat</code> <code>ndarray</code> <p>Predicted values for the training set.</p> <code>test_df_y_hat</code> <code>ndarray</code> <p>Predicted values for the test set.</p> Source code in <code>docs\\notebooks\\model\\m6_lr.py</code> <pre><code>def produce_forecast_m6_lr(model, train_df_X, test_df_X):\n    \"\"\"Generate forecasts for train and test sets using a trained linear regression model.\n\n    Args:\n        model (dict): Contains the trained feature selector and linear regression model:\n            - 'feature_selector': SelectKBest object for feature selection.\n            - 'regression_model': Fitted LinearRegression object.\n        train_df_X (pd.DataFrame): Predictor variables for the training set.\n        test_df_X (pd.DataFrame): Predictor variables for the test set.\n\n    Returns:\n        train_df_y_hat (np.ndarray): Predicted values for the training set.\n        test_df_y_hat (np.ndarray): Predicted values for the test set.\n    \"\"\"\n    fs_lr = model['feature_selector']\n    m06_lr = model['regression_model']\n\n    # SELECT K BEST FEATURES\n    train_df_X = fs_lr.transform(train_df_X)\n    test_df_X = fs_lr.transform(test_df_X)\n\n    # PRODUCE FORECAST\n    train_df_y_hat = m06_lr.predict(train_df_X)\n    test_df_y_hat = m06_lr.predict(test_df_X)\n\n    return train_df_y_hat, test_df_y_hat\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m6_lr.train_model_m6_lr","title":"<code>train_model_m6_lr(hyperparameter, train_df_X, train_df_y)</code>","text":"<p>Train a linear regression model with feature selection.</p> <p>Selects the K best features from the predictors and fits a linear regression model.</p> <p>Parameters:</p> Name Type Description Default <code>hyperparameter</code> <code>Series</code> <p>Hyperparameter values for the model (e.g., number of features).</p> required <code>train_df_X</code> <code>DataFrame</code> <p>Predictor variables for training.</p> required <code>train_df_y</code> <code>DataFrame</code> <p>Target variable for training.</p> required <p>Returns:</p> Name Type Description <code>model</code> <code>dict</code> <p>Dictionary containing the fitted feature selector and linear regression model: - 'feature_selector': SelectKBest object used to select features. - 'regression_model': Trained LinearRegression object.</p> Source code in <code>docs\\notebooks\\model\\m6_lr.py</code> <pre><code>def train_model_m6_lr(hyperparameter, train_df_X, train_df_y):\n    \"\"\"Train a linear regression model with feature selection.\n\n    Selects the K best features from the predictors and fits a linear regression model.\n\n    Args:\n        hyperparameter (pd.Series): Hyperparameter values for the model (e.g., number of features).\n        train_df_X (pd.DataFrame): Predictor variables for training.\n        train_df_y (pd.DataFrame): Target variable for training.\n\n    Returns:\n        model (dict): Dictionary containing the fitted feature selector and linear regression model:\n            - 'feature_selector': SelectKBest object used to select features.\n            - 'regression_model': Trained LinearRegression object.\n    \"\"\"\n\n    #UNPACK HYPERPARAMETER\n    num_feature = int(hyperparameter['num_features'])\n\n    # FEATURE SELECTOR\n    def select_features(train_df_X, train_df_y, num_feature):\n        ''' Make model to select K best feature. \n\n        Args:\n            train_df_X (df) : features matrix for training\n            train_df_y (df) : target matrix for training\n\n        Returns:\n            fs_lr (model) : feature selector\n        '''\n\n        train_df_y = train_df_y.values.ravel()\n        fs_lr = SelectKBest(f_regression, k = num_feature)\n        fs_lr.fit(train_df_X, train_df_y)\n\n        return fs_lr\n\n    fs_lr = select_features(train_df_X, train_df_y, num_feature)\n\n    #TRAIN MODEL\n    train_df_X = fs_lr.transform(train_df_X)\n    m06_lr = LinearRegression()\n    m06_lr.fit(train_df_X, train_df_y)\n\n    # PACK MODEL\n    model = {\"feature_selector\": fs_lr, \"regression_model\": m06_lr}    \n\n    return model\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m7_ann.produce_forecast_m7_ann","title":"<code>produce_forecast_m7_ann(model, train_df_X, test_df_X)</code>","text":"<p>Generate forecasts for train and test sets using a trained ANN model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>dict</code> <p>Contains the trained ANN under key 'model_ann'.</p> required <code>train_df_X</code> <code>DataFrame</code> <p>Predictor variables for the training set.</p> required <code>test_df_X</code> <code>DataFrame</code> <p>Predictor variables for the test set.</p> required <p>Returns:</p> Name Type Description <code>train_df_y_hat</code> <code>DataFrame</code> <p>Forecasted values for the training set.</p> <code>test_df_y_hat</code> <code>DataFrame</code> <p>Forecasted values for the test set.</p> Source code in <code>docs\\notebooks\\model\\m7_ann.py</code> <pre><code>def produce_forecast_m7_ann(model, train_df_X, test_df_X):\n    \"\"\"Generate forecasts for train and test sets using a trained ANN model.\n\n    Args:\n        model (dict): Contains the trained ANN under key 'model_ann'.\n        train_df_X (pd.DataFrame): Predictor variables for the training set.\n        test_df_X (pd.DataFrame): Predictor variables for the test set.\n\n    Returns:\n        train_df_y_hat (pd.DataFrame): Forecasted values for the training set.\n        test_df_y_hat (pd.DataFrame): Forecasted values for the test set.\n    \"\"\"\n\n    # UNPACK MODEL\n    model_ann = model[\"model_ann\"]\n\n    # PREPARE FORMAT\n    train_df_X_tensor = torch.tensor(train_df_X.values, dtype=torch.float32)\n    test_df_X_tensor = torch.tensor(test_df_X.values, dtype=torch.float32)\n\n    # PRODUCE FORECAST\n    # Switch model to evaluation mode for inference\n    model_ann.eval()\n\n    # TRAIN SET FORECAST\n    with torch.no_grad():  # Disable gradient calculation to save memory\n        train_df_y_hat_tensor = model_ann(train_df_X_tensor)\n\n    # TEST SET FORECAST\n    with torch.no_grad():  # Disable gradient calculation to save memory\n        test_df_y_hat_tensor = model_ann(test_df_X_tensor)\n\n    # Create DataFrames of result\n    train_df_y_hat = pd.DataFrame(train_df_y_hat_tensor, index=train_df_X.index, columns=['y_hat'])\n    test_df_y_hat = pd.DataFrame(test_df_y_hat_tensor, index=test_df_X.index, columns=['y_hat'])\n\n    return train_df_y_hat, test_df_y_hat\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m7_ann.train_model_m7_ann","title":"<code>train_model_m7_ann(hyperparameter, train_df_X, train_df_y)</code>","text":"<p>Train an artificial neural network (ANN) for point forecasting.</p> <p>Parameters:</p> Name Type Description Default <code>hyperparameter</code> <code>dict</code> <p>Hyperparameters for the ANN including: - 'seed': Random seed for reproducibility. - 'hidden_size': Number of neurons in the hidden layer. - 'activation_function': Activation function ('relu', 'sigmoid', 'tanh'). - 'learning_rate': Learning rate for the optimizer. - 'solver': Optimizer type ('adam' or 'sgd'). - 'epochs': Number of training epochs.</p> required <code>train_df_X</code> <code>DataFrame</code> <p>Predictor variables for training.</p> required <code>train_df_y</code> <code>DataFrame</code> <p>Target variable for training.</p> required <p>Returns:</p> Name Type Description <code>model</code> <code>dict</code> <p>Contains the trained ANN model under key 'model_ann'.</p> Source code in <code>docs\\notebooks\\model\\m7_ann.py</code> <pre><code>def train_model_m7_ann(hyperparameter, train_df_X, train_df_y):\n    \"\"\"Train an artificial neural network (ANN) for point forecasting.\n\n    Args:\n        hyperparameter (dict): Hyperparameters for the ANN including:\n            - 'seed': Random seed for reproducibility.\n            - 'hidden_size': Number of neurons in the hidden layer.\n            - 'activation_function': Activation function ('relu', 'sigmoid', 'tanh').\n            - 'learning_rate': Learning rate for the optimizer.\n            - 'solver': Optimizer type ('adam' or 'sgd').\n            - 'epochs': Number of training epochs.\n        train_df_X (pd.DataFrame): Predictor variables for training.\n        train_df_y (pd.DataFrame): Target variable for training.\n\n    Returns:\n        model (dict): Contains the trained ANN model under key 'model_ann'.\n    \"\"\"\n\n    #UNPACK HYPERPARAMETER\n\n    # Set random seed for reproducibility\n    def set_seed(seed):\n        random.seed(seed)\n        os.environ[\"PYTHONHASHSEED\"] = str(seed)\n        np.random.seed(seed)\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed(seed)\n        torch.backends.cudnn.deterministic = True\n\n    seed = int(hyperparameter['seed'])\n\n    hidden_size = hyperparameter['hidden_size']\n    activation_function = hyperparameter['activation_function']\n    learning_rate = hyperparameter['learning_rate']\n    # learning_rate = 0.001\n    solver = hyperparameter['solver']\n    epochs = hyperparameter['epochs']\n\n    # Use proper format for X and y\n    X = torch.tensor(train_df_X.values, dtype=torch.float32)\n    y = torch.tensor(train_df_y.values, dtype=torch.float32).view(-1, 1) \n\n    # Define the ANN model\n    class ANNModel(nn.Module):\n        def __init__(self, input_size, hidden_size, output_size):\n            super(ANNModel, self).__init__()\n            self.fc1 = nn.Linear(input_size, hidden_size)\n            self.fc2 = nn.Linear(hidden_size, output_size)\n            self.relu = nn.ReLU()  # Activation function\n\n        def forward(self, x):\n            x = self.fc1(x)\n            if activation_function == 'relu':\n                x = self.relu(x)\n            elif activation_function == 'sigmoid':\n                x = torch.sigmoid(x)\n            else:\n                x = torch.tanh(x)\n            x = self.fc2(x)\n            return x\n\n    # Model initialization\n    input_size = X.shape[1]\n    output_size = y.shape[1]\n\n    set_seed(seed)\n\n    model_ann = ANNModel(input_size, hidden_size, output_size)\n    if solver == 'adam':\n        optimizer = optim.Adam(model_ann.parameters(), lr=learning_rate)\n    elif solver == 'sgd':\n        optimizer = optim.SGD(model_ann.parameters(), lr=learning_rate)\n    else:\n        raise ValueError('Solver not found')\n\n    # Loss function\n    criterion = nn.MSELoss()  # Mean Squared Error loss for regression\n\n    #TRAIN MODEL\n    # Training loop\n    for epoch in range(epochs):\n        model_ann.train()\n\n        # Forward pass\n        output = model_ann(X)\n        loss = criterion(output, y)\n\n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n\n        # Update weights\n        optimizer.step()\n\n        if epoch % 10 == 0:\n            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n\n    # PACK MODEL\n    model = {\"model_ann\": model_ann}\n\n\n    return model\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m8_dnn.produce_forecast_m8_dnn","title":"<code>produce_forecast_m8_dnn(model, train_df_X, test_df_X)</code>","text":"<p>Generate forecasts for train and test sets using a trained DNN model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>dict</code> <p>Contains the trained DNN under key 'model_dnn'.</p> required <code>train_df_X</code> <code>DataFrame</code> <p>Predictor variables for the training set.</p> required <code>test_df_X</code> <code>DataFrame</code> <p>Predictor variables for the test set.</p> required <p>Returns:</p> Name Type Description <code>train_df_y_hat</code> <code>DataFrame</code> <p>Forecasted values for the training set.</p> <code>test_df_y_hat</code> <code>DataFrame</code> <p>Forecasted values for the test set.</p> Source code in <code>docs\\notebooks\\model\\m8_dnn.py</code> <pre><code>def produce_forecast_m8_dnn(model, train_df_X, test_df_X):\n    \"\"\"Generate forecasts for train and test sets using a trained DNN model.\n\n    Args:\n        model (dict): Contains the trained DNN under key 'model_dnn'.\n        train_df_X (pd.DataFrame): Predictor variables for the training set.\n        test_df_X (pd.DataFrame): Predictor variables for the test set.\n\n    Returns:\n        train_df_y_hat (pd.DataFrame): Forecasted values for the training set.\n        test_df_y_hat (pd.DataFrame): Forecasted values for the test set.\n    \"\"\"\n\n    # UNPACK MODEL\n    model_dnn = model[\"model_dnn\"]\n\n    # PREPARE FORMAT\n    train_df_X_tensor = torch.tensor(train_df_X.values, dtype=torch.float32)\n    test_df_X_tensor = torch.tensor(test_df_X.values, dtype=torch.float32)\n\n    # PRODUCE FORECAST\n    # Switch model to evaluation mode for inference\n    model_dnn.eval()\n\n    # TRAIN SET FORECAST\n    with torch.no_grad():  # Disable gradient calculation to save memory\n        train_df_y_hat_tensor = model_dnn(train_df_X_tensor)\n\n    # TEST SET FORECAST\n    with torch.no_grad():  # Disable gradient calculation to save memory\n        test_df_y_hat_tensor = model_dnn(test_df_X_tensor)\n\n    # Create DataFrames of result\n    train_df_y_hat = pd.DataFrame(train_df_y_hat_tensor.numpy(), index=train_df_X.index, columns=['y_hat'])\n    test_df_y_hat = pd.DataFrame(test_df_y_hat_tensor.numpy(), index=test_df_X.index, columns=['y_hat'])\n\n    return train_df_y_hat, test_df_y_hat\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m8_dnn.train_model_m8_dnn","title":"<code>train_model_m8_dnn(hyperparameter, train_df_X, train_df_y)</code>","text":"<p>Train a Deep Neural Network (DNN) for point forecasting.</p> <p>Parameters:</p> Name Type Description Default <code>hyperparameter</code> <code>dict</code> <p>Contains DNN configuration (hidden layers, size, activation, solver, learning rate, epochs, seed).</p> required <code>train_df_X</code> <code>DataFrame</code> <p>Predictor variables for training.</p> required <code>train_df_y</code> <code>DataFrame</code> <p>Target variable for training.</p> required <p>Returns:</p> Name Type Description <code>model</code> <code>dict</code> <p>Contains the trained DNN under key 'model_dnn'.</p> Source code in <code>docs\\notebooks\\model\\m8_dnn.py</code> <pre><code>def train_model_m8_dnn(hyperparameter, train_df_X, train_df_y):\n    \"\"\"Train a Deep Neural Network (DNN) for point forecasting.\n\n    Args:\n        hyperparameter (dict): Contains DNN configuration (hidden layers, size, activation, solver, learning rate, epochs, seed).\n        train_df_X (pd.DataFrame): Predictor variables for training.\n        train_df_y (pd.DataFrame): Target variable for training.\n\n    Returns:\n        model (dict): Contains the trained DNN under key 'model_dnn'.\n    \"\"\"\n\n    # UNPACK HYPERPARAMETER\n    seed = hyperparameter['seed']\n    torch.manual_seed(seed)  # Set seed for PyTorch\n\n    n_hidden = hyperparameter['n_hidden']\n    hidden_size = hyperparameter['hidden_size']\n    activation_function = hyperparameter['activation_function']\n    learning_rate = hyperparameter['learning_rate']\n    solver = hyperparameter['solver']\n    epochs = hyperparameter['epochs']\n\n    # Use proper format for X and y\n    X = torch.tensor(train_df_X.values, dtype=torch.float32)\n    y = torch.tensor(train_df_y.values, dtype=torch.float32).view(-1, 1) \n\n    # Define the DNN model\n    class DNNModel(nn.Module):\n        def __init__(self, input_size, hidden_size, output_size, n_hidden, activation_function):\n            super(DNNModel, self).__init__()\n            self.layers = nn.ModuleList()\n            self.activation_function = activation_function\n\n            # Input layer\n            self.layers.append(nn.Linear(input_size, hidden_size))\n\n            # Hidden layers\n            for _ in range(n_hidden - 1):\n                self.layers.append(nn.Linear(hidden_size, hidden_size))\n\n            # Output layer\n            self.layers.append(nn.Linear(hidden_size, output_size))\n\n        def forward(self, x):\n            for i, layer in enumerate(self.layers[:-1]):  # Iterate through hidden layers\n                x = layer(x)\n                if self.activation_function == 'relu':\n                    x = nn.ReLU()(x)\n                elif self.activation_function == 'sigmoid':\n                    x = torch.sigmoid(x)\n                elif self.activation_function == 'tanh':\n                    x = torch.tanh(x)\n\n            # Apply the output layer without activation function\n            x = self.layers[-1](x)\n            return x\n\n    # Model initialization\n    input_size = X.shape[1]\n    output_size = y.shape[1]\n    model_dnn = DNNModel(input_size, hidden_size, output_size, n_hidden, activation_function)\n\n    if solver == 'adam':\n        optimizer = optim.Adam(model_dnn.parameters(), lr=learning_rate)\n    elif solver == 'sgd':\n        optimizer = optim.SGD(model_dnn.parameters(), lr=learning_rate)\n    else:\n        raise ValueError('Solver not found')\n\n    # Loss function\n    criterion = nn.MSELoss()  # Mean Squared Error loss for regression\n\n    # TRAIN MODEL\n    # Training loop\n    for epoch in range(epochs):\n        model_dnn.train()\n\n        # Forward pass\n        output = model_dnn(X)\n        loss = criterion(output, y)\n\n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n\n        # Update weights\n        optimizer.step()\n\n        if epoch % 10 == 0:\n            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n\n    # PACK MODEL\n    model = {\"model_dnn\": model_dnn}\n\n    return model\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m9_rt.produce_forecast_m9_rt","title":"<code>produce_forecast_m9_rt(model, train_df_X, test_df_X)</code>","text":"<p>Generate forecasts for train and test sets using a trained regression tree.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>dict</code> <p>Trained model containing the regression tree under key 'rt'.</p> required <code>train_df_X</code> <code>DataFrame</code> <p>Predictor variables for the training set.</p> required <code>test_df_X</code> <code>DataFrame</code> <p>Predictor variables for the test set.</p> required <p>Returns:</p> Name Type Description <code>train_df_y_hat</code> <code>DataFrame</code> <p>Forecasts for the training set.</p> <code>test_df_y_hat</code> <code>DataFrame</code> <p>Forecasts for the test set.</p> Source code in <code>docs\\notebooks\\model\\m9_rt.py</code> <pre><code>def produce_forecast_m9_rt(model, train_df_X, test_df_X):\n    \"\"\"Generate forecasts for train and test sets using a trained regression tree.\n\n    Args:\n        model (dict): Trained model containing the regression tree under key 'rt'.\n        train_df_X (pd.DataFrame): Predictor variables for the training set.\n        test_df_X (pd.DataFrame): Predictor variables for the test set.\n\n    Returns:\n        train_df_y_hat (pd.DataFrame): Forecasts for the training set.\n        test_df_y_hat (pd.DataFrame): Forecasts for the test set.\n    \"\"\"\n\n    # UNPACK MODEL\n    regressor = model['rt']\n\n    # PRODUCE FORECAST\n    train_df_y_hat = pd.DataFrame(regressor.predict(train_df_X), index = train_df_X.index, columns = ['y_hat'])\n    test_df_y_hat = pd.DataFrame(regressor.predict(test_df_X), index = test_df_X.index, columns = ['y_hat'])\n\n    # print('I am here after training the model')\n    # print('train_df_y_hat', train_df_y_hat)\n    # print('test_df_y_hat', test_df_y_hat)\n\n    return train_df_y_hat, test_df_y_hat\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m9_rt.train_model_m9_rt","title":"<code>train_model_m9_rt(hyperparameter, train_df_X, train_df_y)</code>","text":"<p>Train a regression tree (DecisionTreeRegressor) for point forecasting.</p> <p>Parameters:</p> Name Type Description Default <code>hyperparameter</code> <code>dict</code> <p>Dictionary of hyperparameters (seed, max_depth, min_samples_split, min_samples_leaf, max_features).</p> required <code>train_df_X</code> <code>DataFrame</code> <p>Predictor variables for training.</p> required <code>train_df_y</code> <code>DataFrame</code> <p>Target variable for training.</p> required <p>Returns:</p> Name Type Description <code>model</code> <code>dict</code> <p>Contains the trained regression tree under key 'rt'.</p> Source code in <code>docs\\notebooks\\model\\m9_rt.py</code> <pre><code>def train_model_m9_rt(hyperparameter, train_df_X, train_df_y):\n    \"\"\"Train a regression tree (DecisionTreeRegressor) for point forecasting.\n\n    Args:\n        hyperparameter (dict): Dictionary of hyperparameters (seed, max_depth, min_samples_split, min_samples_leaf, max_features).\n        train_df_X (pd.DataFrame): Predictor variables for training.\n        train_df_y (pd.DataFrame): Target variable for training.\n\n    Returns:\n        model (dict): Contains the trained regression tree under key 'rt'.\n    \"\"\"\n\n    #UNPACK HYPERPARAMETER\n    seed = hyperparameter['seed']\n    max_depth = hyperparameter['max_depth']\n    min_samples_split = hyperparameter['min_samples_split']\n    min_samples_leaf = hyperparameter['min_samples_leaf']\n    max_features = hyperparameter['max_features']\n\n    #TRAIN MODEL\n    # Initialize the regression tree model with important hyperparameters\n    regressor = DecisionTreeRegressor(\n        criterion='squared_error',\n        max_depth=max_depth,\n        min_samples_split = min_samples_split,\n        min_samples_leaf = min_samples_leaf,\n        max_features = max_features,\n        random_state = seed\n    )\n\n    # Train the model\n    regressor.fit(train_df_X, train_df_y)\n\n    # PACK MODEL\n    model = {\"rt\": regressor}\n\n    # print('I am here after training the model')\n\n    return model\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m10_rf.produce_forecast_m10_rf","title":"<code>produce_forecast_m10_rf(model, train_df_X, test_df_X)</code>","text":"<p>Generate point forecasts on train and test sets using a trained Random Forest model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>dict</code> <p>Trained model containing the Random Forest under key 'rf'.</p> required <code>train_df_X</code> <code>DataFrame</code> <p>Predictor matrix for the training set.</p> required <code>test_df_X</code> <code>DataFrame</code> <p>Predictor matrix for the test set.</p> required <p>Returns:</p> Name Type Description <code>train_df_y_hat</code> <code>DataFrame</code> <p>Forecasted target values for the training set.</p> <code>test_df_y_hat</code> <code>DataFrame</code> <p>Forecasted target values for the test set.</p> Source code in <code>docs\\notebooks\\model\\m10_rf.py</code> <pre><code>def produce_forecast_m10_rf(model, train_df_X, test_df_X):\n    \"\"\"Generate point forecasts on train and test sets using a trained Random Forest model.\n\n    Args:\n        model (dict): Trained model containing the Random Forest under key 'rf'.\n        train_df_X (pd.DataFrame): Predictor matrix for the training set.\n        test_df_X (pd.DataFrame): Predictor matrix for the test set.\n\n    Returns:\n        train_df_y_hat (pd.DataFrame): Forecasted target values for the training set.\n        test_df_y_hat (pd.DataFrame): Forecasted target values for the test set.\n    \"\"\"\n\n    # UNPACK MODEL\n    rf = model['rf']\n\n    # PRODUCE FORECAST\n    train_df_y_hat = pd.DataFrame(rf.predict(train_df_X), index = train_df_X.index, columns = ['y_hat'])\n    test_df_y_hat = pd.DataFrame(rf.predict(test_df_X), index = test_df_X.index, columns = ['y_hat'])\n\n    return train_df_y_hat, test_df_y_hat\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m10_rf.train_model_m10_rf","title":"<code>train_model_m10_rf(hyperparameter, train_df_X, train_df_y)</code>","text":"<p>Train a Random Forest model for point forecasting.</p> <p>Parameters:</p> Name Type Description Default <code>hyperparameter</code> <code>dict</code> <p>Dictionary of model hyperparameters.</p> required <code>train_df_X</code> <code>DataFrame</code> <p>Feature matrix for training.</p> required <code>train_df_y</code> <code>DataFrame</code> <p>Target values for training.</p> required <p>Returns:</p> Name Type Description <code>model</code> <code>dict</code> <p>Trained model containing the Random Forest under key 'rf'.</p> Source code in <code>docs\\notebooks\\model\\m10_rf.py</code> <pre><code>def train_model_m10_rf(hyperparameter, train_df_X, train_df_y):\n    \"\"\"Train a Random Forest model for point forecasting.\n\n    Args:\n        hyperparameter (dict): Dictionary of model hyperparameters.\n        train_df_X (pd.DataFrame): Feature matrix for training.\n        train_df_y (pd.DataFrame): Target values for training.\n\n    Returns:\n        model (dict): Trained model containing the Random Forest under key 'rf'.\n    \"\"\"\n\n    #UNPACK HYPERPARAMETER\n    seed = int(hyperparameter['seed'])\n    n_estimators = int(hyperparameter['n_estimators'])\n    max_depth = int(hyperparameter['max_depth'])\n    min_samples_split = int(hyperparameter['min_samples_split'])\n    min_samples_leaf = int(hyperparameter['min_samples_leaf'])\n\n\n    #TRAIN MODEL\n    rf = RandomForestRegressor(\n        n_estimators=n_estimators,       # number of trees\n        max_depth=max_depth,           # maximum depth of a tree\n        min_samples_split=min_samples_split,    # min samples to split a node\n        min_samples_leaf=min_samples_leaf,     # min samples in a leaf\n        random_state=seed\n    )\n\n    rf.fit(train_df_X, train_df_y) # fit the model to the training data\n\n    # PACK MODEL\n    model = {\"rf\": rf}\n\n\n    return model\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m11_svr.produce_forecast_m11_svr","title":"<code>produce_forecast_m11_svr(model, train_df_X, test_df_X)</code>","text":"<p>Generate forecasts using a trained SVR model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>dict</code> <p>Trained model containing the SVR object under key 'svr'.</p> required <code>train_df_X</code> <code>DataFrame</code> <p>Predictor matrix for the training set.</p> required <code>test_df_X</code> <code>DataFrame</code> <p>Predictor matrix for the test set.</p> required <p>Returns:</p> Name Type Description <code>train_df_y_hat</code> <code>ndarray</code> <p>Forecasted values for the training set.</p> <code>test_df_y_hat</code> <code>ndarray</code> <p>Forecasted values for the test set.</p> Source code in <code>docs\\notebooks\\model\\m11_svr.py</code> <pre><code>def produce_forecast_m11_svr(model, train_df_X, test_df_X):\n    \"\"\"Generate forecasts using a trained SVR model.\n\n    Args:\n        model (dict): Trained model containing the SVR object under key 'svr'.\n        train_df_X (pd.DataFrame): Predictor matrix for the training set.\n        test_df_X (pd.DataFrame): Predictor matrix for the test set.\n\n    Returns:\n        train_df_y_hat (np.ndarray): Forecasted values for the training set.\n        test_df_y_hat (np.ndarray): Forecasted values for the test set.\n    \"\"\"\n\n    svr = model['svr']\n    train_df_y_hat = svr.predict(train_df_X)\n    test_df_y_hat = svr.predict(test_df_X)\n\n    return train_df_y_hat, test_df_y_hat\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m11_svr.train_model_m11_svr","title":"<code>train_model_m11_svr(hyperparameter, train_df_X, train_df_y)</code>","text":"<p>Train a Support Vector Regression (SVR) model for point forecasting.</p> <p>Parameters:</p> Name Type Description Default <code>hyperparameter</code> <code>dict</code> <p>SVR hyperparameters including 'kernel', 'C', 'gamma', and 'epsilon'.</p> required <code>train_df_X</code> <code>DataFrame</code> <p>Predictor matrix for training.</p> required <code>train_df_y</code> <code>DataFrame or Series</code> <p>Target values for training.</p> required <p>Returns:</p> Name Type Description <code>model</code> <code>dict</code> <p>Trained model containing the SVR object under key 'svr'.</p> Source code in <code>docs\\notebooks\\model\\m11_svr.py</code> <pre><code>def train_model_m11_svr(hyperparameter, train_df_X, train_df_y):\n    \"\"\"Train a Support Vector Regression (SVR) model for point forecasting.\n\n    Args:\n        hyperparameter (dict): SVR hyperparameters including 'kernel', 'C', 'gamma', and 'epsilon'.\n        train_df_X (pd.DataFrame): Predictor matrix for training.\n        train_df_y (pd.DataFrame or pd.Series): Target values for training.\n\n    Returns:\n        model (dict): Trained model containing the SVR object under key 'svr'.\n    \"\"\"\n\n    from sklearn.svm import SVR\n\n    #UNPACK HYPERPARAMETER\n    seed = hyperparameter['seed'] #seem we can't use this using sklearn\n    kernel = hyperparameter['kernel']\n    C = hyperparameter['C']\n    gamma = hyperparameter['gamma']\n    epsilon = hyperparameter['epsilon']\n\n    #TRAIN MODEL\n    train_df_y = train_df_y.values.ravel()  # Flatten the target array if necessary\n    svr = SVR(kernel=kernel, C=C, gamma=gamma, epsilon=epsilon)\n    svr.fit(train_df_X, train_df_y)\n\n    # PACK MODEL\n    model = {\"svr\": svr}\n\n\n    return model\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m12_rnn.produce_forecast_m12_rnn","title":"<code>produce_forecast_m12_rnn(model, train_df_X, test_df_X)</code>","text":"<p>Create forecast at the train and test set using the trained RNN model</p> Source code in <code>docs\\notebooks\\model\\m12_rnn.py</code> <pre><code>def produce_forecast_m12_rnn(model, train_df_X, test_df_X):\n    \"\"\"Create forecast at the train and test set using the trained RNN model\"\"\"\n\n    # UNPACK MODEL\n    rnn = model['rnn']\n    hyperparameter = model['hyperparameter']\n\n    # UNPACK HYPERPARAMETER\n    input_size = int(hyperparameter['input_size'])\n    batch_size = int(hyperparameter['batch_size'])\n\n    # PRODUCE FORECAST\n    def produce_forecast(rnn, X):\n        X_lags, X_exog = separate_lag_and_exogenous_features(X)\n        X_lags_tensor = torch.tensor(X_lags.values, dtype=torch.float32)\n        X_exog_tensor = torch.tensor(X_exog.values, dtype=torch.float32)\n\n        total_lag_features = X_lags_tensor.shape[1]\n        sequence_length = total_lag_features // input_size\n        X_lags_tensor = X_lags_tensor.view(-1, sequence_length, input_size)\n\n        predictions = []\n        for i in range(0, len(X_lags_tensor), batch_size):\n            batch_X_lags = X_lags_tensor[i:i+batch_size]\n            batch_X_exog = X_exog_tensor[i:i+batch_size]\n\n            with torch.no_grad():\n                batch_pred = rnn(batch_X_lags, batch_X_exog)\n\n            predictions.append(batch_pred)\n\n        predictions = torch.cat(predictions, dim=0)\n        return predictions.detach().numpy()\n\n    train_df_y_hat = produce_forecast(rnn, train_df_X)\n    test_df_y_hat = produce_forecast(rnn, test_df_X)\n\n    return train_df_y_hat, test_df_y_hat\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m12_rnn.separate_lag_and_exogenous_features","title":"<code>separate_lag_and_exogenous_features(train_df_X, target_column='y', lag_prefix='y_lag')</code>","text":"<p>Generate forecasts for train and test sets using a trained RNN model.</p> <p>The function separates lag and exogenous features, reshapes the lagged inputs  into sequences, and produces batch-wise predictions using the trained RNN.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>dict</code> <p>Dictionary containing the trained RNN, hyperparameters, and training data.</p> required <code>train_df_X</code> <code>DataFrame</code> <p>Predictor matrix for the training set.</p> required <code>test_df_X</code> <code>DataFrame</code> <p>Predictor matrix for the test set.</p> required <p>Returns:</p> Name Type Description <code>train_df_y_hat</code> <code>ndarray</code> <p>Forecasted values for the training set.</p> <code>test_df_y_hat</code> <code>ndarray</code> <p>Forecasted values for the test set.</p> Source code in <code>docs\\notebooks\\model\\m12_rnn.py</code> <pre><code>def separate_lag_and_exogenous_features(train_df_X, target_column='y', lag_prefix='y_lag'):\n    \"\"\"Generate forecasts for train and test sets using a trained RNN model.\n\n    The function separates lag and exogenous features, reshapes the lagged inputs \n    into sequences, and produces batch-wise predictions using the trained RNN.\n\n    Args:\n        model (dict): Dictionary containing the trained RNN, hyperparameters, and training data.\n        train_df_X (pd.DataFrame): Predictor matrix for the training set.\n        test_df_X (pd.DataFrame): Predictor matrix for the test set.\n\n    Returns:\n        train_df_y_hat (np.ndarray): Forecasted values for the training set.\n        test_df_y_hat (np.ndarray): Forecasted values for the test set.\n    \"\"\"\n\n    # Identify lag features (columns that start with 'y_lag')\n    lag_features = [col for col in train_df_X.columns if col.startswith(lag_prefix)]\n\n    # Identify exogenous variables (everything except the target and lag features)\n    exog_features = [col for col in train_df_X.columns if col not in [target_column] + lag_features]\n\n    # Create dataframes for lag features and exogenous features\n    X_lags = train_df_X[lag_features]\n    X_exog = train_df_X[exog_features]\n\n    return X_lags, X_exog\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m12_rnn.train_model_m12_rnn","title":"<code>train_model_m12_rnn(hyperparameter, train_df_X, train_df_y)</code>","text":"<p>Train an RNN model for point forecasting using lag and exogenous features.</p> <p>The RNN learns temporal patterns from lagged features. The last hidden state is  concatenated with exogenous features (e.g., calendar variables) and passed through  a fully connected layer to produce final forecasts.</p> <p>Parameters:</p> Name Type Description Default <code>hyperparameter</code> <code>dict</code> <p>Model hyperparameters including seed, input_size, hidden_size,                    num_layers, output_size, batch_size, epochs, and learning_rate.</p> required <code>train_df_X</code> <code>DataFrame</code> <p>Predictor matrix with lag and exogenous features.</p> required <code>train_df_y</code> <code>DataFrame</code> <p>Target vector for training.</p> required <p>Returns:</p> Name Type Description <code>model</code> <code>dict</code> <p>Contains the trained RNN, hyperparameters, and training data.</p> Source code in <code>docs\\notebooks\\model\\m12_rnn.py</code> <pre><code>def train_model_m12_rnn(hyperparameter, train_df_X, train_df_y):\n    \"\"\"Train an RNN model for point forecasting using lag and exogenous features.\n\n    The RNN learns temporal patterns from lagged features. The last hidden state is \n    concatenated with exogenous features (e.g., calendar variables) and passed through \n    a fully connected layer to produce final forecasts.\n\n    Args:\n        hyperparameter (dict): Model hyperparameters including seed, input_size, hidden_size,\n                               num_layers, output_size, batch_size, epochs, and learning_rate.\n        train_df_X (pd.DataFrame): Predictor matrix with lag and exogenous features.\n        train_df_y (pd.DataFrame): Target vector for training.\n\n    Returns:\n        model (dict): Contains the trained RNN, hyperparameters, and training data.\n    \"\"\"\n\n    # UNPACK HYPERPARAMETER\n    seed = int(hyperparameter['seed'])\n    input_size = int(hyperparameter['input_size'])\n    hidden_size = int(hyperparameter['hidden_size'])\n    num_layers = int(hyperparameter['num_layers'])\n    output_size = int(hyperparameter['output_size'])\n    batch_size = int(hyperparameter['batch_size'])\n    epochs = int(hyperparameter['epochs'])\n    learning_rate = hyperparameter['learning_rate']\n\n    # DEFINE MODEL AND TRAINING FUNCTION\n    class RNNModel(nn.Module):\n        def __init__(self, input_size, hidden_size, num_layers, exog_size, output_size=1):\n            super(RNNModel, self).__init__()\n\n            # Define the RNN layer\n            self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n\n            # Define the Fully Connected (FC) layer\n            self.fc = nn.Linear(hidden_size + exog_size, output_size)\n\n        def forward(self, x, exogenous_data):\n            # Pass the input through the RNN\n            out, h_n = self.rnn(x)\n\n            # Get the last timestep hidden state\n            last_hidden_state = out[:, -1, :]  # Shape: (batch_size, hidden_size)\n\n            # Concatenate hidden state with exogenous vars\n            combined_input = torch.cat((last_hidden_state, exogenous_data), dim=1)\n\n            # Pass through FC\n            out = self.fc(combined_input)\n            return out\n\n    def train_rnn_with_minibatches(model, train_loader, epochs, learning_rate=0.001):\n        criterion = nn.MSELoss()\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n        for epoch in range(epochs):\n            print(f'Epoch [{epoch+1}/{epochs}]')\n            start_time = time.time()\n\n            model.train()\n            batch_no = 1\n            for X_lags_batch, X_exog_batch, y_batch in train_loader:\n                print(f'Epoch [{epoch+1}/{epochs}] and batch [{batch_no}/{len(train_loader)}]')\n                batch_no += 1\n\n                # Forward pass\n                predictions = model(X_lags_batch, X_exog_batch)\n                loss = criterion(predictions, y_batch)\n\n                # Backward pass\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n            end_time = time.time()\n            epoch_time = end_time - start_time\n            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}, time taken: {epoch_time:.2f} seconds')\n\n    def set_seed(seed=seed):\n        random.seed(seed)\n        np.random.seed(seed)\n        torch.manual_seed(seed)\n        os.environ[\"PYTHONHASHSEED\"] = str(seed)\n\n    # PREPARE TRAIN DATA\n    X_lags, X_exog = separate_lag_and_exogenous_features(train_df_X)\n    X_lags_tensor = torch.tensor(X_lags.values, dtype=torch.float32)\n    X_exog_tensor = torch.tensor(X_exog.values, dtype=torch.float32)\n    y_tensor = torch.tensor(train_df_y.values, dtype=torch.float32).view(-1, 1)\n\n    total_lag_features = X_lags_tensor.shape[1]\n    sequence_length = total_lag_features // input_size\n    exog_size = X_exog_tensor.shape[1]\n\n    # Reshape to 3D\n    X_lags_tensor = X_lags_tensor.view(-1, sequence_length, input_size)\n\n    # INITIALIZE MODEL + DATALOADER\n    set_seed(seed=seed)\n    rnn = RNNModel(input_size, hidden_size, num_layers, exog_size, output_size)\n    train_data = TensorDataset(X_lags_tensor, X_exog_tensor, y_tensor)\n    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n\n    # TRAIN MODEL\n    train_rnn_with_minibatches(rnn, train_loader, epochs=epochs, learning_rate=learning_rate)\n\n    # PACK MODEL\n    model = {\"rnn\": rnn, 'hyperparameter': hyperparameter, \"train_df_X\": train_df_X, \"train_df_y\": train_df_y}\n    return model\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m13_lstm.produce_forecast_m13_lstm","title":"<code>produce_forecast_m13_lstm(model, train_df_X, test_df_X)</code>","text":"<p>Generate forecasts for train and test sets using a trained LSTM model.</p> <p>The function handles lag and exogenous features, applies mini-batching  to avoid memory issues, and returns predictions for both train and test sets.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>dict</code> <p>Dictionary containing the trained LSTM model and hyperparameters.</p> required <code>train_df_X</code> <code>DataFrame</code> <p>Training predictors including lag and exogenous features.</p> required <code>test_df_X</code> <code>DataFrame</code> <p>Test predictors including lag and exogenous features.</p> required <p>Returns:</p> Name Type Description <code>train_df_y_hat</code> <code>ndarray</code> <p>Forecasted values for the training set.</p> <code>test_df_y_hat</code> <code>ndarray</code> <p>Forecasted values for the test set.</p> Source code in <code>docs\\notebooks\\model\\m13_lstm.py</code> <pre><code>def produce_forecast_m13_lstm(model, train_df_X, test_df_X):\n    \"\"\"Generate forecasts for train and test sets using a trained LSTM model.\n\n    The function handles lag and exogenous features, applies mini-batching \n    to avoid memory issues, and returns predictions for both train and test sets.\n\n    Args:\n        model (dict): Dictionary containing the trained LSTM model and hyperparameters.\n        train_df_X (pd.DataFrame): Training predictors including lag and exogenous features.\n        test_df_X (pd.DataFrame): Test predictors including lag and exogenous features.\n\n    Returns:\n        train_df_y_hat (np.ndarray): Forecasted values for the training set.\n        test_df_y_hat (np.ndarray): Forecasted values for the test set.\n    \"\"\"\n\n    # UNPACK MODEL\n    lstm = model['lstm']\n    hyperparameter = model['hyperparameter']\n\n    #UNPACK HYPERPARAMETER\n    seed = int(hyperparameter['seed'])\n    input_size = int(hyperparameter['input_size']) #this is one since we only use lag features to be fed into the LSTM. The exogenous features like calenndar are fed to the fully connected layer, together with the last hidden state of LSTM.\n    hidden_size = int(hyperparameter['hidden_size']) #this is the size of hidden state, and we aim to use many to one architecture. Meaning we only take the last hidden state as output, and fed into the fully connected layer.\n    num_layers = int(hyperparameter['num_layers']) # we use 1 by default to make it simple. \n    output_size = int(hyperparameter['output_size']) #this is one since we only predict one value.\n    batch_size = int(hyperparameter['batch_size']) #using minibatch is important cuz if we train all samples at once, the memory is not enough.\n    epochs = int(hyperparameter['epochs'])\n    learning_rate = hyperparameter['learning_rate']  # No change for learning rate\n\n    # PRODUCE FORECAST\n    def produce_forecast(lstm, X):\n        # Convert X into X_lag and X_exog\n        X_lags, X_exog = separate_lag_and_exogenous_features(X)\n        X_lags_tensor = torch.tensor(X_lags.values, dtype=torch.float32)  # Shape: (batch_size, sequence_length, input_size)\n        X_exog_tensor = torch.tensor(X_exog.values, dtype=torch.float32)  # Shape: (batch_size, exog_size)\n        # y_tensor = torch.tensor(train_df_y.values, dtype=torch.float32).view(-1, 1) to be deleted.\n\n        total_lag_features = X_lags_tensor.shape[1]  # Number of lag features (columns)\n        sequence_length = total_lag_features // input_size\n        exog_size = X_exog_tensor.shape[1]  # Number of exogenous features\n\n        # Reshaping X_lags_tensor to 3D: (batch_size, sequence_length, input_size)\n        X_lags_tensor = X_lags_tensor.view(-1, sequence_length, input_size)\n\n        #predictions = lstm(X_lags_tensor, X_exog_tensor) #this doesn't work because of the batch size is too big, not enough memory.\n        predictions = []\n        for i in range(0, len(X_lags_tensor), batch_size):\n            # Get the current minibatch for both X_lags_tensor and X_exog_tensor\n            batch_X_lags = X_lags_tensor[i:i+batch_size]\n            batch_X_exog = X_exog_tensor[i:i+batch_size]\n\n            with torch.no_grad():\n                # Make predictions for the minibatch\n                batch_pred = lstm(batch_X_lags, batch_X_exog)\n\n            # Store the predictions for the current batch\n            predictions.append(batch_pred)\n\n        # Concatenate all predictions to get the full result\n        predictions = torch.cat(predictions, dim=0)\n\n\n        return predictions.detach().numpy()\n\n    train_df_y_hat = produce_forecast(lstm, train_df_X)\n    test_df_y_hat = produce_forecast(lstm, test_df_X)\n\n    return train_df_y_hat, test_df_y_hat\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m13_lstm.separate_lag_and_exogenous_features","title":"<code>separate_lag_and_exogenous_features(train_df_X, target_column='y', lag_prefix='y_lag')</code>","text":"<p>Split a dataframe into lag features and exogenous variables.</p> <p>Parameters:</p> Name Type Description Default <code>train_df_X</code> <code>DataFrame</code> <p>DataFrame containing lagged features, exogenous variables, and possibly the target.</p> required <code>target_column</code> <code>str</code> <p>Name of the target column to exclude from exogenous features. Defaults to 'y'.</p> <code>'y'</code> <code>lag_prefix</code> <code>str</code> <p>Prefix that identifies lagged features. Defaults to 'y_lag'.</p> <code>'y_lag'</code> <p>Returns:</p> Name Type Description <code>X_lags</code> <code>DataFrame</code> <p>DataFrame containing only columns that are lag features.</p> <code>X_exog</code> <code>DataFrame</code> <p>DataFrame containing only exogenous variables (excluding target and lag features).</p> Source code in <code>docs\\notebooks\\model\\m13_lstm.py</code> <pre><code>def separate_lag_and_exogenous_features(train_df_X, target_column='y', lag_prefix='y_lag'):\n    \"\"\"\n    Split a dataframe into lag features and exogenous variables.\n\n    Args:\n        train_df_X (pd.DataFrame): DataFrame containing lagged features, exogenous variables, and possibly the target.\n        target_column (str, optional): Name of the target column to exclude from exogenous features. Defaults to 'y'.\n        lag_prefix (str, optional): Prefix that identifies lagged features. Defaults to 'y_lag'.\n\n    Returns:\n        X_lags (pd.DataFrame): DataFrame containing only columns that are lag features.\n        X_exog (pd.DataFrame): DataFrame containing only exogenous variables (excluding target and lag features).\n    \"\"\"\n\n    # Identify lag features (columns that start with 'y_lag')\n    lag_features = [col for col in train_df_X.columns if col.startswith(lag_prefix)]\n\n    # Identify exogenous variables (everything except the target and lag features)\n    exog_features = [col for col in train_df_X.columns if col not in [target_column] + lag_features]\n\n    # Create dataframes for lag features and exogenous features\n    X_lags = train_df_X[lag_features]\n    X_exog = train_df_X[exog_features]\n\n    return X_lags, X_exog\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m13_lstm.train_model_m13_lstm","title":"<code>train_model_m13_lstm(hyperparameter, train_df_X, train_df_y)</code>","text":"<p>Train an LSTM model for point forecasting using lag and exogenous features.</p> <p>The LSTM captures temporal patterns from lag features. The last hidden state  is concatenated with exogenous features and passed through a fully connected  layer to produce the forecast.</p> <p>Parameters:</p> Name Type Description Default <code>hyperparameter</code> <code>dict</code> <p>Model hyperparameters (seed, input_size, hidden_size,                     num_layers, output_size, batch_size, epochs, learning_rate).</p> required <code>train_df_X</code> <code>DataFrame</code> <p>Training predictors including lag and exogenous features.</p> required <code>train_df_y</code> <code>DataFrame</code> <p>Training target values.</p> required <p>Returns:</p> Name Type Description <code>model</code> <code>dict</code> <p>Dictionary containing the trained LSTM model, hyperparameters,            and original training data.</p> Source code in <code>docs\\notebooks\\model\\m13_lstm.py</code> <pre><code>def train_model_m13_lstm(hyperparameter, train_df_X, train_df_y):\n    \"\"\"Train an LSTM model for point forecasting using lag and exogenous features.\n\n    The LSTM captures temporal patterns from lag features. The last hidden state \n    is concatenated with exogenous features and passed through a fully connected \n    layer to produce the forecast.\n\n    Args:\n        hyperparameter (dict): Model hyperparameters (seed, input_size, hidden_size, \n                               num_layers, output_size, batch_size, epochs, learning_rate).\n        train_df_X (pd.DataFrame): Training predictors including lag and exogenous features.\n        train_df_y (pd.DataFrame): Training target values.\n\n    Returns:\n        model (dict): Dictionary containing the trained LSTM model, hyperparameters, \n                      and original training data.\n    \"\"\"\n\n    #UNPACK HYPERPARAMETER\n    seed = int(hyperparameter['seed'])\n    input_size = int(hyperparameter['input_size']) #this is one since we only use lag features to be fed into the LSTM. The exogenous features like calenndar are fed to the fully connected layer, together with the last hidden state of LSTM.\n    hidden_size = int(hyperparameter['hidden_size']) #this is the size of hidden state, and we aim to use many to one architecture. Meaning we only take the last hidden state as output, and fed into the fully connected layer.\n    num_layers = int(hyperparameter['num_layers']) # we use 1 by default to make it simple. \n    output_size = int(hyperparameter['output_size']) #this is one since we only predict one value.\n    batch_size = int(hyperparameter['batch_size']) #using minibatch is important cuz if we train all samples at once, the memory is not enough.\n    epochs = int(hyperparameter['epochs'])\n    learning_rate = hyperparameter['learning_rate']  # No change for learning rate\n\n\n    #DEFINE MODEL AND TRAINING FUNCTION\n    class LSTMModel(nn.Module):\n        def __init__(self, input_size, hidden_size, num_layers, exog_size, output_size=1):\n            super(LSTMModel, self).__init__()\n\n            # Define the LSTM layer\n            self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n\n            # Define the Fully Connected (FC) layer\n            # The FC layer input size is the concatenation of LSTM output and exogenous variables\n            self.fc = nn.Linear(hidden_size + exog_size, output_size)  # exog_size is the number of exogenous features\n\n        def forward(self, x, exogenous_data):\n            # Pass the input through the LSTM\n            out, (h_n, c_n) = self.lstm(x)\n\n            # Get the last timestep hidden state (h3)\n            last_hidden_state = out[:, -1, :]  # Shape: (batch_size, hidden_size)\n\n            # Concatenate the LSTM output (h3) with the exogenous variables (for timestep t+100)\n            combined_input = torch.cat((last_hidden_state, exogenous_data), dim=1)  # Shape: (batch_size, hidden_size + exog_size)\n\n            # Pass the combined input through the FC layer\n            out = self.fc(combined_input)\n            return out\n\n    def train_lstm_with_minibatches(model, train_loader, epochs, learning_rate=0.001):\n        # Define the loss function (Mean Squared Error)\n        criterion = nn.MSELoss()\n\n        # Define the optimizer (Adam)\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n        for epoch in range(epochs):\n            print(f'Epoch [{epoch+1}/{epochs}]')\n            start_time = time.time()\n\n            model.train()  # Set model to training mode\n            # print(f'I am here')\n\n            # Iterate over mini-batches\n            batch_no = 1\n            for X_lags_batch, X_exog_batch, y_batch in train_loader:\n                # print(f'I am here now')\n                # Print the loss and time taken for this epoch\n                print(f'Epoch [{epoch+1}/{epochs}] and batch [{batch_no}/{len(train_loader)}]')\n                batch_no += 1\n                # Forward pass\n                predictions = model(X_lags_batch, X_exog_batch)\n                loss = criterion(predictions, y_batch)\n\n                # Backward pass\n                optimizer.zero_grad()  # Zero gradients from previous step\n                loss.backward()  # Backpropagate the error\n                optimizer.step()  # Update the model's weights\n\n\n\n            end_time = time.time()\n            epoch_time = end_time - start_time\n            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}, time taken: {epoch_time:.2f} seconds')\n\n    def set_seed(seed=seed):\n        random.seed(seed)\n        np.random.seed(seed)\n        torch.manual_seed(seed)\n        os.environ[\"PYTHONHASHSEED\"] = str(seed)\n\n    # PREPARE TRAIN DATA\n    # SEPARATE LAG AND EXOGENOUS FEATURES\n    X_lags, X_exog = separate_lag_and_exogenous_features(train_df_X)\n    X_lags_tensor = torch.tensor(X_lags.values, dtype=torch.float32)  # Shape: (batch_size, sequence_length, input_size)\n    X_exog_tensor = torch.tensor(X_exog.values, dtype=torch.float32)  # Shape: (batch_size, exog_size)\n    y_tensor = torch.tensor(train_df_y.values, dtype=torch.float32).view(-1, 1)\n\n    total_lag_features = X_lags_tensor.shape[1]  # Number of lag features (columns)\n    sequence_length = total_lag_features // input_size\n    exog_size = X_exog_tensor.shape[1]  # Number of exogenous features\n\n    # Reshaping X_lags_tensor to 3D: (batch_size, sequence_length, input_size)\n    X_lags_tensor = X_lags_tensor.view(-1, sequence_length, input_size)\n\n\n    # INITIALIZE MODEL and MAKE TRAINING BATCHES\n    set_seed(seed = seed) # Set random seed for reproducibility\n    lstm = LSTMModel(input_size, hidden_size, num_layers, exog_size, output_size)\n    # Create a TensorDataset with your features and target\n    train_data = TensorDataset(X_lags_tensor, X_exog_tensor, y_tensor)\n    # Create a DataLoader to handle mini-batching\n    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n\n    # TRAIN MODEL\n    train_lstm_with_minibatches(lstm, train_loader, epochs=epochs, learning_rate=learning_rate)\n\n\n    # PACK MODEL\n    model = {\"lstm\": lstm, 'hyperparameter': hyperparameter, \"train_df_X\": train_df_X, \"train_df_y\": train_df_y}\n\n\n    return model\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m14_gru.produce_forecast_m14_gru","title":"<code>produce_forecast_m14_gru(model, train_df_X, test_df_X)</code>","text":"<p>Generate forecasts using a trained GRU model for both train and test sets.</p> <p>The GRU model processes lag features to capture temporal patterns and  combines them with exogenous variables via a fully connected layer to produce  the predictions. Minibatching is used to avoid memory issues.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>dict</code> <p>Dictionary containing the trained GRU model and hyperparameters.</p> required <code>train_df_X</code> <code>DataFrame</code> <p>Input features for the training set (lag + exogenous).</p> required <code>test_df_X</code> <code>DataFrame</code> <p>Input features for the test set (lag + exogenous).</p> required <p>Returns:</p> Name Type Description <code>train_df_y_hat</code> <code>ndarray</code> <p>Forecasted values for the training set.</p> <code>test_df_y_hat</code> <code>ndarray</code> <p>Forecasted values for the test set.</p> Source code in <code>docs\\notebooks\\model\\m14_gru.py</code> <pre><code>def produce_forecast_m14_gru(model, train_df_X, test_df_X):\n    \"\"\"\n    Generate forecasts using a trained GRU model for both train and test sets.\n\n    The GRU model processes lag features to capture temporal patterns and \n    combines them with exogenous variables via a fully connected layer to produce \n    the predictions. Minibatching is used to avoid memory issues.\n\n    Args:\n        model (dict): Dictionary containing the trained GRU model and hyperparameters.\n        train_df_X (pd.DataFrame): Input features for the training set (lag + exogenous).\n        test_df_X (pd.DataFrame): Input features for the test set (lag + exogenous).\n\n    Returns:\n        train_df_y_hat (np.ndarray): Forecasted values for the training set.\n        test_df_y_hat (np.ndarray): Forecasted values for the test set.\n    \"\"\"\n\n    gru = model['gru']\n    hyperparameter = model['hyperparameter']\n    input_size = int(hyperparameter['input_size'])\n    batch_size = int(hyperparameter['batch_size'])\n\n    def produce_forecast(gru, X):\n        X_lags, X_exog = separate_lag_and_exogenous_features(X)\n        X_lags_tensor = torch.tensor(X_lags.values, dtype=torch.float32)\n        X_exog_tensor = torch.tensor(X_exog.values, dtype=torch.float32)\n\n        total_lag_features = X_lags_tensor.shape[1]\n        sequence_length = total_lag_features // input_size\n        X_lags_tensor = X_lags_tensor.view(-1, sequence_length, input_size)\n\n        predictions = []\n        for i in range(0, len(X_lags_tensor), batch_size):\n            batch_X_lags = X_lags_tensor[i:i+batch_size]\n            batch_X_exog = X_exog_tensor[i:i+batch_size]\n            with torch.no_grad():\n                batch_pred = gru(batch_X_lags, batch_X_exog)\n            predictions.append(batch_pred)\n        return torch.cat(predictions, dim=0).detach().numpy()\n\n    train_df_y_hat = produce_forecast(gru, train_df_X)\n    test_df_y_hat = produce_forecast(gru, test_df_X)\n    return train_df_y_hat, test_df_y_hat\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m14_gru.train_model_m14_gru","title":"<code>train_model_m14_gru(hyperparameter, train_df_X, train_df_y)</code>","text":"<p>Train a GRU-based model for point forecasting.</p> <p>The GRU captures temporal patterns from lag features, and a fully connected layer integrates the last hidden state with exogenous variables to produce the final prediction.</p> <p>Parameters:</p> Name Type Description Default <code>hyperparameter</code> <code>dict</code> <p>Dictionary of hyperparameters, including: - 'seed': random seed for reproducibility - 'input_size': number of input features per time step - 'hidden_size': number of hidden units in GRU - 'num_layers': number of GRU layers - 'output_size': number of outputs - 'batch_size': minibatch size - 'epochs': number of training epochs - 'learning_rate': optimizer learning rate</p> required <code>train_df_X</code> <code>DataFrame</code> <p>DataFrame of input features (lag + exogenous).</p> required <code>train_df_y</code> <code>DataFrame</code> <p>DataFrame of target values.</p> required <p>Returns:</p> Name Type Description <code>model</code> <code>dict</code> <p>Dictionary containing: - 'gru': trained GRU model - 'hyperparameter': hyperparameters used for training - 'train_df_X': training features - 'train_df_y': training target</p> Source code in <code>docs\\notebooks\\model\\m14_gru.py</code> <pre><code>def train_model_m14_gru(hyperparameter, train_df_X, train_df_y):\n    \"\"\"\n    Train a GRU-based model for point forecasting.\n\n    The GRU captures temporal patterns from lag features, and a fully connected\n    layer integrates the last hidden state with exogenous variables to produce\n    the final prediction.\n\n    Args:\n        hyperparameter (dict): Dictionary of hyperparameters, including:\n            - 'seed': random seed for reproducibility\n            - 'input_size': number of input features per time step\n            - 'hidden_size': number of hidden units in GRU\n            - 'num_layers': number of GRU layers\n            - 'output_size': number of outputs\n            - 'batch_size': minibatch size\n            - 'epochs': number of training epochs\n            - 'learning_rate': optimizer learning rate\n        train_df_X (pd.DataFrame): DataFrame of input features (lag + exogenous).\n        train_df_y (pd.DataFrame): DataFrame of target values.\n\n    Returns:\n        model (dict): Dictionary containing:\n            - 'gru': trained GRU model\n            - 'hyperparameter': hyperparameters used for training\n            - 'train_df_X': training features\n            - 'train_df_y': training target\n    \"\"\"\n\n    # UNPACK HYPERPARAMETER\n    seed = int(hyperparameter['seed'])\n    input_size = int(hyperparameter['input_size'])\n    hidden_size = int(hyperparameter['hidden_size'])\n    num_layers = int(hyperparameter['num_layers'])\n    output_size = int(hyperparameter['output_size'])\n    batch_size = int(hyperparameter['batch_size'])\n    epochs = int(hyperparameter['epochs'])\n    learning_rate = hyperparameter['learning_rate']\n\n    # DEFINE MODEL\n    class GRUModel(nn.Module):\n        def __init__(self, input_size, hidden_size, num_layers, exog_size, output_size=1):\n            super(GRUModel, self).__init__()\n            self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n            self.fc = nn.Linear(hidden_size + exog_size, output_size)\n\n        def forward(self, x, exogenous_data):\n            out, h_n = self.gru(x)\n            last_hidden_state = out[:, -1, :]\n            combined_input = torch.cat((last_hidden_state, exogenous_data), dim=1)\n            out = self.fc(combined_input)\n            return out\n\n    def train_gru_with_minibatches(model, train_loader, epochs, learning_rate=0.001):\n        criterion = nn.MSELoss()\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n        for epoch in range(epochs):\n            print(f'Epoch [{epoch+1}/{epochs}]')\n            start_time = time.time()\n            model.train()\n            batch_no = 1\n            for X_lags_batch, X_exog_batch, y_batch in train_loader:\n                print(f'Epoch [{epoch+1}/{epochs}] and batch [{batch_no}/{len(train_loader)}]')\n                batch_no += 1\n\n                predictions = model(X_lags_batch, X_exog_batch)\n                loss = criterion(predictions, y_batch)\n\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}, time taken: {time.time() - start_time:.2f}s')\n\n    def set_seed(seed=seed):\n        random.seed(seed)\n        np.random.seed(seed)\n        torch.manual_seed(seed)\n        os.environ[\"PYTHONHASHSEED\"] = str(seed)\n\n    # PREPARE TRAIN DATA\n    X_lags, X_exog = separate_lag_and_exogenous_features(train_df_X)\n    X_lags_tensor = torch.tensor(X_lags.values, dtype=torch.float32)\n    X_exog_tensor = torch.tensor(X_exog.values, dtype=torch.float32)\n    y_tensor = torch.tensor(train_df_y.values, dtype=torch.float32).view(-1, 1)\n\n    total_lag_features = X_lags_tensor.shape[1]\n    sequence_length = total_lag_features // input_size\n    exog_size = X_exog_tensor.shape[1]\n\n    X_lags_tensor = X_lags_tensor.view(-1, sequence_length, input_size)\n\n    # INITIALIZE MODEL + DATALOADER\n    set_seed(seed=seed)\n    gru = GRUModel(input_size, hidden_size, num_layers, exog_size, output_size)\n    train_data = TensorDataset(X_lags_tensor, X_exog_tensor, y_tensor)\n    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n\n    # TRAIN MODEL\n    train_gru_with_minibatches(gru, train_loader, epochs=epochs, learning_rate=learning_rate)\n\n    # PACK MODEL\n    model = {\"gru\": gru, 'hyperparameter': hyperparameter, \"train_df_X\": train_df_X, \"train_df_y\": train_df_y}\n    return model\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m15_transformer.produce_forecast_m15_transformer","title":"<code>produce_forecast_m15_transformer(model, train_df_X, test_df_X)</code>","text":"<p>Generate forecasts for train and test sets using a trained Transformer model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>dict</code> <p>Dictionary containing the trained Transformer model and hyperparameters.</p> required <code>train_df_X</code> <code>DataFrame</code> <p>Predictors of the training set (lag + exogenous features).</p> required <code>test_df_X</code> <code>DataFrame</code> <p>Predictors of the test set (lag + exogenous features).</p> required <p>Returns:</p> Name Type Description <code>train_df_y_hat</code> <code>ndarray</code> <p>Forecast results for the training set.</p> <code>test_df_y_hat</code> <code>ndarray</code> <p>Forecast results for the test set.</p> Source code in <code>docs\\notebooks\\model\\m15_transformer.py</code> <pre><code>def produce_forecast_m15_transformer(model, train_df_X, test_df_X):\n    \"\"\"\n    Generate forecasts for train and test sets using a trained Transformer model.\n\n    Args:\n        model (dict): Dictionary containing the trained Transformer model and hyperparameters.\n        train_df_X (pd.DataFrame): Predictors of the training set (lag + exogenous features).\n        test_df_X (pd.DataFrame): Predictors of the test set (lag + exogenous features).\n\n    Returns:\n        train_df_y_hat (np.ndarray): Forecast results for the training set.\n        test_df_y_hat (np.ndarray): Forecast results for the test set.\n    \"\"\"\n    transformer = model['transformer']\n    hyperparameter = model['hyperparameter']\n    batch_size = int(hyperparameter['batch_size'])\n    input_size = int(hyperparameter['input_size'])\n\n    def produce_forecast(transformer, X):\n        X_lags, X_exog = separate_lag_and_exogenous_features(X)\n        X_lags_tensor = torch.tensor(X_lags.values, dtype=torch.float32)\n        X_exog_tensor = torch.tensor(X_exog.values, dtype=torch.float32)\n        total_lag_features = X_lags_tensor.shape[1]\n        sequence_length = total_lag_features // input_size\n        X_lags_tensor = X_lags_tensor.view(-1, sequence_length, input_size)\n        predictions = []\n        for i in range(0, len(X_lags_tensor), batch_size):\n            batch_X_lags = X_lags_tensor[i:i+batch_size]\n            batch_X_exog = X_exog_tensor[i:i+batch_size]\n            with torch.no_grad():\n                batch_pred = transformer(batch_X_lags, batch_X_exog)\n            predictions.append(batch_pred)\n        return torch.cat(predictions, dim=0).detach().numpy()\n\n    train_df_y_hat = produce_forecast(transformer, train_df_X)\n    test_df_y_hat = produce_forecast(transformer, test_df_X)\n    return train_df_y_hat, test_df_y_hat\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m15_transformer.train_model_m15_transformer","title":"<code>train_model_m15_transformer(hyperparameter, train_df_X, train_df_y)</code>","text":"<p>Train a Transformer model for point forecasting using lag and exogenous features.</p> <p>The model uses a Transformer encoder to capture temporal dependencies in lag features, and a fully connected layer to combine the last hidden state with exogenous features.</p> <p>Parameters:</p> Name Type Description Default <code>hyperparameter</code> <code>dict</code> <p>Dictionary containing model hyperparameters including seed, input_size, hidden_size, num_layers, output_size, batch_size, epochs, nhead, and learning_rate.</p> required <code>train_df_X</code> <code>DataFrame</code> <p>Feature matrix for training (lag + exogenous features).</p> required <code>train_df_y</code> <code>DataFrame</code> <p>Target variable for training.</p> required <p>Returns:</p> Name Type Description <code>model</code> <code>dict</code> <p>Dictionary containing the trained Transformer model and associated hyperparameters, as well as the original training data.</p> Source code in <code>docs\\notebooks\\model\\m15_transformer.py</code> <pre><code>def train_model_m15_transformer(hyperparameter, train_df_X, train_df_y):\n    \"\"\"\n    Train a Transformer model for point forecasting using lag and exogenous features.\n\n    The model uses a Transformer encoder to capture temporal dependencies in lag features,\n    and a fully connected layer to combine the last hidden state with exogenous features.\n\n    Args:\n        hyperparameter (dict): Dictionary containing model hyperparameters including\n            seed, input_size, hidden_size, num_layers, output_size, batch_size,\n            epochs, nhead, and learning_rate.\n        train_df_X (pd.DataFrame): Feature matrix for training (lag + exogenous features).\n        train_df_y (pd.DataFrame): Target variable for training.\n\n    Returns:\n        model (dict): Dictionary containing the trained Transformer model and associated\n            hyperparameters, as well as the original training data.\n    \"\"\"\n\n    # UNPACK HYPERPARAMETER\n    seed = int(hyperparameter['seed'])\n    input_size = int(hyperparameter['input_size'])\n    hidden_size = int(hyperparameter['hidden_size'])\n    num_layers = int(hyperparameter['num_layers'])\n    output_size = int(hyperparameter['output_size'])\n    batch_size = int(hyperparameter['batch_size'])\n    epochs = int(hyperparameter['epochs'])\n    nhead = int(hyperparameter['nhead'])\n    learning_rate = hyperparameter['learning_rate']\n\n    import torch\n    import torch.nn as nn\n    import torch.optim as optim\n    import random, numpy as np, os, time\n    from torch.utils.data import DataLoader, TensorDataset\n\n    # TRANSFORMER MODEL\n    class TransformerModel(nn.Module):\n        def __init__(self, input_size, hidden_size, num_layers, exog_size, output_size=1):\n            super(TransformerModel, self).__init__()\n            # Transformer embedding\n            self.embedding = nn.Linear(input_size, hidden_size)\n            encoder_layer = nn.TransformerEncoderLayer(\n                d_model=hidden_size,\n                nhead=nhead,\n                dim_feedforward=hidden_size * 2,\n                batch_first=True\n            )\n            self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n            # Fully connected output layer\n            self.fc = nn.Linear(hidden_size + exog_size, output_size)\n\n        def forward(self, x, exogenous_data):\n            x = self.embedding(x) \n            x = self.transformer_encoder(x)\n            last_hidden_state = x[:, -1, :]\n            combined_input = torch.cat((last_hidden_state, exogenous_data), dim=1)\n            out = self.fc(combined_input)\n            return out\n\n    def train_transformer_with_minibatches(model, train_loader, epochs, learning_rate=learning_rate):\n        criterion = nn.MSELoss()\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n        for epoch in range(epochs):\n            print(f'Epoch [{epoch+1}/{epochs}]')\n            start_time = time.time()\n            model.train()\n            batch_no = 1\n            for X_lags_batch, X_exog_batch, y_batch in train_loader:\n                print(f'Epoch [{epoch+1}/{epochs}] batch [{batch_no}/{len(train_loader)}]')\n                batch_no += 1\n                predictions = model(X_lags_batch, X_exog_batch)\n                loss = criterion(predictions, y_batch)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            end_time = time.time()\n            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}, time: {end_time - start_time:.2f}s')\n\n    def set_seed(seed=seed):\n        random.seed(seed)\n        np.random.seed(seed)\n        torch.manual_seed(seed)\n        os.environ[\"PYTHONHASHSEED\"] = str(seed)\n\n    # --- DATA PREP ---\n    X_lags, X_exog = separate_lag_and_exogenous_features(train_df_X)\n    X_lags_tensor = torch.tensor(X_lags.values, dtype=torch.float32)\n    X_exog_tensor = torch.tensor(X_exog.values, dtype=torch.float32)\n    y_tensor = torch.tensor(train_df_y.values, dtype=torch.float32).view(-1, 1)\n    total_lag_features = X_lags_tensor.shape[1]\n    sequence_length = total_lag_features // input_size\n    exog_size = X_exog_tensor.shape[1]\n    X_lags_tensor = X_lags_tensor.view(-1, sequence_length, input_size)\n\n    # --- INIT MODEL AND DATALOADER ---\n    set_seed(seed)\n    transformer = TransformerModel(input_size, hidden_size, num_layers, exog_size, output_size)\n    train_data = TensorDataset(X_lags_tensor, X_exog_tensor, y_tensor)\n    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n    train_transformer_with_minibatches(transformer, train_loader, epochs=epochs, learning_rate=learning_rate)\n\n    model = {\"transformer\": transformer, 'hyperparameter': hyperparameter, \"train_df_X\": train_df_X, \"train_df_y\": train_df_y}\n    return model\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m16_prophet.produce_forecast_m16_prophet","title":"<code>produce_forecast_m16_prophet(model, train_df_X, test_df_X, train_df_y, forecast_horizon)</code>","text":"<p>Generate forecasts for train and test sets using a trained Prophet model, with warm-start updates for test set.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>dict</code> <p>Dictionary containing the trained Prophet model, training data, and hyperparameters.</p> required <code>train_df_X</code> <code>DataFrame</code> <p>Predictors of the training set (lag + exogenous features).</p> required <code>test_df_X</code> <code>DataFrame</code> <p>Predictors of the test set (lag + exogenous features).</p> required <code>train_df_y</code> <code>DataFrame</code> <p>Target series of the training set.</p> required <code>forecast_horizon</code> <code>int</code> <p>Forecast horizon in minutes.</p> required <p>Returns:</p> Name Type Description <code>train_df_y_hat</code> <code>DataFrame</code> <p>Forecast results for the training set, indexed by datetime.</p> <code>test_df_y_hat</code> <code>DataFrame</code> <p>Forecast results for the test set, indexed by datetime.</p> Source code in <code>docs\\notebooks\\model\\m16_prophet.py</code> <pre><code>def produce_forecast_m16_prophet(model, train_df_X, test_df_X, train_df_y, forecast_horizon):\n    \"\"\"\n    Generate forecasts for train and test sets using a trained Prophet model, with warm-start updates for test set.\n\n    Args:\n        model (dict): Dictionary containing the trained Prophet model, training data, and hyperparameters.\n        train_df_X (pd.DataFrame): Predictors of the training set (lag + exogenous features).\n        test_df_X (pd.DataFrame): Predictors of the test set (lag + exogenous features).\n        train_df_y (pd.DataFrame): Target series of the training set.\n        forecast_horizon (int): Forecast horizon in minutes.\n\n    Returns:\n        train_df_y_hat (pd.DataFrame): Forecast results for the training set, indexed by datetime.\n        test_df_y_hat (pd.DataFrame): Forecast results for the test set, indexed by datetime.\n    \"\"\"\n\n    # UNPACK MODEL\n    prophet_model = model['prophet']\n    y = model['y']\n    hyperparameter = model['hyperparameter']\n\n    #UNPACK HYPERPARAMETER\n    seasonality_prior_scale = hyperparameter[\"seasonality_prior_scale\"]\n    seasonality_mode = hyperparameter[\"seasonality_mode\"]\n    weekly_seasonality = hyperparameter[\"weekly_seasonality\"]\n    daily_seasonality = hyperparameter[\"daily_seasonality\"]\n    growth = hyperparameter[\"growth\"]\n\n    # Set up X_exog which is used for prediction\n    timestep_frequency = test_df_X.index[1] - test_df_X.index[0]\n    n_timestep_forecast_horizon = int(forecast_horizon / (timestep_frequency.total_seconds() / 60))\n\n    train_df_X_updated = remove_jump_df(train_df_X)\n    test_df_X_updated = remove_jump_df(test_df_X)\n\n    X_lags, X_exog = separate_lag_and_exogenous_features(train_df_X_updated)\n\n    X_exog.reset_index(inplace=True)\n    X_exog.rename(columns={'datetime': 'ds'}, inplace=True)\n\n    # Forecast train set\n    train_df_y_hat = prophet_model.predict(X_exog)\n\n    train_df_y_hat = train_df_y_hat[['ds', 'yhat']]\n\n    train_df_y_hat.set_index('ds', inplace=True)\n    train_df_y_hat.index.name = 'datetime'\n\n    # Set up function to warm start the model for updating the fit\n    def warm_start_params(m):\n        \"\"\"\n        Retrieve parameters from a trained model in the format used to initialize a new Stan model.\n        Note that the new Stan model must have these same settings:\n            n_changepoints, seasonality features, mcmc sampling\n        for the retrieved parameters to be valid for the new model.\n\n        Parameters\n        ----------\n        m: A trained model of the Prophet class.\n\n        Returns\n        -------\n        A Dictionary containing retrieved parameters of m.\n        \"\"\"\n        res = {}\n        for pname in ['k', 'm', 'sigma_obs']:\n            if m.mcmc_samples == 0:\n                res[pname] = m.params[pname][0][0]\n            else:\n                res[pname] = np.mean(m.params[pname])\n        for pname in ['delta', 'beta']:\n            if m.mcmc_samples == 0:\n                res[pname] = m.params[pname][0]\n            else:\n                res[pname] = np.mean(m.params[pname], axis=0)\n        return res\n\n    # PRODUCE FORECASTFOR TEST SET\n\n    # REFIT THE MODEL AND PRODUCE NEW FORECAST FOR TEST SET\n    # The model is refitted for 100 times only so there will be only 100 forecast results. \n\n    test_df_y_hat = pd.DataFrame(index = test_df_X.index)\n    test_df_y_hat['y_hat'] = np.nan\n\n\n    # in the case of CV 10, which is when test df &lt; train df\n    # don't compute the test forecast\n    if (test_df_X.index[-1] &lt; train_df_X.index[0]):\n    # this is the case when we use CV10, where the test set is before the train set\n        print(\"Test set is before train set / CV 10, no test forecast can be made\")\n        return train_df_y_hat, test_df_y_hat\n\n    _, X_test = separate_lag_and_exogenous_features(test_df_X)\n    X_test.reset_index(inplace=True)\n    X_test.rename(columns={'datetime': 'ds'}, inplace=True)\n\n    n_update = 100\n    n_timesteps_per_update = int(len(test_df_y_hat) / (n_update + 1))\n\n    # TRANSFORM test_df_X to a series with only the last lag\n    horizon_timedelta = pd.Timedelta(minutes=forecast_horizon)\n    last_observation = f'y_lag_{horizon_timedelta}m'\n    test_df_y_last = test_df_X[last_observation]\n\n    new_y = pd.DataFrame(test_df_y_last)\n    new_y.rename(columns={new_y.columns[0]: 'y'}, inplace=True)\n    new_y.insert(0, 'ds', new_y.index - pd.Timedelta(minutes=forecast_horizon))\n    new_y.reset_index(drop = True, inplace=True)\n\n    new_y = new_y.drop(0, axis=0).reset_index(drop=True)\n    X_exog_complete = pd.concat([X_exog, X_test], axis=0)\n    X_exog_complete = X_exog_complete.drop(0, axis=0).reset_index(drop=True)\n    new_y = pd.merge(new_y, X_exog_complete, on='ds', how='left')\n\n    for i in range(n_update):\n    # for i in range(2): #for test only\n        print('Processing i = ', i + 1, ' out of ', n_update),\n        if i == 0:\n            X_test_curr = X_test.iloc[:1,:]\n            test_df_y_hat.iloc[i, 0] = prophet_model.predict(X_test_curr)['yhat'].values[0]\n        else:\n            new_rows = new_y.iloc[(i-1)*n_timesteps_per_update : i*n_timesteps_per_update, :]\n            y = pd.concat([y, new_rows], ignore_index=True)\n\n            current_params = warm_start_params(prophet_model)\n\n            prophet_model = Prophet(\n                seasonality_prior_scale=seasonality_prior_scale,  # Example hyperparameter for seasonality strength\n                seasonality_mode=seasonality_mode,  # Use multiplicative seasonality\n                weekly_seasonality=weekly_seasonality,  # Enable weekly seasonality\n                daily_seasonality=daily_seasonality,  # Enable daily seasonality\n                growth=growth,  # Choose between 'linear' or 'logistic' growth\n            )\n\n            prophet_model = prophet_model.fit(y, init=current_params)  # Adding the last day, warm-starting from the prev model\n            X_test_curr = X_test.iloc[i*n_timesteps_per_update : (1+i*n_timesteps_per_update),:]\n            test_df_y_hat.iloc[i*n_timesteps_per_update, 0] = prophet_model.predict(X_test_curr)['yhat'].values[0]\n\n    return train_df_y_hat, test_df_y_hat\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m16_prophet.train_model_m16_prophet","title":"<code>train_model_m16_prophet(hyperparameter, train_df_X, train_df_y, forecast_horizon)</code>","text":"<p>Train a Prophet model for point forecasting using lag and exogenous features.</p> <p>The model captures trend and seasonality components in the target series, and optionally includes exogenous regressors. A gap is introduced between training and forecast horizon to prevent data leakage.</p> <p>Parameters:</p> Name Type Description Default <code>hyperparameter</code> <code>dict</code> <p>Dictionary containing model hyperparameters, including seed, seasonality_prior_scale, seasonality_mode, weekly_seasonality, daily_seasonality, and growth.</p> required <code>train_df_X</code> <code>DataFrame</code> <p>Feature matrix for training (lag + exogenous features).</p> required <code>train_df_y</code> <code>DataFrame</code> <p>Target variable for training.</p> required <code>forecast_horizon</code> <code>int</code> <p>Forecast horizon to define the gap between training and testing.</p> required <p>Returns:</p> Name Type Description <code>model</code> <code>dict</code> <p>Dictionary containing the trained Prophet model, the processed target DataFrame, and associated hyperparameters.</p> Source code in <code>docs\\notebooks\\model\\m16_prophet.py</code> <pre><code>def train_model_m16_prophet(hyperparameter, train_df_X, train_df_y, forecast_horizon):\n    \"\"\"\n    Train a Prophet model for point forecasting using lag and exogenous features.\n\n    The model captures trend and seasonality components in the target series,\n    and optionally includes exogenous regressors. A gap is introduced between\n    training and forecast horizon to prevent data leakage.\n\n    Args:\n        hyperparameter (dict): Dictionary containing model hyperparameters, including\n            seed, seasonality_prior_scale, seasonality_mode, weekly_seasonality,\n            daily_seasonality, and growth.\n        train_df_X (pd.DataFrame): Feature matrix for training (lag + exogenous features).\n        train_df_y (pd.DataFrame): Target variable for training.\n        forecast_horizon (int): Forecast horizon to define the gap between training and testing.\n\n    Returns:\n        model (dict): Dictionary containing the trained Prophet model, the processed\n            target DataFrame, and associated hyperparameters.\n    \"\"\"\n\n    #UNPACK HYPERPARAMETER\n    seed = hyperparameter[\"seed\"]\n    seasonality_prior_scale = hyperparameter[\"seasonality_prior_scale\"]\n    seasonality_mode = hyperparameter[\"seasonality_mode\"]\n    weekly_seasonality = hyperparameter[\"weekly_seasonality\"]\n    daily_seasonality = hyperparameter[\"daily_seasonality\"]\n    growth = hyperparameter[\"growth\"]\n\n\n    # UPDATE train_df to exclude all rows after a sudden jump in the timestep\n    train_df_y_updated = remove_jump_df(train_df_y)\n    train_df_X_updated = remove_jump_df(train_df_X)\n\n    # Calculate the frequency of the timesteps using the first and second index values\n    timestep_frequency = train_df_y_updated.index[1] - train_df_y_updated.index[0]\n    inferred_frequency = pd.infer_freq(train_df_y_updated.index)\n    train_df_y_updated = train_df_y_updated.asfreq(inferred_frequency) \n\n    # INTRODUCE GAP BETWEEN TRAIN AND TEST SET TO AVOID DATA LEAKAGE\n    n_timestep_forecast_horizon = int(forecast_horizon / (timestep_frequency.total_seconds() / 60))\n    if n_timestep_forecast_horizon == 1:\n        pass\n    else:\n        train_df_y_updated = train_df_y_updated[:-(n_timestep_forecast_horizon - 1)]\n        train_df_X_updated = train_df_X_updated[:-(n_timestep_forecast_horizon - 1)]\n\n    # Assuming train_df_y_updated is your dataframe and 'y' is the column with the training series\n    y = train_df_y_updated.copy()\n    X_lags, X_exog = separate_lag_and_exogenous_features(train_df_X_updated)\n\n    #Initialize the Prophet model with hyperparameters\n    prophet_model = Prophet(\n        seasonality_prior_scale=seasonality_prior_scale,  # Example hyperparameter for seasonality strength\n        seasonality_mode=seasonality_mode,  # Use multiplicative seasonality\n        weekly_seasonality=weekly_seasonality,  # Enable weekly seasonality\n        daily_seasonality=daily_seasonality,  # Enable daily seasonality\n        growth=growth  # Choose between 'linear' or 'logistic' growth\n        # random_state =  seed,  # cannot set seed in prophet\n    )\n    for col in X_exog.columns:\n        prophet_model.add_regressor(col)\n\n    # Add exogenous features to the y DataFrame\n    y = y.merge(X_exog, on='datetime')\n    y.reset_index(inplace=True)\n    y.rename(columns={'datetime': 'ds'}, inplace=True)\n\n    # Train model\n    prophet_model.fit(y)\n\n    # PACK MODEL\n    model = {\"prophet\": prophet_model, \"y\": y, \"hyperparameter\": hyperparameter}\n\n\n    return model\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m17_xgb.produce_forecast_m17_xgb","title":"<code>produce_forecast_m17_xgb(model, train_df_X, test_df_X)</code>","text":"<p>Generate point forecasts for train and test sets using a trained XGBoost model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>dict</code> <p>Dictionary containing the trained XGBoost model under the key 'xgb'.</p> required <code>train_df_X</code> <code>DataFrame</code> <p>Feature matrix for the training set.</p> required <code>test_df_X</code> <code>DataFrame</code> <p>Feature matrix for the test set.</p> required <p>Returns:</p> Name Type Description <code>train_df_y_hat</code> <code>ndarray</code> <p>Predicted values for the training set.</p> <code>test_df_y_hat</code> <code>ndarray</code> <p>Predicted values for the test set.</p> Source code in <code>docs\\notebooks\\model\\m17_xgb.py</code> <pre><code>def produce_forecast_m17_xgb(model, train_df_X, test_df_X):\n    \"\"\"\n    Generate point forecasts for train and test sets using a trained XGBoost model.\n\n    Args:\n        model (dict): Dictionary containing the trained XGBoost model under the key 'xgb'.\n        train_df_X (pd.DataFrame): Feature matrix for the training set.\n        test_df_X (pd.DataFrame): Feature matrix for the test set.\n\n    Returns:\n        train_df_y_hat (np.ndarray): Predicted values for the training set.\n        test_df_y_hat (np.ndarray): Predicted values for the test set.\n    \"\"\"\n\n    # UNPACK MODEL\n    xgb = model[\"xgb\"]\n\n    # PRODUCE FORECAST\n    train_df_y_hat = xgb.predict(train_df_X)\n    test_df_y_hat = xgb.predict(test_df_X)\n\n    return train_df_y_hat, test_df_y_hat\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m17_xgb.train_model_m17_xgb","title":"<code>train_model_m17_xgb(hyperparameter, train_df_X, train_df_y)</code>","text":"<p>Train an XGBoost regressor for point forecasting.</p> <p>Parameters:</p> Name Type Description Default <code>hyperparameter</code> <code>dict</code> <p>Dictionary containing model hyperparameters (e.g., n_estimators, learning_rate, max_depth, subsample, colsample_bytree, xgb_seed).</p> required <code>train_df_X</code> <code>DataFrame</code> <p>Features matrix for training.</p> required <code>train_df_y</code> <code>DataFrame</code> <p>Target vector for training.</p> required <p>Returns:</p> Name Type Description <code>model</code> <code>dict</code> <p>Dictionary containing the trained XGBoost model under the key 'xgb'.</p> Source code in <code>docs\\notebooks\\model\\m17_xgb.py</code> <pre><code>def train_model_m17_xgb(hyperparameter, train_df_X, train_df_y):\n    \"\"\"\n    Train an XGBoost regressor for point forecasting.\n\n    Args:\n        hyperparameter (dict): Dictionary containing model hyperparameters (e.g., n_estimators, learning_rate, max_depth, subsample, colsample_bytree, xgb_seed).\n        train_df_X (pd.DataFrame): Features matrix for training.\n        train_df_y (pd.DataFrame): Target vector for training.\n\n    Returns:\n        model (dict): Dictionary containing the trained XGBoost model under the key 'xgb'.\n    \"\"\"\n\n    #UNPACK HYPERPARAMETER\n    xgb_seed = int(hyperparameter[\"xgb_seed\"])\n    n_estimators=hyperparameter[\"n_estimators\"]\n    learning_rate=hyperparameter[\"learning_rate\"]\n    max_depth=hyperparameter[\"max_depth\"]\n    subsample=hyperparameter[\"subsample\"]\n    colsample_bytree=hyperparameter[\"colsample_bytree\"]\n\n    #INITIALIZE AND TRAIN MODEL\n    xgb = XGBRegressor(n_estimators=200, learning_rate=0.1, max_depth=6, subsample=0.8, colsample_bytree=0.8, random_state=xgb_seed)   \n    xgb.fit(train_df_X, train_df_y)   \n\n    # PACK MODEL\n    model = {\"xgb\": xgb}\n\n\n    return model\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m18_nbeats.produce_forecast_m18_nbeats","title":"<code>produce_forecast_m18_nbeats(model, train_df_X, test_df_X)</code>","text":"<p>Generate forecasts for train and test sets using a trained NBeats model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Trained NBeats PyTorch model.</p> required <code>train_df_X</code> <code>DataFrame</code> <p>Feature matrix for the training set.</p> required <code>test_df_X</code> <code>DataFrame</code> <p>Feature matrix for the test set.</p> required <p>Returns:</p> Name Type Description <code>train_df_y_hat</code> <code>DataFrame</code> <p>Forecasted values for the training set.</p> <code>test_df_y_hat</code> <code>DataFrame</code> <p>Forecasted values for the test set.</p> Source code in <code>docs\\notebooks\\model\\m18_nbeats.py</code> <pre><code>def produce_forecast_m18_nbeats(model, train_df_X, test_df_X):\n    \"\"\"\n    Generate forecasts for train and test sets using a trained NBeats model.\n\n    Args:\n        model (torch.nn.Module): Trained NBeats PyTorch model.\n        train_df_X (pd.DataFrame): Feature matrix for the training set.\n        test_df_X (pd.DataFrame): Feature matrix for the test set.\n\n    Returns:\n        train_df_y_hat (pd.DataFrame): Forecasted values for the training set.\n        test_df_y_hat (pd.DataFrame): Forecasted values for the test set.\n    \"\"\"\n    import torch\n    model.eval()\n\n    with torch.no_grad():\n        X_train_tensor = torch.tensor(train_df_X.values, dtype=torch.float32)\n        X_test_tensor = torch.tensor(test_df_X.values, dtype=torch.float32)\n\n        y_train_hat = model(X_train_tensor).numpy()\n        y_test_hat = model(X_test_tensor).numpy()\n\n    import pandas as pd\n    train_df_y_hat = pd.DataFrame(y_train_hat, index=train_df_X.index, columns=['y_hat'])\n    test_df_y_hat = pd.DataFrame(y_test_hat, index=test_df_X.index, columns=['y_hat'])\n\n    return train_df_y_hat, test_df_y_hat\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m18_nbeats.train_model_m18_nbeats","title":"<code>train_model_m18_nbeats(hyperparameter, train_df_X, train_df_y)</code>","text":"<p>Train an NBeats model for point forecasting using lag and exogenous features.</p> <p>Parameters:</p> Name Type Description Default <code>hyperparameter</code> <code>dict</code> <p>Model hyperparameters (e.g., hidden_size, num_blocks, num_layers, lr, output_size, epochs, seed).</p> required <code>train_df_X</code> <code>DataFrame</code> <p>Feature matrix for training.</p> required <code>train_df_y</code> <code>DataFrame</code> <p>Target values for training.</p> required <p>Returns:</p> Name Type Description <code>model</code> <code>Module</code> <p>Trained NBeats PyTorch model.</p> Source code in <code>docs\\notebooks\\model\\m18_nbeats.py</code> <pre><code>def train_model_m18_nbeats(hyperparameter, train_df_X, train_df_y):\n    \"\"\"\n    Train an NBeats model for point forecasting using lag and exogenous features.\n\n    Args:\n        hyperparameter (dict): Model hyperparameters (e.g., hidden_size, num_blocks, num_layers, lr, output_size, epochs, seed).\n        train_df_X (pd.DataFrame): Feature matrix for training.\n        train_df_y (pd.DataFrame): Target values for training.\n\n    Returns:\n        model (torch.nn.Module): Trained NBeats PyTorch model.\n    \"\"\"\n    # ---- Unpack hyperparameters ----\n    input_size = train_df_X.shape[1]\n    output_size = int(hyperparameter['output_size'])\n    hidden_size = int(hyperparameter['hidden_size'])\n    num_blocks = int(hyperparameter['num_blocks'])\n    num_layers = int(hyperparameter['num_layers'])\n    lr = hyperparameter['lr']\n    epochs = int(hyperparameter['epochs'])\n    seed = int(hyperparameter['seed'])\n\n    # ---- Set seeds for reproducibility ----\n    import torch, numpy as np, random\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n    # ---- Define NBeats model inside the function ----\n    import torch.nn as nn\n    class NBeatsModel(nn.Module):\n        def __init__(self, input_size, output_size, hidden_size, num_blocks, num_layers):\n            super(NBeatsModel, self).__init__()\n            blocks = []\n            for _ in range(num_blocks):\n                block = []\n                for l in range(num_layers):\n                    block.append(nn.Linear(input_size if l==0 else hidden_size, hidden_size))\n                    block.append(nn.ReLU())\n                block.append(nn.Linear(hidden_size, output_size))\n                blocks.append(nn.Sequential(*block))\n            self.blocks = nn.ModuleList(blocks)\n\n        def forward(self, x):\n            out = 0\n            for block in self.blocks:\n                out += block(x)\n            return out\n\n    model = NBeatsModel(input_size, output_size, hidden_size, num_blocks, num_layers)\n\n    # ---- Training setup ----\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    criterion = nn.MSELoss()\n    X_tensor = torch.tensor(train_df_X.values, dtype=torch.float32)\n    y_tensor = torch.tensor(train_df_y.values, dtype=torch.float32)\n\n    # ---- Training loop ----\n    model.train()\n    for epoch in range(epochs):\n        optimizer.zero_grad()\n        output = model(X_tensor)\n        loss = criterion(output, y_tensor)\n        loss.backward()\n        optimizer.step()\n\n    return model\n</code></pre>"},{"location":"contributors/","title":"Contributors","text":"<ul> <li>Samhan Samhan (m.samhan@unsw.edu.au): Lead developer and researcher. Responsible for conceptualization, implementation, documentation, and experimentation.</li> <li>Assoc Prof Anna Bruce: Supervisor. Provided guidance on research direction and methodology.</li> <li>Dr Baran Yildiz: Supervisor. Provided guidance on research direction and methodology.</li> </ul>"},{"location":"datasets_models/","title":"Datasets & Models","text":""},{"location":"datasets_models/#dataset","title":"Dataset","text":"<p>The available datasets are listed in the <code>data</code> folder, with metadata provided in <code>data/metadata.xlsx</code>. Below are some of the datasets currently included in the library:</p>"},{"location":"datasets_models/#ausgrid-solar-home-datasets","title":"Ausgrid Solar Home Datasets","text":"<p>This dataset has been widely used in net load forecasting research since 2016. It includes data from 300 solar-equipped households within the Ausgrid network in Sydney. The original source is here.</p> Dataset Name Description <code>ds1_ahsd.csv</code> Ausgrid Solar Home Dataset, aggregate of 300 household data in Ausgrid network <code>ds4_ashd_with_weather.csv</code> <code>ds1</code> enhanced with temperature, relative humidity, and wind speed data <code>ds13_ashd_with_cloud_solcast.csv</code> <code>ds4</code> further enriched with cloud data from Solcast"},{"location":"datasets_models/#australia-energy-data-platform-aedp-datasets","title":"Australia Energy Data Platform (AEDP) Datasets","text":"<p>These datasets were compiled by UNSW Sydney using data from Solar Analytics and Wattwatchers. Sensitive information such as customer addresses, names, and NMIs has been removed. The original source is  here.</p> Dataset Name Description <code>ds10_aedp_cluster2_30min.csv</code> AEDP dataset for Cluster 2 with 30-minute resolution <code>ds11_aedp_cluster2_30min_with_weather.csv</code> <code>ds10</code> enhanced with weather data including temperature, humidity, etc."},{"location":"datasets_models/#ausgrid-zone-substation-zs-datasets-mascot","title":"Ausgrid Zone Substation (ZS) Datasets \u2013 Mascot","text":"<p>Unlike the previous household-focused datasets, this dataset covers a zone substation, which includes residential, commercial &amp; industrial (C&amp;I), and major customers. The original source is here.</p> Dataset Name Description <code>ds14_ausgrid_zs_mascot.csv</code> Zone Substation data for Mascot <code>ds15_ausgrid_zs_mascot_30min_with_weather.csv</code> <code>ds14</code> enhanced with weather data at 30-minute resolution"},{"location":"datasets_models/#model","title":"Model","text":""},{"location":"datasets_models/#forecasting-models-overview","title":"Forecasting Models Overview","text":"Model ID Model Name Short Description <code>m1_naive</code> Naive Forecast equals the last observed value <code>m2_snaive</code> Seasonal Naive Forecast equals the value from the same season in the previous cycle"},{"location":"datasets_models/#statistical-models","title":"Statistical Models","text":"Model ID Model Name Short Description <code>m3_ets</code> ETS Exponential smoothing model with error, trend, and seasonality components <code>m4_arima</code> ARIMA Autoregressive Integrated Moving Average model for time series forecasting <code>m5_sarima</code> SARIMA Seasonal ARIMA model with seasonal components <code>m6_lr</code> Linear Regression Predicts future values using a linear combination of input features"},{"location":"datasets_models/#machine-learning-models","title":"Machine Learning Models","text":"Model ID Model Name Short Description <code>m7_ann</code> ANN Basic Artificial Neural Network with one hidden layers <code>m8_dnn</code> Deep Neural Network ANN with more than one hidden layer <code>m9_rt</code> Regression Tree Decision tree model for regression tasks <code>m10_rf</code> Random Forest Ensemble of regression trees for improved accuracy and robustness <code>m11_svr</code> Support Vector Regression Uses support vectors to perform regression with margin of tolerance <code>m12_rnn</code> Recurrent Neural Network Neural network with feedback loops for sequential data <code>m13_lstm</code> Long Short-Term Memory RNN variant designed to capture long-term dependencies <code>m14_gru</code> Gated Recurrent Unit Simplified LSTM with fewer parameters <code>m15_transformer</code> Transformer Attention-based model for sequence modeling without recurrence <code>m16_prophet</code> Prophet Time series model developed by Facebook for business forecasting <code>m17_xgb</code> XGBoost Gradient boosting framework optimized for speed and performance <code>m18_nbeats</code> N-BEATS Deep learning model for univariate time series forecasting"},{"location":"datasets_models/#hyperparameter","title":"Hyperparameter","text":"<p>The list of available model and its hyperparameter can be seen on <code>config/model_hyperparameters.ipynb</code>. The values currently available are the hyperparameter values mostly used in academic literature, but not necessarily the optimum value. </p>"},{"location":"examples/","title":"User Input","text":"<p>Suppose we perform a simple test using the inputs below, which should take less than 1 minute in the <code>run_experiment.ipynb</code> file: </p> <pre><code># 1. RUN CONFIG\n%run \"../config/config.ipynb\"\n\n# 2. SETUP FORECAST PROBLEM AND MODEL SPECIFICATION (USER TO INPUT)\n# FORECAST PROBLEM\ndataset = ds0\nforecast_horizon = fh1 # fh1 = 30 minutes ahead, fh9 = 2 days ahead\n# MODEL SPECIFICATION\nmodel_name = m6\nhyperparameter_no = 'hp1'\n\n# 3. RUN EXPERIMENT\nrun_experiment(dataset, forecast_horizon, model_name, hyperparameter_no)\n</code></pre>"},{"location":"examples/#output","title":"Output","text":"<p>The tool generates the following outputs.</p> Name Type Description <code>E00001_cv_test/</code> Folder Time series of observation, forecast, and residual for each cross-validation split <code>E00001_cv_train/</code> Folder Time series of observation, forecast, and residual for each cross-validation split <code>E00001_cv1_plots/</code> Folder Plots for the first cross-validation fold: time plot, scatter plot, residual plot, histogram <code>E00001_models/</code> Folder Saved models used or generated during the experiment <code>E00001_a1_experiment_result.csv</code> File Accuracy (cross-validated test n-RMSE), stability, and training time <code>E00001_a2_hyperparameter.csv</code> File Hyperparameters used for each model <code>E00001_a3_cross_validation_result.csv</code> File Detailed results for each cross-validation split <p>The file <code>a1_experiment_result.csv</code> summarises the results, including the cross validated nRMSE &amp; its standard deviation</p> experiment_no exp_date dataset_no dataset dataset_freq_min dataset_length_week forecast_horizon_min train_pct test_pct model_no hyperparameter_no model_name hyperparameter runtime_ms train_RMSE train_RMSE_stddev test_RMSE test_RMSE_stddev train_nRMSE train_nRMSE_stddev test_nRMSE test_nRMSE_stddev E00001 15/09/2025 ds0 test 30 10 30 0.9 0.1 m6 hp1 m6_lr_hp1 num_features: 50 201.769185 17.33 0.206421 17.7066 1.82726 2.98206 0.03552 3.04686 0.31443 <p>The file <code>a3_cross_validation_result.csv</code> provides the detailed cross-validation (CV) results, from CV1 to CV10.</p>"},{"location":"examples/#experiment-metrics","title":"Experiment Metrics","text":"# runtime_ms train_MBE train_MAE train_RMSE train_MAPE train_MASE train_fskill train_R2 test_MBE test_MAE test_RMSE test_MAPE test_MASE test_fskill test_R2 train_nRMSE test_nRMSE 1 193.2564 0 12.208 17.122 27.366 0.422 55.471 0.988 -0.1804 13.278 19.679 22.808 0.459 43.531 0.981 2.9463 3.3863 2 224.8719 0 12.337 17.495 27.347 0.423 54.817 0.988 -0.1443 12.332 16.346 18.905 0.423 49.006 0.987 3.0105 2.8127 3 251.0867 0 12.361 17.477 22.314 0.433 54.031 0.987 0.3318 11.965 16.321 60.409 0.419 58.022 0.989 3.0074 2.8084 4 174.1974 0 12.401 17.573 28.635 0.434 53.775 0.987 -0.8563 11.654 15.427 13.623 0.408 60.346 0.992 3.0239 2.6546 5 232.7249 0 12.281 17.020 22.091 0.435 54.565 0.988 1.2783 12.536 20.375 70.615 0.444 53.147 0.986 2.9287 3.5060 6 173.2767 0 12.322 17.467 28.530 0.436 53.491 0.987 -0.6225 12.436 16.567 12.690 0.440 61.224 0.991 3.0056 2.8508 7 233.0346 0 12.171 17.004 26.500 0.427 55.250 0.988 0.8754 13.685 20.516 27.756 0.480 47.485 0.981 2.9260 3.5303 8 212.3849 0 12.286 17.390 26.759 0.428 54.516 0.988 -0.7814 12.809 17.191 23.091 0.447 53.467 0.985 2.9924 2.9581 9 157.8457 0 12.330 17.417 28.242 0.435 53.965 0.988 -1.0170 12.308 16.923 10.919 0.434 58.189 0.989 2.9970 2.9120 10 165.0126 0 12.269 17.335 28.311 0.421 55.265 0.988 1.0148 12.840 17.721 11.193 0.441 44.110 0.979 2.9829 3.0493 mean 201.7692 0 12.2966 17.330 26.6095 0.4294 54.5146 0.9877 -0.0102 12.5843 17.7066 27.2009 0.4395 52.8527 0.986 2.9821 3.0469 stddev 33.2319 0 0.0693 0.2064 2.4352 0.00591 0.6866 0.00048 0.8405 0.5983 1.8273 21.106 0.02059 6.5729 0.00447 0.03552 0.31443 <p>Below are some plots on the test set:</p> Figure 1: Observation vs Forecast (Time Plot) Figure 2: Observation vs Forecast (Scatter Plot) Figure 3: Residuals Over Time Figure 4: Residual Histogram"},{"location":"features_limitations/","title":"Features & Limitations","text":"Type Title Description Feature Adjustable User can specify the dataset, forecast horizon, model, and hyperparameters Feature Systematic Each experiment outputs metadata such as date, experiment ID, forecast problem, and model specs Feature Flexible Users can add or modify models, datasets, hyperparameters, and general functions Limitation Basic User Interface Users must manually modify code to change models Limitation Limited Dataset and Model Library Many public net load datasets and models are not yet included. Contributions are welcome!"},{"location":"getting_started/","title":"Installation Instructions","text":"<ol> <li>Clone the repository to your local machine.  </li> <li>Create a Python virtual environment.  </li> <li>Install the required packages:</li> </ol> <pre><code>pip install -r requirements.txt\n</code></pre> <p>\u26a0\ufe0f This may take ~10 minutes. Although newer Python versions may work, the tool was tested on Python 3.12.3.</p>"},{"location":"getting_started/#how-to-use-the-tool","title":"How to Use The Tool","text":"<ol> <li>Open the notebook: <code>notebooks/model/run_experiments.ipynb.</code></li> <li>Fill in the input values for your forecast problem and model specification. </li> <li>Run the notebook.</li> <li>The tool outputs evaluation results to the <code>experiment_result/folder</code>.</li> <li>To evaluate multiple forecast problems and model specifications at once, use <code>notebooks/model/run_experiments_batch.ipynb</code></li> </ol> <p>Example input values:</p> <pre><code>dataset = \"ds0\"            # Dataset for testing\nforecast_horizon = \"fh1\"   # fh1 = 30 minutes ahead\nmodel_name = \"m6\"          # Linear regression\nhyperparameter_no = \"hp1\"  # Hyperparameter ID\n</code></pre> <p>For the list of available datasets &amp; models, how to modify model hyperparameter, how to add a model, how to add a dataset, and exhaustive list of API Reference see the Detailed Guide page.</p>"},{"location":"modify_hyper/","title":"Model Hyperparameter","text":"<p>All hyperparameter values are stored separately from the model code in a designated notebook: <code>notebooks/config/model_hyperparameters.ipynb</code>. This design avoids hard-coded values and provides a centralized location for managing all hyperparameters.</p>"},{"location":"modify_hyper/#format","title":"Format","text":"<p>Hyperparameter values are stored as a list of dictionaries. For example, for <code>m7_ann</code>, the values are stored in the <code>m7_hp_table</code> list. Each dictionary contains an <code>hp_no</code> (hyperparameter set ID) and key-value pairs for the parameters.</p> <pre><code>m7_hp_table = [\n    {\n        \"hp_no\": \"hp1\",\n        \"seed\": 99, # we will use the same seed for reproducibility\n        \"hidden_size\": 10, #number of neurons in hidden layers\n        \"activation_function\": \"relu\",\n        \"learning_rate\" : 0.001,\n        \"solver\" : \"adam\",\n        \"epochs\" : 500\n    },\n    {\n        \"hp_no\": \"hp2\",\n        \"seed\": 99, # we will use the same seed for reproducibility\n        \"hidden_size\": 10, #number of neurons in hidden layers\n        \"activation_function\": \"relu\",\n        \"learning_rate\" : 0.01,\n        \"solver\" : \"adam\",\n        \"epochs\" : 500\n    },\n]\n</code></pre>"},{"location":"modify_hyper/#how-to-modify-model-hyperparameter","title":"How to Modify Model Hyperparameter","text":"<p>If for example we want to modify the learning rate of the ANN model we can create a new hyperparameter set, and below is the update result:</p> <pre><code>m7_hp_table = [\n    {\n        \"hp_no\": \"hp1\",\n        \"seed\": 99, # we will use the same seed for reproducibility\n        \"hidden_size\": 10, #number of neurons in hidden layers\n        \"activation_function\": \"relu\",\n        \"learning_rate\" : 0.001,\n        \"solver\" : \"adam\",\n        \"epochs\" : 500\n    },\n    {\n        \"hp_no\": \"hp2\",\n        \"seed\": 99, # we will use the same seed for reproducibility\n        \"hidden_size\": 10, #number of neurons in hidden layers\n        \"activation_function\": \"relu\",\n        \"learning_rate\" : 0.01,\n        \"solver\" : \"adam\",\n        \"epochs\" : 500\n    },\n    {\n        \"hp_no\": \"hp3\",\n        \"seed\": 99, # we will use the same seed for reproducibility\n        \"hidden_size\": 10, #number of neurons in hidden layers\n        \"activation_function\": \"relu\",\n        \"learning_rate\" : 0.1, **Modified learning rate**\n        \"solver\" : \"adam\",\n        \"epochs\" : 500\n    },\n]\n</code></pre> <p>When running experiments, you can select the desired hyperparameter set by referencing its <code>hp_no</code>, such as <code>hp3</code>.</p>"},{"location":"repo_structure/","title":"Repo structure","text":""},{"location":"repo_structure/#folder-data","title":"Folder: <code>data/</code>","text":"<p>Contains all datasets available for experiments. Metadata is stored in <code>data/metadata.xlsx</code>.</p>"},{"location":"repo_structure/#folder-docs","title":"Folder: <code>docs/</code>","text":"<p>Includes all files required to build the documentation page on GitHub Desktop, including images.</p>"},{"location":"repo_structure/#folder-experiment_result","title":"Folder: <code>experiment_result/</code>","text":"<p>Stores all experiment outputs generated by the PyNNLF tool. Testing results are saved in <code>experiment_result/Archive/Testing Result</code>.</p> <ul> <li><code>result_summary.xlsx</code>: Summary of all experiment results, generated by <code>notebooks/result_aggregation.ipynb</code>.</li> <li><code>testing_benchmark.xlsx</code>: Benchmark results from three different computers.</li> </ul>"},{"location":"repo_structure/#folder-notebooks","title":"Folder: <code>notebooks/</code>","text":"<p>Contains all source code for the tool.</p>"},{"location":"repo_structure/#subfolder-notebooksconfig","title":"Subfolder: <code>notebooks/config/</code>","text":"<p>Stores configuration files with hard-coded values.</p> <ul> <li><code>config.ipynb</code>: Contains tool parameters that rarely change (e.g., cross-validation folds, lookback period, train-test split ratio).</li> <li><code>constant.ipynb</code>: Stores constants like month names.</li> <li><code>general_functions.ipynb</code>: Core backend functions for PyNNLF, including lag feature creation, train-test splitting, cross-validation, evaluation, etc.</li> <li><code>hyperparameters.ipynb</code>: Stores all model hyperparameters.</li> </ul>"},{"location":"repo_structure/#subfolder-notebooksmodel","title":"Subfolder: <code>notebooks/model/</code>","text":"<p>Contains all model files and experiment notebooks.</p>"},{"location":"repo_structure/#model-files","title":"Model Files","text":"<p>Each model file follows the naming format <code>[model_id]_[model_name].ipynb</code>.</p>"},{"location":"repo_structure/#run-experiments","title":"Run Experiments","text":"<ul> <li><code>run_experiment.ipynb</code>: Runs a single experiment.</li> <li><code>run_experiment_batch.ipynb</code>: Runs batch experiments across multiple forecast problems or model configurations.</li> </ul>"},{"location":"repo_structure/#result-aggregation","title":"Result Aggregation","text":"<ul> <li><code>result_aggregation.ipynb</code>: Aggregates results from all experiments in <code>experiment_result/</code> into <code>result_summary.xlsx</code>.</li> </ul>"},{"location":"repo_structure/#run-tests","title":"Run Tests","text":"<ul> <li><code>run_experiment_batch.ipynb</code>: Also used for testing the tool.   Important to run full tests after major updates, especially those affecting the backend.</li> </ul>"},{"location":"run_experiment/","title":"How to Use The Tool","text":"<ul> <li>Open the notebook: <code>notebooks/model/run_experiments.ipynb.</code></li> <li>Fill in the input values for your forecast problem and model specification. Example:</li> </ul> <pre><code>dataset = \"ds0\"            # Dataset for testing\nforecast_horizon = \"fh1\"   # fh1 = 30 minutes ahead\nmodel_name = \"m6\"          # Linear regression\nhyperparameter_no = \"hp1\"  # Hyperparameter ID\n</code></pre> <ul> <li> <p>Run the notebook.</p> </li> <li> <p>The tool outputs evaluation results to the <code>experiment_result/folder</code>.</p> </li> <li> <p>To evaluate multiple forecast problems and model specifications at once, use <code>notebooks/model/run_experiments_batch.ipynb</code></p> </li> </ul> <p>For the list of available datasets &amp; models, how to modify model hyperparameter, how to add a model, how to add a dataset, and exhaustive list of API Reference see the Detailed Guide page.</p>"},{"location":"support_contributing/","title":"Support & Contributing","text":"<p>To report bugs, request features, or suggest improvements, please use the GitHub Issues feature. For contributing or seeking support, contact m.samhan@unsw.edu.au.</p>"},{"location":"tool_testing/","title":"Simple Testing","text":"<p>To quickly test the tool, follow the steps in the Getting Started section before. You can try it out with the following example inputs:</p> <pre><code>dataset = ds0              # Sample dataset for testing\nforecast_horizon = fh1     # fh1 = 30 minutes ahead\nmodel_name = m6            # m6 = Linear Regression\nhyperparameter_no = 'hp1'  # Example hyperparameter set\n</code></pre> <p>This is a great way to get familiar with how the tool works before running full-scale tests.</p>"},{"location":"tool_testing/#full-model-testing-all-18-models","title":"Full Model Testing (All 18 Models)","text":"<p>For a complete evaluation and checking the result against benchmark values, which is available on <code>experiment_result/Archive/Testing Result/testing_benchmark</code>.  To test it, follow these steps:</p> <ol> <li>Open the file <code>run_tests.ipynb</code>.</li> <li>Run all cells without making any changes.</li> <li>The notebook will automatically: Run all 18 models, compare their outputs against three benchmark results. The results are available in: <code>experiment_result/Archive/Testing Result/</code></li> </ol> <p>\u23f1 Estimated time: ~1 hour on a personal computer with an Intel i5 processor and 32GB RAM. This tool has undergone full model testing on three different computers, and the result can be seen on </p> <pre><code>experiment_result/Archive/Testing Result/20250821_test_result_CEEM Computer.csv\nexperiment_result/Archive/Testing Result/20250821_test_result_UNSW Laptop.csv\nexperiment_result/Archive/Testing Result/20250822_test_result_SS Personal Laptop\n</code></pre>"}]}