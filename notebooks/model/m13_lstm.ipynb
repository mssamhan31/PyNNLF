{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT IMPORTANT LIBRARY\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_m13_lstm(hyperparameter, train_df_X, train_df_y):\n",
    "    \"\"\"Train an LSTM model for point forecasting using lag and exogenous features.\n",
    "\n",
    "    The LSTM captures temporal patterns from lag features. The last hidden state \n",
    "    is concatenated with exogenous features and passed through a fully connected \n",
    "    layer to produce the forecast.\n",
    "\n",
    "    Args:\n",
    "        hyperparameter (dict): Model hyperparameters (seed, input_size, hidden_size, \n",
    "                               num_layers, output_size, batch_size, epochs, learning_rate).\n",
    "        train_df_X (pd.DataFrame): Training predictors including lag and exogenous features.\n",
    "        train_df_y (pd.DataFrame): Training target values.\n",
    "\n",
    "    Returns:\n",
    "        model (dict): Dictionary containing the trained LSTM model, hyperparameters, \n",
    "                      and original training data.\n",
    "    \"\"\"\n",
    "    \n",
    "    #UNPACK HYPERPARAMETER\n",
    "    seed = int(hyperparameter['seed'])\n",
    "    input_size = int(hyperparameter['input_size']) #this is one since we only use lag features to be fed into the LSTM. The exogenous features like calenndar are fed to the fully connected layer, together with the last hidden state of LSTM.\n",
    "    hidden_size = int(hyperparameter['hidden_size']) #this is the size of hidden state, and we aim to use many to one architecture. Meaning we only take the last hidden state as output, and fed into the fully connected layer.\n",
    "    num_layers = int(hyperparameter['num_layers']) # we use 1 by default to make it simple. \n",
    "    output_size = int(hyperparameter['output_size']) #this is one since we only predict one value.\n",
    "    batch_size = int(hyperparameter['batch_size']) #using minibatch is important cuz if we train all samples at once, the memory is not enough.\n",
    "    epochs = int(hyperparameter['epochs'])\n",
    "    learning_rate = hyperparameter['learning_rate']  # No change for learning rate\n",
    "\n",
    "    \n",
    "    #DEFINE MODEL AND TRAINING FUNCTION\n",
    "    class LSTMModel(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, num_layers, exog_size, output_size=1):\n",
    "            super(LSTMModel, self).__init__()\n",
    "            \n",
    "            # Define the LSTM layer\n",
    "            self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "            \n",
    "            # Define the Fully Connected (FC) layer\n",
    "            # The FC layer input size is the concatenation of LSTM output and exogenous variables\n",
    "            self.fc = nn.Linear(hidden_size + exog_size, output_size)  # exog_size is the number of exogenous features\n",
    "\n",
    "        def forward(self, x, exogenous_data):\n",
    "            # Pass the input through the LSTM\n",
    "            out, (h_n, c_n) = self.lstm(x)\n",
    "            \n",
    "            # Get the last timestep hidden state (h3)\n",
    "            last_hidden_state = out[:, -1, :]  # Shape: (batch_size, hidden_size)\n",
    "            \n",
    "            # Concatenate the LSTM output (h3) with the exogenous variables (for timestep t+100)\n",
    "            combined_input = torch.cat((last_hidden_state, exogenous_data), dim=1)  # Shape: (batch_size, hidden_size + exog_size)\n",
    "            \n",
    "            # Pass the combined input through the FC layer\n",
    "            out = self.fc(combined_input)\n",
    "            return out\n",
    "        \n",
    "    def train_lstm_with_minibatches(model, train_loader, epochs, learning_rate=0.001):\n",
    "        # Define the loss function (Mean Squared Error)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        # Define the optimizer (Adam)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            print(f'Epoch [{epoch+1}/{epochs}]')\n",
    "            start_time = time.time()\n",
    "\n",
    "            model.train()  # Set model to training mode\n",
    "            # print(f'I am here')\n",
    "            \n",
    "            # Iterate over mini-batches\n",
    "            batch_no = 1\n",
    "            for X_lags_batch, X_exog_batch, y_batch in train_loader:\n",
    "                # print(f'I am here now')\n",
    "                # Print the loss and time taken for this epoch\n",
    "                print(f'Epoch [{epoch+1}/{epochs}] and batch [{batch_no}/{len(train_loader)}]')\n",
    "                batch_no += 1\n",
    "                # Forward pass\n",
    "                predictions = model(X_lags_batch, X_exog_batch)\n",
    "                loss = criterion(predictions, y_batch)\n",
    "\n",
    "                # Backward pass\n",
    "                optimizer.zero_grad()  # Zero gradients from previous step\n",
    "                loss.backward()  # Backpropagate the error\n",
    "                optimizer.step()  # Update the model's weights\n",
    "                \n",
    "            \n",
    "            \n",
    "            end_time = time.time()\n",
    "            epoch_time = end_time - start_time\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}, time taken: {epoch_time:.2f} seconds')\n",
    "            \n",
    "    def set_seed(seed=seed):\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "            \n",
    "    # PREPARE TRAIN DATA\n",
    "    # SEPARATE LAG AND EXOGENOUS FEATURES\n",
    "    X_lags, X_exog = separate_lag_and_exogenous_features(train_df_X)\n",
    "    X_lags_tensor = torch.tensor(X_lags.values, dtype=torch.float32)  # Shape: (batch_size, sequence_length, input_size)\n",
    "    X_exog_tensor = torch.tensor(X_exog.values, dtype=torch.float32)  # Shape: (batch_size, exog_size)\n",
    "    y_tensor = torch.tensor(train_df_y.values, dtype=torch.float32).view(-1, 1)\n",
    "    \n",
    "    total_lag_features = X_lags_tensor.shape[1]  # Number of lag features (columns)\n",
    "    sequence_length = total_lag_features // input_size\n",
    "    exog_size = X_exog_tensor.shape[1]  # Number of exogenous features\n",
    "    \n",
    "    # Reshaping X_lags_tensor to 3D: (batch_size, sequence_length, input_size)\n",
    "    X_lags_tensor = X_lags_tensor.view(-1, sequence_length, input_size)\n",
    "\n",
    "    \n",
    "    # INITIALIZE MODEL and MAKE TRAINING BATCHES\n",
    "    set_seed(seed = seed) # Set random seed for reproducibility\n",
    "    lstm = LSTMModel(input_size, hidden_size, num_layers, exog_size, output_size)\n",
    "    # Create a TensorDataset with your features and target\n",
    "    train_data = TensorDataset(X_lags_tensor, X_exog_tensor, y_tensor)\n",
    "    # Create a DataLoader to handle mini-batching\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # TRAIN MODEL\n",
    "    train_lstm_with_minibatches(lstm, train_loader, epochs=epochs, learning_rate=learning_rate)\n",
    "\n",
    "  \n",
    "    # PACK MODEL\n",
    "    model = {\"lstm\": lstm, 'hyperparameter': hyperparameter, \"train_df_X\": train_df_X, \"train_df_y\": train_df_y}\n",
    "  \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_forecast_m13_lstm(model, train_df_X, test_df_X):\n",
    "    \"\"\"Generate forecasts for train and test sets using a trained LSTM model.\n",
    "\n",
    "    The function handles lag and exogenous features, applies mini-batching \n",
    "    to avoid memory issues, and returns predictions for both train and test sets.\n",
    "\n",
    "    Args:\n",
    "        model (dict): Dictionary containing the trained LSTM model and hyperparameters.\n",
    "        train_df_X (pd.DataFrame): Training predictors including lag and exogenous features.\n",
    "        test_df_X (pd.DataFrame): Test predictors including lag and exogenous features.\n",
    "\n",
    "    Returns:\n",
    "        train_df_y_hat (np.ndarray): Forecasted values for the training set.\n",
    "        test_df_y_hat (np.ndarray): Forecasted values for the test set.\n",
    "    \"\"\"\n",
    "    \n",
    "    # UNPACK MODEL\n",
    "    lstm = model['lstm']\n",
    "    hyperparameter = model['hyperparameter']\n",
    "    \n",
    "    #UNPACK HYPERPARAMETER\n",
    "    seed = int(hyperparameter['seed'])\n",
    "    input_size = int(hyperparameter['input_size']) #this is one since we only use lag features to be fed into the LSTM. The exogenous features like calenndar are fed to the fully connected layer, together with the last hidden state of LSTM.\n",
    "    hidden_size = int(hyperparameter['hidden_size']) #this is the size of hidden state, and we aim to use many to one architecture. Meaning we only take the last hidden state as output, and fed into the fully connected layer.\n",
    "    num_layers = int(hyperparameter['num_layers']) # we use 1 by default to make it simple. \n",
    "    output_size = int(hyperparameter['output_size']) #this is one since we only predict one value.\n",
    "    batch_size = int(hyperparameter['batch_size']) #using minibatch is important cuz if we train all samples at once, the memory is not enough.\n",
    "    epochs = int(hyperparameter['epochs'])\n",
    "    learning_rate = hyperparameter['learning_rate']  # No change for learning rate\n",
    "    \n",
    "    # PRODUCE FORECAST\n",
    "    def produce_forecast(lstm, X):\n",
    "        # Convert X into X_lag and X_exog\n",
    "        X_lags, X_exog = separate_lag_and_exogenous_features(X)\n",
    "        X_lags_tensor = torch.tensor(X_lags.values, dtype=torch.float32)  # Shape: (batch_size, sequence_length, input_size)\n",
    "        X_exog_tensor = torch.tensor(X_exog.values, dtype=torch.float32)  # Shape: (batch_size, exog_size)\n",
    "        # y_tensor = torch.tensor(train_df_y.values, dtype=torch.float32).view(-1, 1) to be deleted.\n",
    "        \n",
    "        total_lag_features = X_lags_tensor.shape[1]  # Number of lag features (columns)\n",
    "        sequence_length = total_lag_features // input_size\n",
    "        exog_size = X_exog_tensor.shape[1]  # Number of exogenous features\n",
    "        \n",
    "        # Reshaping X_lags_tensor to 3D: (batch_size, sequence_length, input_size)\n",
    "        X_lags_tensor = X_lags_tensor.view(-1, sequence_length, input_size)\n",
    "        \n",
    "        #predictions = lstm(X_lags_tensor, X_exog_tensor) #this doesn't work because of the batch size is too big, not enough memory.\n",
    "        predictions = []\n",
    "        for i in range(0, len(X_lags_tensor), batch_size):\n",
    "            # Get the current minibatch for both X_lags_tensor and X_exog_tensor\n",
    "            batch_X_lags = X_lags_tensor[i:i+batch_size]\n",
    "            batch_X_exog = X_exog_tensor[i:i+batch_size]\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Make predictions for the minibatch\n",
    "                batch_pred = lstm(batch_X_lags, batch_X_exog)\n",
    "            \n",
    "            # Store the predictions for the current batch\n",
    "            predictions.append(batch_pred)\n",
    "\n",
    "        # Concatenate all predictions to get the full result\n",
    "        predictions = torch.cat(predictions, dim=0)\n",
    "\n",
    "        \n",
    "        return predictions.detach().numpy()\n",
    "    \n",
    "    train_df_y_hat = produce_forecast(lstm, train_df_X)\n",
    "    test_df_y_hat = produce_forecast(lstm, test_df_X)\n",
    "    \n",
    "    return train_df_y_hat, test_df_y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_lag_and_exogenous_features(train_df_X, target_column='y', lag_prefix='y_lag'):\n",
    "    \"\"\"\n",
    "    Split a dataframe into lag features and exogenous variables.\n",
    "\n",
    "    Args:\n",
    "        train_df_X (pd.DataFrame): DataFrame containing lagged features, exogenous variables, and possibly the target.\n",
    "        target_column (str, optional): Name of the target column to exclude from exogenous features. Defaults to 'y'.\n",
    "        lag_prefix (str, optional): Prefix that identifies lagged features. Defaults to 'y_lag'.\n",
    "\n",
    "    Returns:\n",
    "        X_lags (pd.DataFrame): DataFrame containing only columns that are lag features.\n",
    "        X_exog (pd.DataFrame): DataFrame containing only exogenous variables (excluding target and lag features).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Identify lag features (columns that start with 'y_lag')\n",
    "    lag_features = [col for col in train_df_X.columns if col.startswith(lag_prefix)]\n",
    "    \n",
    "    # Identify exogenous variables (everything except the target and lag features)\n",
    "    exog_features = [col for col in train_df_X.columns if col not in [target_column] + lag_features]\n",
    "    \n",
    "    # Create dataframes for lag features and exogenous features\n",
    "    X_lags = train_df_X[lag_features]\n",
    "    X_exog = train_df_X[exog_features]\n",
    "    \n",
    "    return X_lags, X_exog"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
