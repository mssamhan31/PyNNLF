{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT ALL NECESSARY LIBRARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "import os\n",
    "import time\n",
    "import pickle # for saving trained model\n",
    "import dill # for saving trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOLDER PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_exp_no(path_result):\n",
    "    \"\"\"Compute experiment number for folder & file naming based on the number of existing experiments that have been done.\n",
    "    For example, if on the folder there are already 5 experiment folders, then the new experiment no. is E00006.\n",
    "\n",
    "    Args:\n",
    "        path_result (str): relative path of experiment folder, stored in config\n",
    "\n",
    "    Returns:\n",
    "        int: exp no \n",
    "        str: exp no in str\n",
    "    \"\"\"\n",
    "    subfolders = os.listdir(path_result)\n",
    "    number_of_folders = len(subfolders)\n",
    "    experiment_no = number_of_folders - 1 # there is one archive folder\n",
    "    experiment_no_str = f\"E{str(experiment_no).zfill(5)}\"\n",
    "    \n",
    "    return experiment_no, experiment_no_str\n",
    "\n",
    "def compute_folder_name(experiment_no_str, forecast_horizon, model_name, hyperparameter_no):\n",
    "    \"\"\"Folder name in the format of [exp number]_[exp date]_[dataset]_[forecast horizon]_[model]_[hyperparameter]\n",
    "\n",
    "    Args:\n",
    "        experiment_no_str (str): exp number\n",
    "        forecast_horizon (int): forecast horizon in minutes\n",
    "        model_name (str): for example, m1_naive\n",
    "        hyperparameter_no (str): for example, hp1\n",
    "\n",
    "    Returns:\n",
    "        str: folder name\n",
    "    \"\"\"\n",
    "    folder_name = \\\n",
    "        experiment_no_str + '_' +\\\n",
    "        datetime.today().date().strftime(\"%y%m%d\") + '_' +\\\n",
    "        dataset.split('_')[0] + '_' +\\\n",
    "        'fh' + str(forecast_horizon) + '_' +\\\n",
    "        model_name + '_' +\\\n",
    "        hyperparameter_no\n",
    "    return folder_name\n",
    "\n",
    "def prepare_directory(path_result, forecast_horizon, model_name, hyperparameter_no):\n",
    "    \"\"\"Do two things,\n",
    "    1. Create folders inside the experiment result folder\n",
    "    2. Create some file names to be used when exporting result later\n",
    "    \n",
    "\n",
    "    Args:\n",
    "        path_result (str): relative path to the experiment result folder\n",
    "        forecast_horizon (int): forecast horizon in minutes\n",
    "        model_name (str): eg m1_naive\n",
    "        hyperparameter_no (str): eg\n",
    "\n",
    "    Returns:\n",
    "        hyperparameter (df): pd df series of hyperparameter chosen\n",
    "        experiment_no_str (str) : experiment number\n",
    "        filepath (dict) : dict of all filepaths that will be exported\n",
    "    \"\"\"\n",
    "    \n",
    "    hyperparameter_table = globals()[f\"{model_name.split('_')[0]}_hp_table\"]\n",
    "    hyperparameter = hyperparameter_table.loc[hyperparameter_no]\n",
    "    \n",
    "    experiment_no, experiment_no_str = compute_exp_no(path_result)\n",
    "    folder_name = compute_folder_name(experiment_no_str, forecast_horizon, model_name, hyperparameter_no)\n",
    "\n",
    "    # CREATE FOLDER\n",
    "    cv_folder_train = experiment_no_str + '_cv_train'\n",
    "    cv_folder_test = experiment_no_str + '_cv_test'\n",
    "    cv1_plot_folder = experiment_no_str + '_cv1_plots'\n",
    "    folder_model = experiment_no_str + '_models'\n",
    "    \n",
    "    path_result2 = path_result + folder_name +'/'\n",
    "    path_result_train = path_result2 + cv_folder_train +'/'\n",
    "    path_result_test = path_result2 + cv_folder_test +'/'\n",
    "    path_result_plot = path_result2 + cv1_plot_folder +'/'\n",
    "    path_model = path_result2 + folder_model +'/'\n",
    "\n",
    "    # MAKE FOLDERS\n",
    "    os.mkdir(path_result2)\n",
    "    os.mkdir(path_result_train)\n",
    "    os.mkdir(path_result_test)\n",
    "    os.mkdir(path_result_plot)\n",
    "    os.mkdir(path_model)\n",
    "\n",
    "    # MAKE FILE PATH\n",
    "    filepath = {\n",
    "        'a1' : path_result2 + experiment_no_str + '_a1_experiment_result.csv',\n",
    "        'a2' : path_result2 + experiment_no_str + '_a2_hyperparameter.csv',\n",
    "        'a3' : path_result2 + experiment_no_str + '_a3_cross_validation_result.csv',\n",
    "        'b1' : path_result_plot + experiment_no_str + '_b1_train_timeplot.png', # Time Plot of Forecast vs Observation\n",
    "        'b2' : path_result_plot + experiment_no_str + '_b2_train_scatterplot.png', # Scatter Plot of Forecast vs Observation\n",
    "        'b3' : path_result_plot + experiment_no_str + '_b3_train_residual_timeplot.png', # Time Plot of Residual\n",
    "        'b4' : path_result_plot + experiment_no_str + '_b4_train_residual_histogram.png', # Histogram of Residual\n",
    "        'b5' : path_result_plot + experiment_no_str + '_b5_train_learningcurve.png', # Learning Curve vs Epoch\n",
    "        'c1' : path_result_plot + experiment_no_str + '_c1_test_timeplot.png',  # Time Plot of Forecast vs Observation\n",
    "        'c2' : path_result_plot + experiment_no_str + '_c2_test_scatterplot.png',  # Scatter Plot of Forecast vs Observation\n",
    "        'c3' : path_result_plot + experiment_no_str + '_c3_test_residual_timeplot.png',  # Time Plot of Residual\n",
    "        'c4' : path_result_plot + experiment_no_str + '_c4_test_residual_histogram.png',  # Histogram of Residual\n",
    "        'c5' : path_result_plot + experiment_no_str + '_c5_test_learningcurve.png',  # Learning Curve vs Epoch\n",
    "        \n",
    "        # B. FOLDER FOR CROSS VALIDATION TIME SERIES\n",
    "        'train_cv' : {\n",
    "            1 : path_result_train + experiment_no_str + '_cv1_train_result.csv',\n",
    "            2 : path_result_train + experiment_no_str + '_cv2_train_result.csv',\n",
    "            3 : path_result_train + experiment_no_str + '_cv3_train_result.csv',\n",
    "            4 : path_result_train + experiment_no_str + '_cv4_train_result.csv',\n",
    "            5 : path_result_train + experiment_no_str + '_cv5_train_result.csv',\n",
    "            6 : path_result_train + experiment_no_str + '_cv6_train_result.csv',\n",
    "            7 : path_result_train + experiment_no_str + '_cv7_train_result.csv',\n",
    "            8 : path_result_train + experiment_no_str + '_cv8_train_result.csv',\n",
    "            9 : path_result_train + experiment_no_str + '_cv9_train_result.csv',\n",
    "            10 : path_result_train + experiment_no_str + '_cv10_train_result.csv'\n",
    "        },\n",
    "\n",
    "        'test_cv' : {\n",
    "            1 : path_result_test + experiment_no_str + '_cv1_test_result.csv',\n",
    "            2 : path_result_test + experiment_no_str + '_cv2_test_result.csv',\n",
    "            3 : path_result_test + experiment_no_str + '_cv3_test_result.csv',\n",
    "            4 : path_result_test + experiment_no_str + '_cv4_test_result.csv',\n",
    "            5 : path_result_test + experiment_no_str + '_cv5_test_result.csv',\n",
    "            6 : path_result_test + experiment_no_str + '_cv6_test_result.csv',\n",
    "            7 : path_result_test + experiment_no_str + '_cv7_test_result.csv',\n",
    "            8 : path_result_test + experiment_no_str + '_cv8_test_result.csv',\n",
    "            9 : path_result_test + experiment_no_str + '_cv9_test_result.csv',\n",
    "            10 : path_result_test + experiment_no_str + '_cv10_test_result.csv'\n",
    "        },\n",
    "        \n",
    "        'model' : {\n",
    "            1 : path_model + experiment_no_str + '_cv1_model.pkl',\n",
    "            2 : path_model + experiment_no_str + '_cv2_model.pkl',\n",
    "            3 : path_model + experiment_no_str + '_cv3_model.pkl',\n",
    "            4 : path_model + experiment_no_str + '_cv4_model.pkl',\n",
    "            5 : path_model + experiment_no_str + '_cv5_model.pkl',\n",
    "            6 : path_model + experiment_no_str + '_cv6_model.pkl',\n",
    "            7 : path_model + experiment_no_str + '_cv7_model.pkl',\n",
    "            8 : path_model + experiment_no_str + '_cv8_model.pkl',\n",
    "            9 : path_model + experiment_no_str + '_cv9_model.pkl',\n",
    "            10 : path_model + experiment_no_str + '_cv10_model.pkl'\n",
    "        }\n",
    "    }\n",
    "    return hyperparameter,experiment_no_str, filepath\n",
    "\n",
    "def export_result(filepath, df_a1_result, cross_val_result_df):\n",
    "    \"\"\"Export experiment summary:\n",
    "    1. experiment result\n",
    "    2. hyperparameter\n",
    "    3. cross validation detailed result\n",
    "\n",
    "    Args:\n",
    "        filepath (dict): dictionary of filepaths for exporting result\n",
    "    \"\"\"\n",
    "    # Create a df of hyperparameter being used\n",
    "    df_a2 = pd.DataFrame(hyperparameter)\n",
    "    \n",
    "    # EXPORT IT\n",
    "    df_a1_result.to_csv(filepath['a1'], index=False)\n",
    "    df_a2.to_csv(filepath['a2'])\n",
    "    cross_val_result_df.to_csv(filepath['a3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA INPUT, CALENDAR FEATURE MAKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD NET LOAD HISTORICAL DATA\n",
    "def add_lag_features(df, forecast_horizon, max_lag_day):\n",
    "    \"\"\"\n",
    "    Adds a lagged column to the dataframe based on the given horizon in minutes and max lag in days.\n",
    "    \n",
    "    Args:\n",
    "    df (pd.DataFrame): The input dataframe with a datetime index and a column 'y'.\n",
    "    forecast_horizon (int): The horizon in minutes for the lag.\n",
    "    max_lag_day (int): the number of days until the longest lag\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: The dataframe with additional columns for the lags.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert the horizon to a timedelta object\n",
    "    horizon_timedelta = pd.Timedelta(minutes=forecast_horizon)\n",
    "    consecutive_timedelta = df.index[1] - df.index[0]\n",
    "    \n",
    "    # Calculate the number of new columns\n",
    "    n_new_cols = len(df[df.index < df.index[0] + pd.DateOffset(days=max_lag_day)])\n",
    "    \n",
    "    # List to hold all the new lagged columns\n",
    "    new_cols = []\n",
    "    \n",
    "    # Generate lagged columns based on the horizon and max lag\n",
    "    for i in range(n_new_cols):\n",
    "        shift_timedelta = horizon_timedelta + i * consecutive_timedelta\n",
    "        new_col_name = f'y_lag_{shift_timedelta}m'\n",
    "        new_cols.append(df['y'].shift(freq=shift_timedelta).rename(new_col_name))\n",
    "    \n",
    "    # Concatenate the new lagged columns with the original dataframe\n",
    "    df = pd.concat([df] + new_cols, axis=1)\n",
    "    \n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_holdout(df, n_block):    \n",
    "    \"\"\"Separating df into two parts:\n",
    "    1. df : df that will be used for training and blocked k-fold cross validation. \n",
    "    The block is a multiple of a week because net load data has weekly seasonality\n",
    "    2. hold_out_df : this section is not used for now, but can be useful for final test of the chosen model\n",
    "    if wanted, to show the generalized error. This is at least 1 block of data. \n",
    "    \n",
    "    By default, the chosen k for k-fold cross validation is 10.\n",
    "    \n",
    "    For example, the original df has 12 weeks worth of data. \n",
    "    In this case,\n",
    "    new df is week 1-10,\n",
    "    hold_out_df is week 11-12,\n",
    "    \n",
    "    the new df will be used for cross validation, for example\n",
    "    CV1: training: week 1-9, validation (test) week 10\n",
    "    CV2: training: week 1-8, week 10, validation (test) week 9,\n",
    "    etc. \n",
    "\n",
    "    Args:\n",
    "        df (df): cleaned df consisting of y and all predictors\n",
    "        n_block (int): number of blocks to divide the original df. This includes the block for hold_out_df, so if k=10, this n_block = k+1 = 11\n",
    "\n",
    "    Returns:\n",
    "        block_length (int) : number of weeks per block\n",
    "        hodout_df (df) : unused df, can be used later for unbiased estimate of final model performance\n",
    "        df (df) : df that will be used for training and validation (test) set\n",
    "    \"\"\"\n",
    "    \n",
    "    dataset_length_week= ((df.index[-1] - df.index[0]).total_seconds() / 86400/7)\n",
    "    block_length = int(dataset_length_week / n_block)\n",
    "    consecutive_timedelta = df.index[1] - df.index[0]\n",
    "    n_timestep_per_week = int(one_week / consecutive_timedelta)\n",
    "    holdout_start = (n_block - 1)* block_length * n_timestep_per_week\n",
    "    holdout_df = df.iloc[holdout_start:]\n",
    "    df = df.drop(df.index[holdout_start:])\n",
    "    \n",
    "    return block_length, holdout_df, df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_and_process(path_data_cleaned, forecast_horizon, max_lag_day, n_block):\n",
    "    \"\"\"read dataset, add calendar features, add lag features (which depends on the forecast horizon).\n",
    "\n",
    "    Args:\n",
    "        path_data_cleaned (str): path to the dataset chosen\n",
    "        forecast_horizon (int): forecast horizon in minutes\n",
    "        max_lag_day (int): how much lag data will be used, written in days. For example, 7 means lag data until d-7 is used. \n",
    "        n_block (int): number of blocks to divide the original df. This includes the block for hold_out_df, so if k=10, this n_block = k+1 = 11\n",
    "\n",
    "    Returns:\n",
    "        block_length (int): number of weeks per block\n",
    "        holdout_df (df): unused df, can be used later for unbiased estimate of final model performance\n",
    "        df (df): df that will be used for training and validation (test) set\n",
    "    \"\"\"\n",
    "    # MAKE THIS AS FUNCTION\n",
    "    # ADD CALENDAR DATA (holiday to add)\n",
    "    columns_to_use = ['datetime', 'netload_kW']\n",
    "    df = pd.read_csv(path_data_cleaned + dataset, usecols=columns_to_use, index_col=0, parse_dates=True)\n",
    "    df.rename(columns={'netload_kW': 'y'}, inplace=True)\n",
    "    \n",
    "    # 1. Check if forecast horizon is >= dataset frequency\n",
    "    # for example, if dataset is daily, forecast horizon should be at least 1 day\n",
    "    # compute dataset frequency in minutes based on the datetime index\n",
    "    dataset_freq = (df.index[1] - df.index[0]).seconds / 60\n",
    "    if forecast_horizon < dataset_freq:\n",
    "        raise ValueError('Forecast horizon should be >= dataset frequency')\n",
    "    else:\n",
    "        print('Pass Test 1 - Forecast horizon is >= dataset frequency')\n",
    "        \n",
    "    # 2. Check if hyperparameter choice is possible given the forecast horizon\n",
    "    # for example, with forecast horizon of 2 days, we cannot use 1 day as the hyperparameter of seasonal naive forecast.\n",
    "\n",
    "    \n",
    "    if model_name == 'm2_snaive':\n",
    "        if int(hyperparameter['days'] * 24 * 60) < forecast_horizon:\n",
    "            raise ValueError('Choice of seasonal naive hyperparameter needs to be >= forecast horizon! Please change the hyperparameter.')\n",
    "    # if model_name == 'm4_sarima':\n",
    "    #     if int(hyperparameter['seasonal_period_days'] * 24 * 60) < forecast_horizon:\n",
    "    #         raise ValueError('Choice of seasonal_period_days in SARIMA hyperparameter >= forecast horizon! Please change the hyperparameter.')\n",
    "    print('Pass Test 2 - Hyperparameter choice is possible given the forecast horizon')\n",
    "            \n",
    "    \n",
    "\n",
    "    # 1. Numerical representation of the datetime (Excel-style)\n",
    "    numeric_datetime = pd.Series((df.index - pd.Timestamp(\"1970-01-01\")) / pd.Timedelta(days=1), index=df.index)\n",
    "\n",
    "    # 2. Year\n",
    "    year = pd.Series(df.index.year, index=df.index)\n",
    "\n",
    "    # 3. One-hot encoding of month (is_jan, is_feb, ..., is_nov, excluding December)\n",
    "    month_dummies = pd.get_dummies(df.index.month, prefix='is', drop_first=False)\n",
    "\n",
    "    # Custom column names for months: is_jan, is_feb, ..., is_nov\n",
    "    month_names = ['is_jan', 'is_feb', 'is_mar', 'is_apr', 'is_may', 'is_jun', \n",
    "                'is_jul', 'is_aug', 'is_sep', 'is_oct', 'is_nov', 'is_dec']  \n",
    "\n",
    "    # Drop the last column (December) to avoid redundancy and rename the columns\n",
    "    month_dummies = month_dummies.iloc[:, :-1]  # Exclude December column\n",
    "    month_dummies.columns = month_names[:month_dummies.shape[1]]  # Apply custom column names\n",
    "    month_dummies = month_dummies.astype(int)  # Convert to 1 and 0\n",
    "    month_dummies.index = df.index\n",
    "\n",
    "    # 4. One-hot encoding of hour (hour_0, hour_1, ..., hour_22, excluding hour_23)\n",
    "    hour_dummies = pd.get_dummies(df.index.hour, prefix='hour', drop_first=False).iloc[:, :-1]\n",
    "    hour_dummies = hour_dummies.astype(int)  # Convert to 1 and 0\n",
    "    hour_dummies.index = df.index\n",
    "\n",
    "    # 5. One-hot encoding of day of week (is_mon, is_tue, ..., is_sat, excluding Sunday)\n",
    "    # Mapping day of week (0=Mon, 1=Tue, ..., 6=Sun)\n",
    "    dayofweek_dummies = pd.get_dummies(df.index.dayofweek, prefix='is', drop_first=False).iloc[:, :-1]\n",
    "\n",
    "    # Custom mapping for days of the week: is_mon, is_tue, ..., is_sat\n",
    "    dayofweek_names = ['is_mon', 'is_tue', 'is_wed', 'is_thu', 'is_fri', 'is_sat']  # Custom day names\n",
    "    dayofweek_dummies.columns = dayofweek_names[:dayofweek_dummies.shape[1]]  # Apply custom column names\n",
    "    dayofweek_dummies = dayofweek_dummies.astype(int)  # Convert to 1 and 0\n",
    "    dayofweek_dummies.index = df.index\n",
    "\n",
    "    # 6. Is weekday (1 if Monday to Friday, 0 if Saturday/Sunday)\n",
    "    is_weekday = pd.Series((df.index.dayofweek < 5).astype(int), index=df.index)\n",
    "\n",
    "\n",
    "    # Concatenate all new features into the original dataframe at once\n",
    "    df = pd.concat([df, \n",
    "                    numeric_datetime.rename('numeric_datetime'), \n",
    "                    year.rename('year'),\n",
    "                    month_dummies, \n",
    "                    hour_dummies, \n",
    "                    dayofweek_dummies, \n",
    "                    is_weekday.rename('is_weekday')], axis=1)\n",
    "    \n",
    "    df = add_lag_features(df, forecast_horizon, max_lag_day)\n",
    "    block_length, holdout_df, df = separate_holdout(df, n_block)\n",
    "    \n",
    "    return block_length, holdout_df, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT TRAIN - DEV - TEST SET\n",
    "def split_time_series(df, cv_no):\n",
    "    \"\"\"Split df to train and test set using blocked cross validation.\n",
    "\n",
    "    Args:\n",
    "        df (df): df that will be used for training and validation (test) set, consists of X and Y\n",
    "        cv_no (int): number of current cv order. \n",
    "                     cv_no=1 means the test set is at the last, cv_no = k means the test set is at the beginning\n",
    "\n",
    "    Returns:\n",
    "        train_df (df) : df used for training\n",
    "        test_df (df) : df used for validation, formal name is validation set / dev set. \n",
    "    \"\"\"\n",
    "      \n",
    "    n = len(df)\n",
    "    test_start = int(n*(1 - cv_no*test_pct))\n",
    "    test_end = int(n*(1 - (cv_no-1)*test_pct))\n",
    "    \n",
    "    test_df = df.iloc[test_start:test_end]\n",
    "    train_df = df.drop(df.index[test_start:test_end])\n",
    "    \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT X AND y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_xy(df):\n",
    "    \"\"\"separate forecast target y and all predictors X into two dfs\n",
    "\n",
    "    Args:\n",
    "        df (df): df containing the forecast target y and all predictors X\n",
    "        \n",
    "    Return:\n",
    "        df_X (df): df of all predictors X\n",
    "        df_y (df): df of target forecast y\n",
    "    \"\"\"\n",
    "\n",
    "    df_y = df[['y']]\n",
    "    df_X = df.drop(\"y\", axis=1)\n",
    "    \n",
    "    return df_X, df_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform below scripts into function with input train_df_y and output train_df_y_updated\n",
    "def remove_jump_df(train_df_y):\n",
    "    #make docstring with the same format like other cells\n",
    "    \"\"\"\n",
    "    Remove jump in the time series data\n",
    "    Parameters:\n",
    "        train_df_y (pd.Series): Time series data\n",
    "        \n",
    "    Returns:\n",
    "        train_df_y_updated (pd.Series): Time series data with jump removed\n",
    "    \"\"\"\n",
    "    \n",
    "    time_diff = train_df_y.index.to_series().diff().dt.total_seconds()\n",
    "    initial_freq = time_diff.iloc[1]\n",
    "    jump_indices = time_diff[time_diff > initial_freq].index\n",
    "    if not jump_indices.empty:\n",
    "        jump_index = jump_indices[0]\n",
    "        jump_pos = train_df_y.index.get_loc(jump_index)\n",
    "        train_df_y_updated = train_df_y.iloc[:jump_pos]\n",
    "    else:\n",
    "        train_df_y_updated = train_df_y\n",
    "    return train_df_y_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_name, hyperparameter, train_df_X, train_df_y, forecast_horizon):\n",
    "    \"\"\"train model based on the model choice, hyperparamter, predictors X and target y\n",
    "\n",
    "    Args:\n",
    "        model_name (string): eg 'm06_lr'\n",
    "        hyperparameter (pd series): list of hyperparameter for that model\n",
    "        train_df_X (df): matrix of predictors\n",
    "        train_df_y (df): target forecast y\n",
    "        forecast_horizon (int): Forecast horizon in minutes.\n",
    "\n",
    "    Returns:\n",
    "        model (dict) : general object storing all models info including the predictor, feature selection, \n",
    "        and all other relevant features\n",
    "    \"\"\"\n",
    "    \n",
    "    if model_name == 'm1_naive':\n",
    "        model = train_model_m1_naive(hyperparameter, train_df_X, train_df_y)\n",
    "    elif model_name == 'm2_snaive':\n",
    "        model = train_model_m2_snaive(hyperparameter, train_df_X, train_df_y)\n",
    "    elif model_name == 'm3_ets':\n",
    "        model = train_model_m3_ets(hyperparameter, train_df_X, train_df_y, forecast_horizon)\n",
    "    elif model_name == 'm4_arima':\n",
    "        model = train_model_m4_arima(hyperparameter, train_df_X, train_df_y, forecast_horizon)\n",
    "    elif model_name == 'm5_sarima':\n",
    "        model = train_model_m5_sarima(hyperparameter, train_df_X, train_df_y, forecast_horizon)\n",
    "    elif model_name == 'm6_lr':\n",
    "        model = train_model_m6_lr(hyperparameter, train_df_X, train_df_y)\n",
    "    elif model_name == 'm7_ann':\n",
    "        model = train_model_m7_ann(hyperparameter, train_df_X, train_df_y)\n",
    "    elif model_name == 'm8_dnn':\n",
    "        model = train_model_m8_dnn(hyperparameter, train_df_X, train_df_y)\n",
    "    elif model_name == 'm9_rt':\n",
    "        model = train_model_m9_rt(hyperparameter, train_df_X, train_df_y)\n",
    "    elif model_name == 'm10_rf':\n",
    "        model = train_model_m10_rf(hyperparameter, train_df_X, train_df_y)\n",
    "    elif model_name == 'm11_svr':\n",
    "        model = train_model_m11_svr(hyperparameter, train_df_X, train_df_y)\n",
    "    elif model_name == 'm12_rnn':\n",
    "        model = train_model_m12_rnn(hyperparameter, train_df_X, train_df_y)\n",
    "    elif model_name == 'm13_lstm':\n",
    "        model = train_model_m13_lstm(hyperparameter, train_df_X, train_df_y)\n",
    "    elif model_name == 'm14_gru':\n",
    "        model = train_model_m14_gru(hyperparameter, train_df_X, train_df_y)\n",
    "    elif model_name == 'm15_transformer':\n",
    "        model = train_model_m15_transformer(hyperparameter, train_df_X, train_df_y)\n",
    "    elif model_name == 'm16_prophet':\n",
    "        model = train_model_m16_prophet(hyperparameter, train_df_X, train_df_y, forecast_horizon)\n",
    "    elif model_name == 'm17_xgb':\n",
    "        model = train_model_m17_xgb(hyperparameter, train_df_X, train_df_y)\n",
    "    elif model_name == 'm18_nbeats':\n",
    "        model = train_model_m18_nbeats(hyperparameter, train_df_X, train_df_y)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Wrong Model Choice! Available models are: m1_naive, m2_snaive, m3_ets, m4_arima, m5_sarima, m6_lr, m7_ann, m8_dnn, m9_rt, m10_rf, m11_svr, m12_rnn, m13_lstm, m14_gru, m15_transformer, m16_prophet, m17_xgb, m18_nbeats\"\n",
    "        )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_forecast(model_name, model, train_df_X, test_df_X, train_df_y, forecast_horizon):\n",
    "    \"\"\"Generate forecasts based on the model and its name.\n",
    "\n",
    "    Args:\n",
    "        model_name (string): Model identifier (e.g., 'm1_naive').\n",
    "        model (dict): Trained model object containing all relevant features.\n",
    "        train_df_X (DataFrame): Matrix of predictors for training set.\n",
    "        test_df_X (DataFrame): Matrix of predictors for test set.\n",
    "        train_df_y (DataFrame): Target forecast y for training set.\n",
    "        forecast_horizon (int): Forecast horizon in minutes.\n",
    "\n",
    "    Returns:\n",
    "        train_df_y_hat (DataFrame): Forecasted values for training set.\n",
    "        test_df_y_hat (DataFrame): Forecasted values for test set.\n",
    "    \"\"\"\n",
    "    \n",
    "    if model_name == 'm1_naive':\n",
    "        train_df_y_hat, test_df_y_hat = produce_forecast_m1_naive(model, train_df_X, test_df_X)\n",
    "    elif model_name == 'm2_snaive':\n",
    "        train_df_y_hat, test_df_y_hat = produce_forecast_m2_snaive(model, train_df_X, test_df_X)\n",
    "    elif model_name == 'm3_ets':\n",
    "        train_df_y_hat, test_df_y_hat = produce_forecast_m3_ets(model, train_df_X, test_df_X, forecast_horizon)\n",
    "    elif model_name == 'm4_arima':\n",
    "        train_df_y_hat, test_df_y_hat = produce_forecast_m4_arima(model, train_df_X, test_df_X, forecast_horizon)\n",
    "    elif model_name == 'm5_sarima':\n",
    "        train_df_y_hat, test_df_y_hat = produce_forecast_m5_sarima(model, train_df_X, test_df_X, forecast_horizon)\n",
    "    elif model_name == 'm6_lr':\n",
    "        train_df_y_hat, test_df_y_hat = produce_forecast_m6_lr(model, train_df_X, test_df_X)\n",
    "    elif model_name == 'm7_ann':\n",
    "        train_df_y_hat, test_df_y_hat = produce_forecast_m7_ann(model, train_df_X, test_df_X)\n",
    "    elif model_name == 'm8_dnn':\n",
    "        train_df_y_hat, test_df_y_hat = produce_forecast_m8_dnn(model, train_df_X, test_df_X)\n",
    "    elif model_name == 'm9_rt':\n",
    "        train_df_y_hat, test_df_y_hat = produce_forecast_m9_rt(model, train_df_X, test_df_X)\n",
    "    elif model_name == 'm10_rf':\n",
    "        train_df_y_hat, test_df_y_hat = produce_forecast_m10_rf(model, train_df_X, test_df_X)\n",
    "    elif model_name == 'm11_svr':\n",
    "        train_df_y_hat, test_df_y_hat = produce_forecast_m11_svr(model, train_df_X, test_df_X)\n",
    "    elif model_name == 'm12_rnn':\n",
    "        train_df_y_hat, test_df_y_hat = produce_forecast_m12_rnn(model, train_df_X, test_df_X)\n",
    "    elif model_name == 'm13_lstm':\n",
    "        train_df_y_hat, test_df_y_hat = produce_forecast_m13_lstm(model, train_df_X, test_df_X)\n",
    "    elif model_name == 'm14_gru':\n",
    "        train_df_y_hat, test_df_y_hat = produce_forecast_m14_gru(model, train_df_X, test_df_X)\n",
    "    elif model_name == 'm15_transformer':\n",
    "        train_df_y_hat, test_df_y_hat = produce_forecast_m15_transformer(model, train_df_X, test_df_X)\n",
    "    elif model_name == 'm16_prophet':\n",
    "        train_df_y_hat, test_df_y_hat = produce_forecast_m16_prophet(model, train_df_X, test_df_X, train_df_y, forecast_horizon)\n",
    "    elif model_name == 'm17_xgb':\n",
    "        train_df_y_hat, test_df_y_hat = produce_forecast_m17_xgb(model, train_df_X, test_df_X)\n",
    "    elif model_name == 'm18_nbeats':\n",
    "        train_df_y_hat, test_df_y_hat = produce_forecast_m18_nbeats(model, train_df_X, test_df_X)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Wrong Model Choice! Available models are: m1_naive, m2_snaive, m3_ets, m4_arima, m5_sarima, m6_lr, m7_ann, m8_dnn, m9_rt, m10_rf, m11_svr, m12_rnn, m13_lstm, m14_gru, m15_transformer, m16_prophet, m17_xgb, m18_nbeats\"\n",
    "        )\n",
    "\n",
    "    return train_df_y_hat, test_df_y_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(filepath, cv_no, model):\n",
    "    \"\"\"Export model into binary file using pickle to a designated file\n",
    "\n",
    "    Args:\n",
    "        filepath (dictionary): dictionary of the file path\n",
    "        cv_no (int) : cv number\n",
    "        model (dictionary): trained model\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(filepath['model'][cv_no], \"wb\") as model_file:\n",
    "        # pickle.dump(model, model_file)\n",
    "        dill.dump(model, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(df, model_name, hyperparameter, filepath, forecast_horizon):\n",
    "    \"\"\"Run model! This will be updated so that it can adapt to any model \n",
    "    This consists of\n",
    "    1. Loop over all CV, then inside the loop,\n",
    "    2. Split df to train and test set\n",
    "    3. Train model on train set\n",
    "    4. Produce naive forecast on both trian and test set for benchmark\n",
    "    5. Produce forecast on both train and test set\n",
    "    6. Compute residual on both train and test set\n",
    "    7. Export the observation, naive, forecast, and residual of train and test set\n",
    "    8. Produce plots only on CV 1\n",
    "    9. Evaluate forecast performance on train and test set\n",
    "    10. Quit the loop,\n",
    "    11. Summarise the overall performance of the model using RMSE and its Stddev\n",
    "    12. Export all results     \n",
    "    \n",
    "    Args:\n",
    "        df (df): df that will be used for training and validation (test) set, consists of X and Y\n",
    "        model_name (string): eg 'm06_lr'\n",
    "        hyperparameter (pd series): list of hyperparameter for that model\n",
    "        filepath (dict): dictionary of filepaths for exporting result\n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "    cross_val_result_df = pd.DataFrame()\n",
    "\n",
    "    # Compute max_y for normalization later\n",
    "    max_y = df['y'].max()\n",
    "    \n",
    "    # DO CROSS VALIDATION\n",
    "    for cv_no in range(1, k+1):\n",
    "        print(f'Processing CV {cv_no} / {k}....')\n",
    "        \n",
    "        # SPLIT INTO TRAIN AND TEST X AND Y\n",
    "        train_df, test_df = split_time_series(df, cv_no)\n",
    "        train_df_X, train_df_y = split_xy(train_df)\n",
    "        test_df_X, test_df_y = split_xy(test_df)\n",
    "\n",
    "        # INITIALISE RESULT DF   \n",
    "        train_result = train_df_y.copy()\n",
    "        train_result = train_result.rename(columns={'y': 'observation'})\n",
    "\n",
    "        test_result = test_df_y.copy()\n",
    "        test_result = test_result.rename(columns={'y': 'observation'})\n",
    "\n",
    "        # PRODUCE NAIVE FORECAST\n",
    "        horizon_timedelta = pd.Timedelta(minutes=forecast_horizon)\n",
    "        last_observation = f'y_lag_{horizon_timedelta}m'\n",
    "        train_result['naive'] = train_df[last_observation]\n",
    "        test_result['naive'] = test_df[last_observation]\n",
    "\n",
    "        # TRAIN MODEL\n",
    "        start_time = time.time()\n",
    "        model = train_model(model_name, hyperparameter, train_df_X, train_df_y, forecast_horizon)\n",
    "        save_model(filepath, cv_no, model)\n",
    "        end_time = time.time()\n",
    "        runtime_ms = (end_time - start_time) * 1000  # Convert to milliseconds\n",
    "\n",
    "        # PRODUCE FORECAST\n",
    "        train_df_y_hat, test_df_y_hat = produce_forecast(model_name, model, train_df_X, test_df_X, train_df_y, forecast_horizon)\n",
    "        train_result['forecast'] = train_df_y_hat\n",
    "        test_result['forecast'] = test_df_y_hat\n",
    "        \n",
    "        # EVALUATE FORECAST\n",
    "        train_result['residual'] = train_result['forecast'] - train_result['observation']\n",
    "        test_result['residual'] = test_result['forecast'] - test_result['observation']\n",
    "        train_R2 = compute_R2(train_result['forecast'], train_result['observation'])\n",
    "        test_R2 = compute_R2(test_result['forecast'], test_result['observation'])\n",
    "        \n",
    "        train_RMSE = compute_RMSE(train_result['forecast'], train_result['observation'])\n",
    "        test_RMSE = compute_RMSE(test_result['forecast'], test_result['observation'])\n",
    "        \n",
    "        train_nRMSE = 100*train_RMSE / max_y # in percent\n",
    "        test_nRMSE = 100*test_RMSE / max_y # in percent\n",
    "        \n",
    "        cross_val_result = pd.DataFrame(\n",
    "        {\n",
    "            \"runtime_ms\": runtime_ms,\n",
    "            \"train_MBE\": compute_MBE(train_result['forecast'], train_result['observation']), \n",
    "            \"train_MAE\": compute_MAE(train_result['forecast'], train_result['observation']),\n",
    "            \"train_RMSE\": train_RMSE,\n",
    "            \"train_MAPE\": compute_MAPE(train_result['forecast'], train_result['observation']),\n",
    "            \"train_MASE\": compute_MASE(train_result['forecast'], train_result['observation'], train_result),\n",
    "            \"train_fskill\": compute_fskill(train_result['forecast'], train_result['observation'], train_result['naive']),\n",
    "            \"train_R2\": train_R2,\n",
    "            \"test_MBE\": compute_MBE(test_result['forecast'], test_result['observation']),\n",
    "            \"test_MAE\": compute_MAE(test_result['forecast'], test_result['observation']),\n",
    "            \"test_RMSE\": test_RMSE,\n",
    "            \"test_MAPE\": compute_MAPE(test_result['forecast'], test_result['observation']),\n",
    "            \"test_MASE\": compute_MASE(test_result['forecast'], test_result['observation'], train_result),\n",
    "            \"test_fskill\": compute_fskill(test_result['forecast'], test_result['observation'], test_result['naive']),\n",
    "            \"test_R2\": test_R2,\n",
    "            \"train_nRMSE\": train_nRMSE,\n",
    "            \"test_nRMSE\": test_nRMSE\n",
    "        }, \n",
    "        index=[cv_no]\n",
    "        )\n",
    "        \n",
    "        if cross_val_result_df.empty:\n",
    "            cross_val_result_df = cross_val_result\n",
    "        else:\n",
    "            cross_val_result_df = pd.concat([cross_val_result_df, cross_val_result], ignore_index=False)\n",
    "        cross_val_result_df.index.name = 'cv_no'\n",
    "        \n",
    "        # EXPORT RESULTS DF TO CSV\n",
    "        train_result.to_csv(filepath['train_cv'][cv_no])\n",
    "        test_result.to_csv(filepath['test_cv'][cv_no])\n",
    "        \n",
    "        # IF CV_NO = 1, ALSO EXPORT SOME PLOTS\n",
    "        if cv_no == 1:\n",
    "            timeplot_forecast(train_result['observation'], train_result['forecast'], filepath['b1'])\n",
    "            timeplot_forecast(test_result['observation'], test_result['forecast'], filepath['c1'])\n",
    "            scatterplot_forecast(train_result['observation'], train_result['forecast'], train_R2, filepath['b2'])\n",
    "            scatterplot_forecast(test_result['observation'], test_result['forecast'], test_R2, filepath['c2'])\n",
    "            timeplot_residual(train_result['residual'], filepath['b3'])\n",
    "            timeplot_residual(test_result['residual'], filepath['c3'])\n",
    "            histogram_residual(train_result['residual'], df, filepath['b4'])\n",
    "            histogram_residual(test_result['residual'], df, filepath['c4'])\n",
    "        print()\n",
    "        \n",
    "        \n",
    "    cross_val_result = pd.DataFrame(\n",
    "        {\n",
    "            \"runtime_ms\": [cross_val_result_df['runtime_ms'].mean(), cross_val_result_df['runtime_ms'].std()],\n",
    "            \"train_MBE\": [cross_val_result_df['train_MBE'].mean(), cross_val_result_df['train_MBE'].std()], \n",
    "            \"train_MAE\": [cross_val_result_df['train_MAE'].mean(), cross_val_result_df['train_MAE'].std()],\n",
    "            \"train_RMSE\": [cross_val_result_df['train_RMSE'].mean(), cross_val_result_df['train_RMSE'].std()],\n",
    "            \"train_MAPE\": [cross_val_result_df['train_MAPE'].mean(), cross_val_result_df['train_MAPE'].std()],\n",
    "            \"train_MASE\": [cross_val_result_df['train_MASE'].mean(), cross_val_result_df['train_MASE'].std()],\n",
    "            \"train_fskill\": [cross_val_result_df['train_fskill'].mean(), cross_val_result_df['train_fskill'].std()],\n",
    "            \"train_R2\": [cross_val_result_df['train_R2'].mean(), cross_val_result_df['train_R2'].std()],\n",
    "            \"test_MBE\": [cross_val_result_df['test_MBE'].mean(), cross_val_result_df['test_MBE'].std()],\n",
    "            \"test_MAE\": [cross_val_result_df['test_MAE'].mean(), cross_val_result_df['test_MAE'].std()],\n",
    "            \"test_RMSE\": [cross_val_result_df['test_RMSE'].mean(), cross_val_result_df['test_RMSE'].std()],\n",
    "            \"test_MAPE\": [cross_val_result_df['test_MAPE'].mean(), cross_val_result_df['test_MAPE'].std()],\n",
    "            \"test_MASE\": [cross_val_result_df['test_MASE'].mean(), cross_val_result_df['test_MASE'].std()],\n",
    "            \"test_fskill\": [cross_val_result_df['test_fskill'].mean(), cross_val_result_df['test_fskill'].std()],\n",
    "            \"test_R2\": [cross_val_result_df['test_R2'].mean(), cross_val_result_df['test_R2'].std()],\n",
    "            \"train_nRMSE\": [cross_val_result_df['train_nRMSE'].mean(), cross_val_result_df['train_nRMSE'].std()],\n",
    "            \"test_nRMSE\": [cross_val_result_df['test_nRMSE'].mean(), cross_val_result_df['test_nRMSE'].std()]\n",
    "        }, \n",
    "        index=['mean', 'stddev']\n",
    "        )\n",
    "\n",
    "    cross_val_result_df = pd.concat([cross_val_result_df, cross_val_result], ignore_index=False)\n",
    "\n",
    "    data_a1 = {\n",
    "        \"experiment_no\": experiment_no_str,\n",
    "        \"exp_date\": datetime.today().strftime('%Y-%m-%d'), #today date in YYYY-MM-DD format\n",
    "        \"dataset_no\": dataset.split('_')[0],\n",
    "        \"dataset\": dataset.split('_')[1].split('.')[0],\n",
    "        \"dataset_freq_min\": int((df.index[1] - df.index[0]).total_seconds() / 60),\n",
    "        \"dataset_length_week\": block_length * (n_block - 1),\n",
    "        \"forecast_horizon_min\": forecast_horizon,\n",
    "        \"train_pct\": train_pct,\n",
    "        \"test_pct\": test_pct,\n",
    "        \"model_no\": model_name.split('_')[0],\n",
    "        \"hyperparameter_no\": hyperparameter_no,\n",
    "        \"model_name\": model_name + '_' + hyperparameter_no,\n",
    "        \"hyperparamter\": ', '.join(f\"{k}: {v}\" for k, v in hyperparameter.items()),  \n",
    "        \"runtime_ms\": cross_val_result_df.loc['mean', 'runtime_ms'],\n",
    "        \"train_RMSE\": cross_val_result_df.loc['mean', 'train_RMSE'],\n",
    "        \"train_RMSE_stddev\": cross_val_result_df.loc['stddev', 'train_RMSE'],\n",
    "        \"test_RMSE\": cross_val_result_df.loc['mean', 'test_RMSE'],\n",
    "        \"test_RMSE_stddev\": cross_val_result_df.loc['stddev', 'test_RMSE'],\n",
    "        \"train_nRMSE\": cross_val_result_df.loc['mean', 'train_nRMSE'],\n",
    "        \"train_nRMSE_stddev\": cross_val_result_df.loc['stddev', 'train_nRMSE'],\n",
    "        \"test_nRMSE\": cross_val_result_df.loc['mean', 'test_nRMSE'],\n",
    "        \"test_nRMSE_stddev\": cross_val_result_df.loc['stddev', 'test_nRMSE']\n",
    "    }\n",
    "\n",
    "    # Create a df of experiment result\n",
    "    df_a1_result = pd.DataFrame([data_a1])\n",
    "    \n",
    "    export_result(filepath, df_a1_result, cross_val_result_df)\n",
    "    \n",
    "    # return df_a1_result, cross_val_result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PERFORMANCE COMPUTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Mean Bias Error (MBE)\n",
    "def compute_MBE(forecast, observation):\n",
    "    \"\"\"As the name suggest.\n",
    "\n",
    "    Args:\n",
    "        forecast (df): series of the forecast result from the model\n",
    "        observation (df): series of the observed value (actual value)\n",
    "\n",
    "    Returns:\n",
    "        error as the name suggest (float): as the name suggest\n",
    "    \"\"\"\n",
    "    return round(((forecast - observation).sum()) / len(observation), 5)\n",
    "\n",
    "# Mean Absolute Error (MAE)\n",
    "def compute_MAE(forecast, observation):\n",
    "    \"\"\"As the name suggest.\n",
    "\n",
    "    Args:\n",
    "        forecast (df): series of the forecast result from the model\n",
    "        observation (df): series of the observed value (actual value)\n",
    "\n",
    "    Returns:\n",
    "        error as the name suggest (float): as the name suggest\n",
    "    \"\"\"\n",
    "    return round((abs(forecast - observation)).mean(), 3)\n",
    "\n",
    "# Root Mean Square Error (RMSE)\n",
    "def compute_RMSE(forecast, observation):\n",
    "    \"\"\"As the name suggest.\n",
    "\n",
    "    Args:\n",
    "        forecast (df): series of the forecast result from the model\n",
    "        observation (df): series of the observed value (actual value)\n",
    "\n",
    "    Returns:\n",
    "        error as the name suggest (float): as the name suggest\n",
    "    \"\"\"\n",
    "    return round(np.sqrt(((forecast - observation) ** 2).mean()), 3)\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "def compute_MAPE(forecast, observation):\n",
    "    \"\"\"As the name suggest. Be careful with MAPE though because its value can go to inf since the observed value can be 0. \n",
    "\n",
    "    Args:\n",
    "        forecast (df): series of the forecast result from the model\n",
    "        observation (df): series of the observed value (actual value)\n",
    "\n",
    "    Returns:\n",
    "        error as the name suggest (float): as the name suggest\n",
    "    \"\"\"\n",
    "    return round((abs((forecast - observation) / observation) * 100).mean(), 3)\n",
    "\n",
    "# Mean Absolute Scaled Error (MASE)\n",
    "def compute_MASE(forecast, observation, train_result):\n",
    "    \"\"\"As the name suggest. MASE is first introduced by Rob Hyndman, used to handle MAPE problem being infinity. \n",
    "    Instead of using observed value as denominator,\n",
    "    MASE uses MAE of the naive forecast at the train set for denominator. \n",
    "\n",
    "    Args:\n",
    "        forecast (df): series of the forecast result from the model\n",
    "        observation (df): series of the observed value (actual value)\n",
    "\n",
    "    Returns:\n",
    "        error as the name suggest (float): as the name suggest\n",
    "    \"\"\"\n",
    "    errors = abs(forecast - observation)\n",
    "    MAE_naive = compute_MAE(train_result['naive'], train_result['observation'])\n",
    "    \n",
    "    MASE = errors.mean() / MAE_naive\n",
    "    return round(MASE, 3)\n",
    "\n",
    "# Forecast Skill (FS)\n",
    "def compute_fskill(forecast, observation, naive):\n",
    "    \"\"\"As the name suggest. Forecast Skill is a relative measure seeing the improvement \n",
    "    of the model performance over naive model. \n",
    "\n",
    "    Args:\n",
    "        forecast (df): series of the forecast result from the model\n",
    "        observation (df): series of the observed value (actual value)\n",
    "\n",
    "    Returns:\n",
    "        error as the name suggest (float): as the name suggest\n",
    "    \"\"\"\n",
    "    return round((1 - compute_RMSE(forecast, observation) / compute_RMSE(naive, observation)) * 100, 3)\n",
    "\n",
    "# R2\n",
    "def compute_R2(forecast, observation):\n",
    "    \"\"\"As the name suggest. Be careful with R2 though because it is not a forecast evaluation. \n",
    "    It is just used to show linearity on the scatter plot of forecast and observed value. \n",
    "\n",
    "    Args:\n",
    "        forecast (df): series of the forecast result from the model\n",
    "        observation (df): series of the observed value (actual value)\n",
    "\n",
    "    Returns:\n",
    "        error as the name suggest (float): as the name suggest\n",
    "    \"\"\"\n",
    "    return round(forecast.corr(observation)**2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def timeplot_forecast(observation, forecast, pathname):\n",
    "    \"\"\"Produce time plot of observation vs forecast value and save it on the designated folder\n",
    "\n",
    "    Args:\n",
    "        observation (df): observed value\n",
    "        forecast (df): forecast value\n",
    "        pathname (str): filepath to save the figure\n",
    "    \"\"\"\n",
    "    consecutive_timedelta = observation.index[-1] - observation.index[-2]\n",
    "    # Calculate total minutes in a week\n",
    "    minutes_per_week = 7 * 24 * 60  # 7 days * 24 hours * 60 minutes\n",
    "\n",
    "    # Calculate the number of minutes per timestep\n",
    "    minutes_per_timestep = consecutive_timedelta.total_seconds() / 60  # convert seconds to minutes\n",
    "\n",
    "    # Compute the number of timesteps in a week\n",
    "    timesteps_per_week = int(minutes_per_week / minutes_per_timestep)\n",
    "\n",
    "    # Create the figure with specified size\n",
    "    plt.figure(figsize=(9, 9))\n",
    "\n",
    "    # Set background color\n",
    "    # plt.gcf().patch.set_facecolor(platinum)\n",
    "\n",
    "    # Plot the actual and forecast data\n",
    "    plt.plot(observation[-timesteps_per_week:], color=dark_blue, label='Actual')\n",
    "    plt.plot(forecast[-timesteps_per_week:], color=orange, label='Forecast')\n",
    "\n",
    "    # Set the font to Arial\n",
    "    plt.rcParams['font.family'] = 'Arial'\n",
    "\n",
    "    # Remove grid lines\n",
    "    plt.grid(False)\n",
    "\n",
    "    # Set tick marks for x and y axis\n",
    "    plt.xticks(fontsize=12, color=dark_blue, alpha=0.5, rotation=30)\n",
    "    plt.yticks(fontsize=12, color=dark_blue, alpha=0.5)\n",
    "\n",
    "    # Add borders to the plot\n",
    "    plt.gca().spines['top'].set_color(dark_blue)\n",
    "    plt.gca().spines['right'].set_color(dark_blue)\n",
    "    plt.gca().spines['bottom'].set_color(dark_blue)\n",
    "    plt.gca().spines['left'].set_color(dark_blue)\n",
    "\n",
    "    # Remove the tick markers (the small lines)\n",
    "    plt.tick_params(axis='x', which='both', length=0)  # Remove x-axis tick markers\n",
    "    plt.tick_params(axis='y', which='both', length=0)  # Remove y-axis tick markers\n",
    "\n",
    "    # Set axis titles\n",
    "    plt.xlabel('Time', fontsize=14, color=dark_blue)\n",
    "    plt.ylabel('Net Load (kW)', fontsize=14, color=dark_blue)\n",
    "\n",
    "    # Remove title\n",
    "    plt.title('')\n",
    "\n",
    "    plt.legend(loc='upper left', fontsize=12, frameon=False, labelspacing=1, bbox_to_anchor=(1, 1))\n",
    "\n",
    "    plt.savefig(pathname, format='png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "    # Show the plot\n",
    "    # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatterplot_forecast(observation, forecast, R2, pathname):\n",
    "    \"\"\"Produce scatterplot observation vs forecast value and save it on the designated folder\n",
    "\n",
    "    Args:\n",
    "        observation (df): observed value\n",
    "        forecast (df): forecast value\n",
    "        pathname (str): filepath to save the figure\n",
    "    \"\"\"\n",
    "    # Create the figure with specified size\n",
    "    plt.figure(figsize=(9, 9))\n",
    "\n",
    "    # Set background color\n",
    "    # plt.gcf().patch.set_facecolor(platinum)\n",
    "\n",
    "    # Plot the actual and forecast data\n",
    "    plt.scatter(forecast, observation, color=dark_blue, label='Actual', s=40, alpha=0.7)  # 's' sets the size of the points\n",
    "\n",
    "\n",
    "    # Set the font to Arial\n",
    "    plt.rcParams['font.family'] = 'Arial'\n",
    "\n",
    "    # Remove grid lines\n",
    "    plt.grid(False)\n",
    "\n",
    "    # Set tick marks for x and y axis\n",
    "    plt.xticks(fontsize=12, color=dark_blue, alpha=0.5, rotation=0)\n",
    "    plt.yticks(fontsize=12, color=dark_blue, alpha=0.5)\n",
    "\n",
    "    # Add borders to the plot\n",
    "    plt.gca().spines['top'].set_color(dark_blue)\n",
    "    plt.gca().spines['right'].set_color(dark_blue)\n",
    "    plt.gca().spines['bottom'].set_color(dark_blue)\n",
    "    plt.gca().spines['left'].set_color(dark_blue)\n",
    "\n",
    "    # Remove the tick markers (the small lines)\n",
    "    plt.tick_params(axis='x', which='both', length=0)  # Remove x-axis tick markers\n",
    "    plt.tick_params(axis='y', which='both', length=0)  # Remove y-axis tick markers\n",
    "\n",
    "    # Set axis titles\n",
    "    plt.xlabel('Net Load Forecast (kW)', fontsize=14, color=dark_blue)\n",
    "    plt.ylabel('Net Load Observation (kW)', fontsize=14, color=dark_blue)\n",
    "\n",
    "    # Remove title\n",
    "    plt.title('')\n",
    "    \n",
    "    # Add R value at the top-left corner\n",
    "    plt.text(0.95, 0.05, f'R = {R2:.3f}', transform=plt.gca().transAxes, \n",
    "         fontsize=14, color=dark_blue, verticalalignment='bottom', horizontalalignment='right',\n",
    "         bbox=dict(facecolor='white', edgecolor=dark_blue, boxstyle='round,pad=0.5', linewidth=1))\n",
    "\n",
    "\n",
    "    plt.savefig(pathname, format='png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "    # Show the plot\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeplot_residual(residual, pathname):\n",
    "    \"\"\"Produce time plot of resodia; value and save it on the designated folder\n",
    "\n",
    "    Args:\n",
    "        residual (df): forecast - observation\n",
    "        pathname (str): filepath to save the figure\n",
    "    \"\"\"\n",
    "    consecutive_timedelta = residual.index[-1] - residual.index[-2]\n",
    "    # Calculate total minutes in a week\n",
    "    minutes_per_week = 7 * 24 * 60  # 7 days * 24 hours * 60 minutes\n",
    "\n",
    "    # Calculate the number of minutes per timestep\n",
    "    minutes_per_timestep = consecutive_timedelta.total_seconds() / 60  # convert seconds to minutes\n",
    "\n",
    "    # Compute the number of timesteps in a week\n",
    "    timesteps_per_week = int(minutes_per_week / minutes_per_timestep)\n",
    "\n",
    "    # Create the figure with specified size\n",
    "    plt.figure(figsize=(9, 9))\n",
    "\n",
    "    # Set background color\n",
    "    # plt.gcf().patch.set_facecolor(platinum)\n",
    "\n",
    "    # Plot the actual and forecast data\n",
    "    plt.plot(residual[-timesteps_per_week:], color=dark_blue, label='Actual')\n",
    "\n",
    "    # Set the font to Arial\n",
    "    plt.rcParams['font.family'] = 'Arial'\n",
    "\n",
    "    # Remove grid lines\n",
    "    plt.grid(False)\n",
    "\n",
    "    # Set tick marks for x and y axis\n",
    "    plt.xticks(fontsize=12, color=dark_blue, alpha=0.5, rotation=30)\n",
    "    plt.yticks(fontsize=12, color=dark_blue, alpha=0.5)\n",
    "\n",
    "    # Add borders to the plot\n",
    "    plt.gca().spines['top'].set_color(dark_blue)\n",
    "    plt.gca().spines['right'].set_color(dark_blue)\n",
    "    plt.gca().spines['bottom'].set_color(dark_blue)\n",
    "    plt.gca().spines['left'].set_color(dark_blue)\n",
    "\n",
    "    # Remove the tick markers (the small lines)\n",
    "    plt.tick_params(axis='x', which='both', length=0)  # Remove x-axis tick markers\n",
    "    plt.tick_params(axis='y', which='both', length=0)  # Remove y-axis tick markers\n",
    "\n",
    "    # Set axis titles\n",
    "    plt.xlabel('Time', fontsize=14, color=dark_blue)\n",
    "    plt.ylabel('Forecast Residual (kW)', fontsize=14, color=dark_blue)\n",
    "\n",
    "    # Remove title\n",
    "    plt.title('')\n",
    "\n",
    "    plt.savefig(pathname, format='png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "    # Show the plot\n",
    "    # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram_residual(residual, df, pathname):\n",
    "    \"\"\"Produce histogiram of residual value and save it on the designated folder\n",
    "\n",
    "    Args:\n",
    "        residual (df): forecast - observation\n",
    "        pathname (str): filepath to save the figure\n",
    "    \"\"\"\n",
    "    # Create the figure with specified size\n",
    "    plt.figure(figsize=(9, 9))\n",
    "\n",
    "    # Set background color\n",
    "    # plt.gcf().patch.set_facecolor(platinum)\n",
    "\n",
    "    # Compute the range\n",
    "    dataset_range = df['y'].max() - df['y'].min()\n",
    "    bin_min = -dataset_range/7\n",
    "    bin_max = dataset_range/7\n",
    "    \n",
    "    # Plot the actual and forecast data\n",
    "    plt.hist(residual, bins=31, range=(bin_min, bin_max), color=dark_blue, edgecolor=dark_blue, alpha=0.7)\n",
    "    \n",
    "\n",
    "    # Set the font to Arial\n",
    "    plt.rcParams['font.family'] = 'Arial'\n",
    "\n",
    "    # Remove grid lines\n",
    "    plt.grid(False)\n",
    "\n",
    "    # Set tick marks for x and y axis\n",
    "    plt.xticks(fontsize=12, color=dark_blue, alpha=0.5, rotation=0)\n",
    "    plt.yticks(fontsize=12, color=dark_blue, alpha=0.5)\n",
    "\n",
    "    # Add borders to the plot\n",
    "    plt.gca().spines['top'].set_color(dark_blue)\n",
    "    plt.gca().spines['right'].set_color(dark_blue)\n",
    "    plt.gca().spines['bottom'].set_color(dark_blue)\n",
    "    plt.gca().spines['left'].set_color(dark_blue)\n",
    "\n",
    "    # Remove the tick markers (the small lines)\n",
    "    plt.tick_params(axis='x', which='both', length=0)  # Remove x-axis tick markers\n",
    "    plt.tick_params(axis='y', which='both', length=0)  # Remove y-axis tick markers\n",
    "\n",
    "    # Set axis titles\n",
    "    plt.xlabel('Forecast Residual (kW)', fontsize=14, color=dark_blue)\n",
    "    plt.ylabel('Count', fontsize=14, color=dark_blue)\n",
    "\n",
    "    # Remove title\n",
    "    plt.title('')\n",
    "\n",
    "\n",
    "    plt.savefig(pathname, format='png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "    # Show the plot\n",
    "    # plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
