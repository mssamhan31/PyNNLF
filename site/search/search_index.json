{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to PyNNLF","text":"<p>PyNNLF (Python for Network Net Load Forecast) is a tool to evaluate net load forecasting model performance in a reliable and reproducible way.</p> <p>You can access the GitHub repository here.</p>"},{"location":"#objective","title":"Objective","text":"<p>This tool evaluates net load forecasting models reliably and reproducibly. It includes a library of public net load datasets and common forecasting models, including simple benchmark models. Users input the forecast problem and model specification, and the tool outputs evaluation results. </p> <p>It also allows users to add datasets, models, and modify hyperparameters. Researchers claiming a new or superior model can compare their model with existing ones on public datasets. The target audience includes researchers in academia or industry focused on evaluating and optimizing net load forecasting models. </p> <p>A visual illustration of the tool workflow is shown below. </p>"},{"location":"#input","title":"Input","text":"<ol> <li>Forecast Target: dataset &amp; forecast horizon. List of possible forecast problem values is in <code>notebooks/config/config.ipynb</code>.</li> <li>Model Specification: model &amp; hyperparameters. List of possible model specifications is in <code>notebooks/config/model_hyperparameters.ipynb</code>.</li> </ol>"},{"location":"#output","title":"Output","text":"<ol> <li><code>a1_experiment_result.csv</code> \u2013 contains accuracy (cross-validated test n-RMSE), stability (accuracy stddev), and training time.</li> <li><code>a2_hyperparameter.csv</code> \u2013 lists hyperparameters used for each model.</li> <li><code>a3_cross_validation_result.csv</code> \u2013 detailed results for each cross-validation split.</li> <li><code>cv_plots/</code> \u2013 folder with plots including:</li> <li>Observation vs forecast (time plot)</li> <li>Observation vs forecast (scatter plot)</li> <li>Residual time plot</li> <li>Residual histogram</li> <li><code>cv_test/</code> and <code>cv_train/</code> \u2013 folders containing time series of observation, forecast, and residuals for each cross-validation split.</li> </ol>"},{"location":"#tool-output-naming-convention","title":"Tool Output Naming Convention","text":"<p>Format: <code>[experiment_no]_[experiment_date]_[dataset]_[forecast_horizon]_[model]_[hyperparameter]</code></p> <p>Example: <code>E00001_250915_ds0_fh30_m6_lr_hp1</code></p>"},{"location":"ack_license/","title":"Acknowledgements","text":"<p>This project is part of Samhan's PhD study, supported by the University International Postgraduate Award (UIPA) Scholarship from UNSW, the Industry Collaboration Project Scholarship from Ausgrid, and the RACE for 2030 Industry PhD Scholarship. We also acknowledge Solcast and the Australian Bureau of Meteorology (BOM) for providing access to historical weather datasets for this research. We further acknowledge the use of Python libraries including Pandas, NumPy, PyTorch, Scikit-learn, XGBoost, Prophet, Statsmodels, and Matplotlib. Finally, we thank the reviewers and editor of the Journal of Open Source Software for their valuable feedback and guidance.</p>"},{"location":"ack_license/#potential-conflict-disclosure","title":"Potential Conflict Disclosure","text":"<p>The authors declare that they have no competing financial, personal, or professional interests related to this work.</p>"},{"location":"ack_license/#license","title":"License","text":"<p>MIT License.</p>"},{"location":"add_dataset/","title":"How to add a dataset","text":""},{"location":"add_dataset/#dataset-format","title":"Dataset Format","text":"<p>All datasets are stored in the <code>data/</code> folder in <code>.csv</code> format. Each file is named using the pattern <code>[dataset_id]_[dataset_name].csv</code>, e.g., <code>ds4_ashd_with_weather.csv</code>.</p> <p>Some datasets may share the same net load data but differ in the availability of exogenous variables. For instance, <code>ds1_ashd.csv</code> is equivalent to <code>ds4_ashd_with_weather.csv</code> but without weather data.</p> <p>Each CSV file must include two required columns: <code>datetime</code> and <code>netload_kW</code>. PyNNLF uses <code>netload_kW</code> as the target variable for forecasting, and automatically generates lag features based on it.</p> <p>Any additional columns are treated as exogenous variables. These are also processed into lag features based on the forecast horizon, but are not used as targets.</p> <p>Calendar features are excluded from the CSV files, as PyNNLF generates them dynamically during each experiment.</p>"},{"location":"add_dataset/#how-to-add-a-dataset","title":"How to Add a Dataset","text":"<p>To add a new dataset, simply create a <code>.csv</code> file in the <code>data/</code> folder following the naming convention above. Make sure to update <code>data/metadata.xlsx</code> to document the new dataset.</p>"},{"location":"add_model/","title":"How to add a model","text":""},{"location":"add_model/#model-structure","title":"Model Structure","text":"<p>All models are stored in the <code>model</code> folder. Each model has a dedicated <code>.ipynb</code> file, making it easy to navigate and understand its implementation.</p> <p>Each model typically includes two functions: one for training and one for generating forecasts. Tasks such as train-test splitting, 10-fold cross-validation, evaluation, and plotting are handled in <code>general_functions.ipynb</code>, keeping model files focused solely on model-specific code.</p>"},{"location":"add_model/#training-function","title":"Training Function","text":"<p>The training function trains the model using the training set predictors and target values. It returns a single object, <code>model</code>, which contains the trained model.</p>"},{"location":"add_model/#testing-function","title":"Testing Function","text":"<p>The testing function uses the trained model along with the training and testing predictors. It returns two DataFrames: forecasted values for both the training and testing sets.</p> <p>Evaluation and plotting are managed by <code>notebooks/config/general_functions.ipynb</code>.</p>"},{"location":"add_model/#how-to-add-a-model","title":"How to Add a Model","text":"<p>To add a new model, follow these three steps:</p>"},{"location":"add_model/#1-create-a-new-model-file-in-the-notebooksmodel-folder","title":"1. Create a new model file in the <code>notebooks/model/</code> folder","text":"<p>For example, to add a model named <code>new_model</code>, create a file called <code>m19_new_model.ipynb</code>. Define the training and testing functions in this file, named <code>train_model_m19_new_model</code> and <code>produce_forecast_model_m19_new_model</code>.</p> <p>You can refer to existing models for examples, such as the ANN model:</p> <pre><code>def train_model_m7_ann(hyperparameter, train_df_X, train_df_y):\n    ''' Train and test a linear model for point forecasting. \n\n    Args:\n        hyperparameter (df) : hyperparameter value of the model consisting of number of features\n        train_df_X (df) : features matrix for training\n        train_df_y (df) : target matrix for training\n\n\n    Returns:\n        model (model) : trained model with all features\n    '''\n\n    #UNPACK HYPERPARAMETER\n\n    # Set random seed for reproducibility\n    def set_seed(seed):\n        random.seed(seed)\n        os.environ[\"PYTHONHASHSEED\"] = str(seed)\n        np.random.seed(seed)\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed(seed)\n        torch.backends.cudnn.deterministic = True\n\n    seed = int(hyperparameter['seed'])\n\n    hidden_size = hyperparameter['hidden_size']\n    activation_function = hyperparameter['activation_function']\n    learning_rate = hyperparameter['learning_rate']\n    # learning_rate = 0.001\n    solver = hyperparameter['solver']\n    epochs = hyperparameter['epochs']\n\n    # Use proper format for X and y\n    X = torch.tensor(train_df_X.values, dtype=torch.float32)\n    y = torch.tensor(train_df_y.values, dtype=torch.float32).view(-1, 1) \n\n    # Define the ANN model\n    class ANNModel(nn.Module):\n        def __init__(self, input_size, hidden_size, output_size):\n            super(ANNModel, self).__init__()\n            self.fc1 = nn.Linear(input_size, hidden_size)\n            self.fc2 = nn.Linear(hidden_size, output_size)\n            self.relu = nn.ReLU()  # Activation function\n\n        def forward(self, x):\n            x = self.fc1(x)\n            if activation_function == 'relu':\n                x = self.relu(x)\n            elif activation_function == 'sigmoid':\n                x = torch.sigmoid(x)\n            else:\n                x = torch.tanh(x)\n            x = self.fc2(x)\n            return x\n\n    # Model initialization\n    input_size = X.shape[1]\n    output_size = y.shape[1]\n\n    set_seed(seed)\n\n    model_ann = ANNModel(input_size, hidden_size, output_size)\n    if solver == 'adam':\n        optimizer = optim.Adam(model_ann.parameters(), lr=learning_rate)\n    elif solver == 'sgd':\n        optimizer = optim.SGD(model_ann.parameters(), lr=learning_rate)\n    else:\n        raise ValueError('Solver not found')\n\n    # Loss function\n    criterion = nn.MSELoss()  # Mean Squared Error loss for regression\n\n    #TRAIN MODEL\n    # Training loop\n    for epoch in range(epochs):\n        model_ann.train()\n\n        # Forward pass\n        output = model_ann(X)\n        loss = criterion(output, y)\n\n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n\n        # Update weights\n        optimizer.step()\n\n        if epoch % 10 == 0:\n            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n\n    # PACK MODEL\n    model = {\"model_ann\": model_ann}\n\n\n    return model\n</code></pre> <pre><code>def produce_forecast_m7_ann(model, train_df_X, test_df_X):\n    \"\"\"Create forecast at the train and test set using the trained model\n\n    Args:\n        model (dictionary): all parameters of the trained model\n        train_df_X (df): predictors of train set\n        test_df_X (df): predictors of test set\n\n    Returns:\n        train_df_y_hat (df) : forecast result at train set\n        test_df_y_hat (df) : forecast result at test set\n\n    \"\"\"\n\n    # UNPACK MODEL\n    model_ann = model[\"model_ann\"]\n\n    # PREPARE FORMAT\n    train_df_X_tensor = torch.tensor(train_df_X.values, dtype=torch.float32)\n    test_df_X_tensor = torch.tensor(test_df_X.values, dtype=torch.float32)\n\n    # PRODUCE FORECAST\n    # Switch model to evaluation mode for inference\n    model_ann.eval()\n\n    # TRAIN SET FORECAST\n    with torch.no_grad():  # Disable gradient calculation to save memory\n        train_df_y_hat_tensor = model_ann(train_df_X_tensor)\n\n    # TEST SET FORECAST\n    with torch.no_grad():  # Disable gradient calculation to save memory\n        test_df_y_hat_tensor = model_ann(test_df_X_tensor)\n\n    # Create DataFrames of result\n    train_df_y_hat = pd.DataFrame(train_df_y_hat_tensor, index=train_df_X.index, columns=['y_hat'])\n    test_df_y_hat = pd.DataFrame(test_df_y_hat_tensor, index=test_df_X.index, columns=['y_hat'])\n\n    return train_df_y_hat, test_df_y_hat\n</code></pre> <p>You can add any model you\u2019ve developed or proposed, as long as it can be trained using the training set features and target values. The next steps are straightforward.</p>"},{"location":"add_model/#2-update-train_model-function-in-notebooksconfiggeneral_functionsipynb-file","title":"2. Update <code>train_model</code> function in <code>notebooks/config/general_functions.ipynb</code> file","text":"<p>This file contains utility functions, including train_model, which dispatches training based on the selected model.</p> <p>Add a new condition like:</p> <pre><code>elif model_name == 'm19_new_model':\n        model = train_model_m19_new_model(hyperparameter, train_df_X, train_df_y)\n</code></pre>"},{"location":"add_model/#3-update-produce_forecast-function-in-notebooksconfiggeneral_functionsipynb-file","title":"3. Update <code>produce_forecast</code> function in <code>notebooks/config/general_functions.ipynb</code> file","text":"<p>Similarly, update the <code>produce_forecast</code> function by adding:</p> <pre><code>elif model_name == 'm18_nbeats':\n        train_df_y_hat, test_df_y_hat = produce_forecast_m19_new_model(model, train_df_X, test_df_X)\n</code></pre>"},{"location":"api_reference/","title":"API Reference","text":""},{"location":"api_reference/#folder-config","title":"Folder <code>Config/</code>","text":""},{"location":"api_reference/#docs.notebooks.config.general_functions.add_lag_features","title":"<code>add_lag_features(df, forecast_horizon, max_lag_day)</code>","text":"<p>Adds a lagged column to the dataframe based on the given horizon in minutes and max lag in days.</p> <p>Args: df (pd.DataFrame): The input dataframe with a datetime index and a column 'y'. forecast_horizon (int): The horizon in minutes for the lag. max_lag_day (int): the number of days until the longest lag</p> <p>Returns: pd.DataFrame: The dataframe with additional columns for the lags.</p> Source code in <code>docs\\notebooks\\config\\general_functions.py</code> <pre><code>def add_lag_features(df, forecast_horizon, max_lag_day):\n    \"\"\"\n    Adds a lagged column to the dataframe based on the given horizon in minutes and max lag in days.\n\n    Args:\n    df (pd.DataFrame): The input dataframe with a datetime index and a column 'y'.\n    forecast_horizon (int): The horizon in minutes for the lag.\n    max_lag_day (int): the number of days until the longest lag\n\n    Returns:\n    pd.DataFrame: The dataframe with additional columns for the lags.\n    \"\"\"\n\n    # Convert the horizon to a timedelta object\n    horizon_timedelta = pd.Timedelta(minutes=forecast_horizon)\n    consecutive_timedelta = df.index[1] - df.index[0]\n\n    # Calculate the number of new columns\n    n_new_cols = len(df[df.index &lt; df.index[0] + pd.DateOffset(days=max_lag_day)])\n\n    # List to hold all the new lagged columns\n    new_cols = []\n\n    # Generate lagged columns based on the horizon and max lag\n\n    #Generate lagged columns not only based on net load but also based on weather data if available\n    for column in df.columns:\n    # Generate lagged columns for the current column\n        for i in range(n_new_cols):\n            shift_timedelta = horizon_timedelta + i * consecutive_timedelta\n            new_col_name = f'{column}_lag_{shift_timedelta}m'\n            new_cols.append(df[column].shift(freq=shift_timedelta).rename(new_col_name))\n\n\n    # Concatenate the new lagged columns with the original dataframe\n    df = pd.concat([df] + new_cols, axis=1)\n\n    df.dropna(inplace=True)\n\n    return df\n</code></pre>"},{"location":"api_reference/#docs.notebooks.config.general_functions.compute_MAE","title":"<code>compute_MAE(forecast, observation)</code>","text":"<p>As the name suggest.</p> <p>Parameters:</p> Name Type Description Default <code>forecast</code> <code>df</code> <p>series of the forecast result from the model</p> required <code>observation</code> <code>df</code> <p>series of the observed value (actual value)</p> required <p>Returns:</p> Type Description <p>error as the name suggest (float): as the name suggest</p> Source code in <code>docs\\notebooks\\config\\general_functions.py</code> <pre><code>def compute_MAE(forecast, observation):\n    \"\"\"As the name suggest.\n\n    Args:\n        forecast (df): series of the forecast result from the model\n        observation (df): series of the observed value (actual value)\n\n    Returns:\n        error as the name suggest (float): as the name suggest\n    \"\"\"\n    return round((abs(forecast - observation)).mean(), 3)\n</code></pre>"},{"location":"api_reference/#docs.notebooks.config.general_functions.compute_MAPE","title":"<code>compute_MAPE(forecast, observation)</code>","text":"<p>As the name suggest. Be careful with MAPE though because its value can go to inf since the observed value can be 0. </p> <p>Parameters:</p> Name Type Description Default <code>forecast</code> <code>df</code> <p>series of the forecast result from the model</p> required <code>observation</code> <code>df</code> <p>series of the observed value (actual value)</p> required <p>Returns:</p> Type Description <p>error as the name suggest (float): as the name suggest</p> Source code in <code>docs\\notebooks\\config\\general_functions.py</code> <pre><code>def compute_MAPE(forecast, observation):\n    \"\"\"As the name suggest. Be careful with MAPE though because its value can go to inf since the observed value can be 0. \n\n    Args:\n        forecast (df): series of the forecast result from the model\n        observation (df): series of the observed value (actual value)\n\n    Returns:\n        error as the name suggest (float): as the name suggest\n    \"\"\"\n    return round((abs((forecast - observation) / observation) * 100).mean(), 3)\n</code></pre>"},{"location":"api_reference/#docs.notebooks.config.general_functions.compute_MASE","title":"<code>compute_MASE(forecast, observation, train_result)</code>","text":"<p>As the name suggest. MASE is first introduced by Rob Hyndman, used to handle MAPE problem being infinity.  Instead of using observed value as denominator, MASE uses MAE of the naive forecast at the train set for denominator. </p> <p>Parameters:</p> Name Type Description Default <code>forecast</code> <code>df</code> <p>series of the forecast result from the model</p> required <code>observation</code> <code>df</code> <p>series of the observed value (actual value)</p> required <p>Returns:</p> Type Description <p>error as the name suggest (float): as the name suggest</p> Source code in <code>docs\\notebooks\\config\\general_functions.py</code> <pre><code>def compute_MASE(forecast, observation, train_result):\n    \"\"\"As the name suggest. MASE is first introduced by Rob Hyndman, used to handle MAPE problem being infinity. \n    Instead of using observed value as denominator,\n    MASE uses MAE of the naive forecast at the train set for denominator. \n\n    Args:\n        forecast (df): series of the forecast result from the model\n        observation (df): series of the observed value (actual value)\n\n    Returns:\n        error as the name suggest (float): as the name suggest\n    \"\"\"\n    errors = abs(forecast - observation)\n    MAE_naive = compute_MAE(train_result['naive'], train_result['observation'])\n\n    MASE = errors.mean() / MAE_naive\n    return round(MASE, 3)\n</code></pre>"},{"location":"api_reference/#docs.notebooks.config.general_functions.compute_MBE","title":"<code>compute_MBE(forecast, observation)</code>","text":"<p>As the name suggest.</p> <p>Parameters:</p> Name Type Description Default <code>forecast</code> <code>df</code> <p>series of the forecast result from the model</p> required <code>observation</code> <code>df</code> <p>series of the observed value (actual value)</p> required <p>Returns:</p> Type Description <p>error as the name suggest (float): as the name suggest</p> Source code in <code>docs\\notebooks\\config\\general_functions.py</code> <pre><code>def compute_MBE(forecast, observation):\n    \"\"\"As the name suggest.\n\n    Args:\n        forecast (df): series of the forecast result from the model\n        observation (df): series of the observed value (actual value)\n\n    Returns:\n        error as the name suggest (float): as the name suggest\n    \"\"\"\n    return round(((forecast - observation).sum()) / len(observation), 5)\n</code></pre>"},{"location":"api_reference/#docs.notebooks.config.general_functions.compute_R2","title":"<code>compute_R2(forecast, observation)</code>","text":"<p>As the name suggest. Be careful with R2 though because it is not a forecast evaluation.  It is just used to show linearity on the scatter plot of forecast and observed value. </p> <p>Parameters:</p> Name Type Description Default <code>forecast</code> <code>df</code> <p>series of the forecast result from the model</p> required <code>observation</code> <code>df</code> <p>series of the observed value (actual value)</p> required <p>Returns:</p> Type Description <p>error as the name suggest (float): as the name suggest</p> Source code in <code>docs\\notebooks\\config\\general_functions.py</code> <pre><code>def compute_R2(forecast, observation):\n    \"\"\"As the name suggest. Be careful with R2 though because it is not a forecast evaluation. \n    It is just used to show linearity on the scatter plot of forecast and observed value. \n\n    Args:\n        forecast (df): series of the forecast result from the model\n        observation (df): series of the observed value (actual value)\n\n    Returns:\n        error as the name suggest (float): as the name suggest\n    \"\"\"\n    return round(forecast.corr(observation)**2, 3)\n</code></pre>"},{"location":"api_reference/#docs.notebooks.config.general_functions.compute_RMSE","title":"<code>compute_RMSE(forecast, observation)</code>","text":"<p>As the name suggest.</p> <p>Parameters:</p> Name Type Description Default <code>forecast</code> <code>df</code> <p>series of the forecast result from the model</p> required <code>observation</code> <code>df</code> <p>series of the observed value (actual value)</p> required <p>Returns:</p> Type Description <p>error as the name suggest (float): as the name suggest</p> Source code in <code>docs\\notebooks\\config\\general_functions.py</code> <pre><code>def compute_RMSE(forecast, observation):\n    \"\"\"As the name suggest.\n\n    Args:\n        forecast (df): series of the forecast result from the model\n        observation (df): series of the observed value (actual value)\n\n    Returns:\n        error as the name suggest (float): as the name suggest\n    \"\"\"\n    return round(np.sqrt(((forecast - observation) ** 2).mean()), 3)\n</code></pre>"},{"location":"api_reference/#docs.notebooks.config.general_functions.compute_exp_no","title":"<code>compute_exp_no(path_result)</code>","text":"<p>Compute experiment number for folder &amp; file naming based on the number of existing experiments that have been done. For example, if on the folder there are already 5 experiment folders, then the new experiment no. is E00006.</p> <p>Parameters:</p> Name Type Description Default <code>path_result</code> <code>str</code> <p>relative path of experiment folder, stored in config</p> required <p>Returns:</p> Name Type Description <code>int</code> <p>exp no </p> <code>str</code> <p>exp no in str</p> Source code in <code>docs\\notebooks\\config\\general_functions.py</code> <pre><code>def compute_exp_no(path_result):\n    \"\"\"Compute experiment number for folder &amp; file naming based on the number of existing experiments that have been done.\n    For example, if on the folder there are already 5 experiment folders, then the new experiment no. is E00006.\n\n    Args:\n        path_result (str): relative path of experiment folder, stored in config\n\n    Returns:\n        int: exp no \n        str: exp no in str\n    \"\"\"\n    subfolders = os.listdir(path_result)\n    number_of_folders = len(subfolders)\n    experiment_no = number_of_folders - 1 # there is one archive folder\n    experiment_no_str = f\"E{str(experiment_no).zfill(5)}\"\n\n    return experiment_no, experiment_no_str\n</code></pre>"},{"location":"api_reference/#docs.notebooks.config.general_functions.compute_folder_name","title":"<code>compute_folder_name(experiment_no_str, forecast_horizon, model_name, hyperparameter_no)</code>","text":"<p>Folder name in the format of [exp number][exp date][dataset][forecast horizon][model]_[hyperparameter]</p> <p>Parameters:</p> Name Type Description Default <code>experiment_no_str</code> <code>str</code> <p>exp number</p> required <code>forecast_horizon</code> <code>int</code> <p>forecast horizon in minutes</p> required <code>model_name</code> <code>str</code> <p>for example, m1_naive</p> required <code>hyperparameter_no</code> <code>str</code> <p>for example, hp1</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>folder name</p> Source code in <code>docs\\notebooks\\config\\general_functions.py</code> <pre><code>def compute_folder_name(experiment_no_str, forecast_horizon, model_name, hyperparameter_no):\n    \"\"\"Folder name in the format of [exp number]_[exp date]_[dataset]_[forecast horizon]_[model]_[hyperparameter]\n\n    Args:\n        experiment_no_str (str): exp number\n        forecast_horizon (int): forecast horizon in minutes\n        model_name (str): for example, m1_naive\n        hyperparameter_no (str): for example, hp1\n\n    Returns:\n        str: folder name\n    \"\"\"\n    folder_name = \\\n        experiment_no_str + '_' +\\\n        datetime.today().date().strftime(\"%y%m%d\") + '_' +\\\n        dataset.split('_')[0] + '_' +\\\n        'fh' + str(forecast_horizon) + '_' +\\\n        model_name + '_' +\\\n        hyperparameter_no\n    return folder_name\n</code></pre>"},{"location":"api_reference/#docs.notebooks.config.general_functions.compute_fskill","title":"<code>compute_fskill(forecast, observation, naive)</code>","text":"<p>As the name suggest. Forecast Skill is a relative measure seeing the improvement  of the model performance over naive model. </p> <p>Parameters:</p> Name Type Description Default <code>forecast</code> <code>df</code> <p>series of the forecast result from the model</p> required <code>observation</code> <code>df</code> <p>series of the observed value (actual value)</p> required <p>Returns:</p> Type Description <p>error as the name suggest (float): as the name suggest</p> Source code in <code>docs\\notebooks\\config\\general_functions.py</code> <pre><code>def compute_fskill(forecast, observation, naive):\n    \"\"\"As the name suggest. Forecast Skill is a relative measure seeing the improvement \n    of the model performance over naive model. \n\n    Args:\n        forecast (df): series of the forecast result from the model\n        observation (df): series of the observed value (actual value)\n\n    Returns:\n        error as the name suggest (float): as the name suggest\n    \"\"\"\n    return round((1 - compute_RMSE(forecast, observation) / compute_RMSE(naive, observation)) * 100, 3)\n</code></pre>"},{"location":"api_reference/#docs.notebooks.config.general_functions.export_result","title":"<code>export_result(filepath, df_a1_result, cross_val_result_df, hyperparameter)</code>","text":"<p>Export experiment summary: 1. experiment result 2. hyperparameter 3. cross validation detailed result</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>dict</code> <p>dictionary of filepaths for exporting result</p> required Source code in <code>docs\\notebooks\\config\\general_functions.py</code> <pre><code>def export_result(filepath, df_a1_result, cross_val_result_df, hyperparameter):\n    \"\"\"Export experiment summary:\n    1. experiment result\n    2. hyperparameter\n    3. cross validation detailed result\n\n    Args:\n        filepath (dict): dictionary of filepaths for exporting result\n    \"\"\"\n    # Create a df of hyperparameter being used\n    df_a2 = pd.DataFrame(hyperparameter)\n\n    # EXPORT IT\n    df_a1_result.to_csv(filepath['a1'], index=False)\n    df_a2.to_csv(filepath['a2'])\n    cross_val_result_df.to_csv(filepath['a3'])\n</code></pre>"},{"location":"api_reference/#docs.notebooks.config.general_functions.histogram_residual","title":"<code>histogram_residual(residual, df, pathname)</code>","text":"<p>Produce histogiram of residual value and save it on the designated folder</p> <p>Parameters:</p> Name Type Description Default <code>residual</code> <code>df</code> <p>forecast - observation</p> required <code>pathname</code> <code>str</code> <p>filepath to save the figure</p> required Source code in <code>docs\\notebooks\\config\\general_functions.py</code> <pre><code>def histogram_residual(residual, df, pathname):\n    \"\"\"Produce histogiram of residual value and save it on the designated folder\n\n    Args:\n        residual (df): forecast - observation\n        pathname (str): filepath to save the figure\n    \"\"\"\n    # Create the figure with specified size\n    plt.figure(figsize=(9, 9))\n\n    # Set background color\n    # plt.gcf().patch.set_facecolor(platinum)\n\n    # Compute the range\n    dataset_range = df['y'].max() - df['y'].min()\n    bin_min = -dataset_range/7\n    bin_max = dataset_range/7\n\n    # Plot the actual and forecast data\n    plt.hist(residual, bins=31, range=(bin_min, bin_max), color=dark_blue, edgecolor=dark_blue, alpha=0.7)\n\n\n    # Set the font to Arial\n    plt.rcParams['font.family'] = 'Arial'\n\n    # Remove grid lines\n    plt.grid(False)\n\n    # Set tick marks for x and y axis\n    plt.xticks(fontsize=12, color=dark_blue, alpha=0.5, rotation=0)\n    plt.yticks(fontsize=12, color=dark_blue, alpha=0.5)\n\n    # Add borders to the plot\n    plt.gca().spines['top'].set_color(dark_blue)\n    plt.gca().spines['right'].set_color(dark_blue)\n    plt.gca().spines['bottom'].set_color(dark_blue)\n    plt.gca().spines['left'].set_color(dark_blue)\n\n    # Remove the tick markers (the small lines)\n    plt.tick_params(axis='x', which='both', length=0)  # Remove x-axis tick markers\n    plt.tick_params(axis='y', which='both', length=0)  # Remove y-axis tick markers\n\n    # Set axis titles\n    plt.xlabel('Forecast Residual (kW)', fontsize=14, color=dark_blue)\n    plt.ylabel('Count', fontsize=14, color=dark_blue)\n\n    # Remove title\n    plt.title('')\n\n\n    plt.savefig(pathname, format='png', bbox_inches='tight')\n    plt.close()\n</code></pre>"},{"location":"api_reference/#docs.notebooks.config.general_functions.input_and_process","title":"<code>input_and_process(path_data_cleaned, forecast_horizon, max_lag_day, n_block, hyperparameter)</code>","text":"<p>read dataset, add calendar features, add lag features (which depends on the forecast horizon).</p> <p>Parameters:</p> Name Type Description Default <code>path_data_cleaned</code> <code>str</code> <p>path to the dataset chosen</p> required <code>forecast_horizon</code> <code>int</code> <p>forecast horizon in minutes</p> required <code>max_lag_day</code> <code>int</code> <p>how much lag data will be used, written in days. For example, 7 means lag data until d-7 is used. </p> required <code>n_block</code> <code>int</code> <p>number of blocks to divide the original df. This includes the block for hold_out_df, so if k=10, this n_block = k+1 = 11</p> required <code>hyperparameter</code> <code>dict</code> <p>hyperparameters for the model</p> required <p>Returns:</p> Name Type Description <code>block_length</code> <code>int</code> <p>number of weeks per block</p> <code>holdout_df</code> <code>df</code> <p>unused df, can be used later for unbiased estimate of final model performance</p> <code>df</code> <code>df</code> <p>df that will be used for training and validation (test) set</p> Source code in <code>docs\\notebooks\\config\\general_functions.py</code> <pre><code>def input_and_process(path_data_cleaned, forecast_horizon, max_lag_day, n_block, hyperparameter):\n    \"\"\"read dataset, add calendar features, add lag features (which depends on the forecast horizon).\n\n    Args:\n        path_data_cleaned (str): path to the dataset chosen\n        forecast_horizon (int): forecast horizon in minutes\n        max_lag_day (int): how much lag data will be used, written in days. For example, 7 means lag data until d-7 is used. \n        n_block (int): number of blocks to divide the original df. This includes the block for hold_out_df, so if k=10, this n_block = k+1 = 11\n        hyperparameter (dict): hyperparameters for the model\n\n    Returns:\n        block_length (int): number of weeks per block\n        holdout_df (df): unused df, can be used later for unbiased estimate of final model performance\n        df (df): df that will be used for training and validation (test) set\n    \"\"\"\n    # MAKE THIS AS FUNCTION\n    # ADD CALENDAR DATA (holiday to add)\n    # columns_to_use = ['datetime', 'netload_kW']\n    df = pd.read_csv(path_data_cleaned + dataset, index_col=0, parse_dates=True)\n    df.rename(columns={'netload_kW': 'y'}, inplace=True)\n\n    # 1. Check if forecast horizon is &gt;= dataset frequency\n    # for example, if dataset is daily, forecast horizon should be at least 1 day\n    # compute dataset frequency in minutes based on the datetime index\n    dataset_freq = (df.index[1] - df.index[0]).seconds / 60\n    if forecast_horizon &lt; dataset_freq:\n        raise ValueError('Forecast horizon should be &gt;= dataset frequency')\n    else:\n        print('Pass Test 1 - Forecast horizon is &gt;= dataset frequency')\n\n    # 2. Check if hyperparameter choice is possible given the forecast horizon\n    # for example, with forecast horizon of 2 days, we cannot use 1 day as the hyperparameter of seasonal naive forecast.\n\n\n    if model_name == 'm2_snaive':\n        if int(hyperparameter['days'] * 24 * 60) &lt; forecast_horizon:\n            raise ValueError('Choice of seasonal naive hyperparameter needs to be &gt;= forecast horizon! Please change the hyperparameter.')\n    # if model_name == 'm4_sarima':\n    #     if int(hyperparameter['seasonal_period_days'] * 24 * 60) &lt; forecast_horizon:\n    #         raise ValueError('Choice of seasonal_period_days in SARIMA hyperparameter &gt;= forecast horizon! Please change the hyperparameter.')\n    print('Pass Test 2 - Hyperparameter choice is possible given the forecast horizon')\n\n\n# ADD LAG FEATURES\n    df = add_lag_features(df, forecast_horizon, max_lag_day)\n\n# ADD CALENDAR FEATURES    \n    # 1. Numerical representation of the datetime (Excel-style)\n    numeric_datetime = pd.Series((df.index - pd.Timestamp(\"1970-01-01\")) / pd.Timedelta(days=1), index=df.index)\n\n    # 2. Year\n    year = pd.Series(df.index.year, index=df.index)\n\n    # 3. One-hot encoding of month (is_jan, is_feb, ..., is_nov, excluding December)\n    month_dummies = pd.get_dummies(df.index.month, prefix='is', drop_first=False)\n\n    # Custom column names for months: is_jan, is_feb, ..., is_nov\n    month_names = ['is_jan', 'is_feb', 'is_mar', 'is_apr', 'is_may', 'is_jun', \n                'is_jul', 'is_aug', 'is_sep', 'is_oct', 'is_nov', 'is_dec']  \n\n    # Drop the last column (December) to avoid redundancy and rename the columns\n    month_dummies = month_dummies.iloc[:, :-1]  # Exclude December column\n    month_dummies.columns = month_names[:month_dummies.shape[1]]  # Apply custom column names\n    month_dummies = month_dummies.astype(int)  # Convert to 1 and 0\n    month_dummies.index = df.index\n\n    # 4. One-hot encoding of hour (hour_0, hour_1, ..., hour_22, excluding hour_23)\n    hour_dummies = pd.get_dummies(df.index.hour, prefix='hour', drop_first=False).iloc[:, :-1]\n    hour_dummies = hour_dummies.astype(int)  # Convert to 1 and 0\n    hour_dummies.index = df.index\n\n    # 5. One-hot encoding of day of week (is_mon, is_tue, ..., is_sat, excluding Sunday)\n    # Mapping day of week (0=Mon, 1=Tue, ..., 6=Sun)\n    dayofweek_dummies = pd.get_dummies(df.index.dayofweek, prefix='is', drop_first=False).iloc[:, :-1]\n\n    # Custom mapping for days of the week: is_mon, is_tue, ..., is_sat\n    dayofweek_names = ['is_mon', 'is_tue', 'is_wed', 'is_thu', 'is_fri', 'is_sat']  # Custom day names\n    dayofweek_dummies.columns = dayofweek_names[:dayofweek_dummies.shape[1]]  # Apply custom column names\n    dayofweek_dummies = dayofweek_dummies.astype(int)  # Convert to 1 and 0\n    dayofweek_dummies.index = df.index\n\n    # 6. Is weekday (1 if Monday to Friday, 0 if Saturday/Sunday)\n    is_weekday = pd.Series((df.index.dayofweek &lt; 5).astype(int), index=df.index)\n\n\n    # Concatenate all new features into the original dataframe at once\n    df = pd.concat([df, \n                    numeric_datetime.rename('numeric_datetime'), \n                    year.rename('year'),\n                    month_dummies, \n                    hour_dummies, \n                    dayofweek_dummies, \n                    is_weekday.rename('is_weekday')], axis=1)\n\n    block_length, holdout_df, df = separate_holdout(df, n_block)\n\n    return block_length, holdout_df, df\n</code></pre>"},{"location":"api_reference/#docs.notebooks.config.general_functions.prepare_directory","title":"<code>prepare_directory(path_result, forecast_horizon, model_name, hyperparameter_no)</code>","text":"<p>Do two things, 1. Create folders inside the experiment result folder 2. Create some file names to be used when exporting result later</p> <p>Parameters:</p> Name Type Description Default <code>path_result</code> <code>str</code> <p>relative path to the experiment result folder</p> required <code>forecast_horizon</code> <code>int</code> <p>forecast horizon in minutes</p> required <code>model_name</code> <code>str</code> <p>eg m1_naive</p> required <code>hyperparameter_no</code> <code>str</code> <p>eg</p> required <p>Returns:</p> Name Type Description <code>hyperparameter</code> <code>df</code> <p>pd df series of hyperparameter chosen</p> <p>experiment_no_str (str) : experiment number</p> <p>filepath (dict) : dict of all filepaths that will be exported</p> Source code in <code>docs\\notebooks\\config\\general_functions.py</code> <pre><code>def prepare_directory(path_result, forecast_horizon, model_name, hyperparameter_no):\n    \"\"\"Do two things,\n    1. Create folders inside the experiment result folder\n    2. Create some file names to be used when exporting result later\n\n\n    Args:\n        path_result (str): relative path to the experiment result folder\n        forecast_horizon (int): forecast horizon in minutes\n        model_name (str): eg m1_naive\n        hyperparameter_no (str): eg\n\n    Returns:\n        hyperparameter (df): pd df series of hyperparameter chosen\n        experiment_no_str (str) : experiment number\n        filepath (dict) : dict of all filepaths that will be exported\n    \"\"\"\n\n    hyperparameter_table = globals()[f\"{model_name.split('_')[0]}_hp_table\"]\n    hyperparameter = hyperparameter_table.loc[hyperparameter_no]\n\n    experiment_no, experiment_no_str = compute_exp_no(path_result)\n    folder_name = compute_folder_name(experiment_no_str, forecast_horizon, model_name, hyperparameter_no)\n\n    # CREATE FOLDER\n    cv_folder_train = experiment_no_str + '_cv_train'\n    cv_folder_test = experiment_no_str + '_cv_test'\n    cv1_plot_folder = experiment_no_str + '_cv1_plots'\n    folder_model = experiment_no_str + '_models'\n\n    path_result2 = path_result + folder_name +'/'\n    path_result_train = path_result2 + cv_folder_train +'/'\n    path_result_test = path_result2 + cv_folder_test +'/'\n    path_result_plot = path_result2 + cv1_plot_folder +'/'\n    path_model = path_result2 + folder_model +'/'\n\n    # MAKE FOLDERS\n    os.mkdir(path_result2)\n    os.mkdir(path_result_train)\n    os.mkdir(path_result_test)\n    os.mkdir(path_result_plot)\n    os.mkdir(path_model)\n\n    # MAKE FILE PATH\n    filepath = {\n        'a1' : path_result2 + experiment_no_str + '_a1_experiment_result.csv',\n        'a2' : path_result2 + experiment_no_str + '_a2_hyperparameter.csv',\n        'a3' : path_result2 + experiment_no_str + '_a3_cross_validation_result.csv',\n        'b1' : path_result_plot + experiment_no_str + '_b1_train_timeplot.png', # Time Plot of Forecast vs Observation\n        'b2' : path_result_plot + experiment_no_str + '_b2_train_scatterplot.png', # Scatter Plot of Forecast vs Observation\n        'b3' : path_result_plot + experiment_no_str + '_b3_train_residual_timeplot.png', # Time Plot of Residual\n        'b4' : path_result_plot + experiment_no_str + '_b4_train_residual_histogram.png', # Histogram of Residual\n        'b5' : path_result_plot + experiment_no_str + '_b5_train_learningcurve.png', # Learning Curve vs Epoch\n        'c1' : path_result_plot + experiment_no_str + '_c1_test_timeplot.png',  # Time Plot of Forecast vs Observation\n        'c2' : path_result_plot + experiment_no_str + '_c2_test_scatterplot.png',  # Scatter Plot of Forecast vs Observation\n        'c3' : path_result_plot + experiment_no_str + '_c3_test_residual_timeplot.png',  # Time Plot of Residual\n        'c4' : path_result_plot + experiment_no_str + '_c4_test_residual_histogram.png',  # Histogram of Residual\n        'c5' : path_result_plot + experiment_no_str + '_c5_test_learningcurve.png',  # Learning Curve vs Epoch\n\n        # B. FOLDER FOR CROSS VALIDATION TIME SERIES\n        'train_cv' : {\n            1 : path_result_train + experiment_no_str + '_cv1_train_result.csv',\n            2 : path_result_train + experiment_no_str + '_cv2_train_result.csv',\n            3 : path_result_train + experiment_no_str + '_cv3_train_result.csv',\n            4 : path_result_train + experiment_no_str + '_cv4_train_result.csv',\n            5 : path_result_train + experiment_no_str + '_cv5_train_result.csv',\n            6 : path_result_train + experiment_no_str + '_cv6_train_result.csv',\n            7 : path_result_train + experiment_no_str + '_cv7_train_result.csv',\n            8 : path_result_train + experiment_no_str + '_cv8_train_result.csv',\n            9 : path_result_train + experiment_no_str + '_cv9_train_result.csv',\n            10 : path_result_train + experiment_no_str + '_cv10_train_result.csv'\n        },\n\n        'test_cv' : {\n            1 : path_result_test + experiment_no_str + '_cv1_test_result.csv',\n            2 : path_result_test + experiment_no_str + '_cv2_test_result.csv',\n            3 : path_result_test + experiment_no_str + '_cv3_test_result.csv',\n            4 : path_result_test + experiment_no_str + '_cv4_test_result.csv',\n            5 : path_result_test + experiment_no_str + '_cv5_test_result.csv',\n            6 : path_result_test + experiment_no_str + '_cv6_test_result.csv',\n            7 : path_result_test + experiment_no_str + '_cv7_test_result.csv',\n            8 : path_result_test + experiment_no_str + '_cv8_test_result.csv',\n            9 : path_result_test + experiment_no_str + '_cv9_test_result.csv',\n            10 : path_result_test + experiment_no_str + '_cv10_test_result.csv'\n        },\n\n        'model' : {\n            1 : path_model + experiment_no_str + '_cv1_model.pkl',\n            2 : path_model + experiment_no_str + '_cv2_model.pkl',\n            3 : path_model + experiment_no_str + '_cv3_model.pkl',\n            4 : path_model + experiment_no_str + '_cv4_model.pkl',\n            5 : path_model + experiment_no_str + '_cv5_model.pkl',\n            6 : path_model + experiment_no_str + '_cv6_model.pkl',\n            7 : path_model + experiment_no_str + '_cv7_model.pkl',\n            8 : path_model + experiment_no_str + '_cv8_model.pkl',\n            9 : path_model + experiment_no_str + '_cv9_model.pkl',\n            10 : path_model + experiment_no_str + '_cv10_model.pkl'\n        }\n    }\n    return hyperparameter,experiment_no_str, filepath\n</code></pre>"},{"location":"api_reference/#docs.notebooks.config.general_functions.produce_forecast","title":"<code>produce_forecast(model_name, model, train_df_X, test_df_X, train_df_y, forecast_horizon)</code>","text":"<p>Generate forecasts based on the model and its name.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>string</code> <p>Model identifier (e.g., 'm1_naive').</p> required <code>model</code> <code>dict</code> <p>Trained model object containing all relevant features.</p> required <code>train_df_X</code> <code>DataFrame</code> <p>Matrix of predictors for training set.</p> required <code>test_df_X</code> <code>DataFrame</code> <p>Matrix of predictors for test set.</p> required <code>train_df_y</code> <code>DataFrame</code> <p>Target forecast y for training set.</p> required <code>forecast_horizon</code> <code>int</code> <p>Forecast horizon in minutes.</p> required <p>Returns:</p> Name Type Description <code>train_df_y_hat</code> <code>DataFrame</code> <p>Forecasted values for training set.</p> <code>test_df_y_hat</code> <code>DataFrame</code> <p>Forecasted values for test set.</p> Source code in <code>docs\\notebooks\\config\\general_functions.py</code> <pre><code>def produce_forecast(model_name, model, train_df_X, test_df_X, train_df_y, forecast_horizon):\n    \"\"\"Generate forecasts based on the model and its name.\n\n    Args:\n        model_name (string): Model identifier (e.g., 'm1_naive').\n        model (dict): Trained model object containing all relevant features.\n        train_df_X (DataFrame): Matrix of predictors for training set.\n        test_df_X (DataFrame): Matrix of predictors for test set.\n        train_df_y (DataFrame): Target forecast y for training set.\n        forecast_horizon (int): Forecast horizon in minutes.\n\n    Returns:\n        train_df_y_hat (DataFrame): Forecasted values for training set.\n        test_df_y_hat (DataFrame): Forecasted values for test set.\n    \"\"\"\n\n    if model_name == 'm1_naive':\n        train_df_y_hat, test_df_y_hat = produce_forecast_m1_naive(model, train_df_X, test_df_X)\n    elif model_name == 'm2_snaive':\n        train_df_y_hat, test_df_y_hat = produce_forecast_m2_snaive(model, train_df_X, test_df_X)\n    elif model_name == 'm3_ets':\n        train_df_y_hat, test_df_y_hat = produce_forecast_m3_ets(model, train_df_X, test_df_X, forecast_horizon)\n    elif model_name == 'm4_arima':\n        train_df_y_hat, test_df_y_hat = produce_forecast_m4_arima(model, train_df_X, test_df_X, forecast_horizon)\n    elif model_name == 'm5_sarima':\n        train_df_y_hat, test_df_y_hat = produce_forecast_m5_sarima(model, train_df_X, test_df_X, forecast_horizon)\n    elif model_name == 'm6_lr':\n        train_df_y_hat, test_df_y_hat = produce_forecast_m6_lr(model, train_df_X, test_df_X)\n    elif model_name == 'm7_ann':\n        train_df_y_hat, test_df_y_hat = produce_forecast_m7_ann(model, train_df_X, test_df_X)\n    elif model_name == 'm8_dnn':\n        train_df_y_hat, test_df_y_hat = produce_forecast_m8_dnn(model, train_df_X, test_df_X)\n    elif model_name == 'm9_rt':\n        train_df_y_hat, test_df_y_hat = produce_forecast_m9_rt(model, train_df_X, test_df_X)\n    elif model_name == 'm10_rf':\n        train_df_y_hat, test_df_y_hat = produce_forecast_m10_rf(model, train_df_X, test_df_X)\n    elif model_name == 'm11_svr':\n        train_df_y_hat, test_df_y_hat = produce_forecast_m11_svr(model, train_df_X, test_df_X)\n    elif model_name == 'm12_rnn':\n        train_df_y_hat, test_df_y_hat = produce_forecast_m12_rnn(model, train_df_X, test_df_X)\n    elif model_name == 'm13_lstm':\n        train_df_y_hat, test_df_y_hat = produce_forecast_m13_lstm(model, train_df_X, test_df_X)\n    elif model_name == 'm14_gru':\n        train_df_y_hat, test_df_y_hat = produce_forecast_m14_gru(model, train_df_X, test_df_X)\n    elif model_name == 'm15_transformer':\n        train_df_y_hat, test_df_y_hat = produce_forecast_m15_transformer(model, train_df_X, test_df_X)\n    elif model_name == 'm16_prophet':\n        train_df_y_hat, test_df_y_hat = produce_forecast_m16_prophet(model, train_df_X, test_df_X, train_df_y, forecast_horizon)\n    elif model_name == 'm17_xgb':\n        train_df_y_hat, test_df_y_hat = produce_forecast_m17_xgb(model, train_df_X, test_df_X)\n    elif model_name == 'm18_nbeats':\n        train_df_y_hat, test_df_y_hat = produce_forecast_m18_nbeats(model, train_df_X, test_df_X)\n    else:\n        raise ValueError(\n            \"Wrong Model Choice! Available models are: m1_naive, m2_snaive, m3_ets, m4_arima, m5_sarima, m6_lr, m7_ann, m8_dnn, m9_rt, m10_rf, m11_svr, m12_rnn, m13_lstm, m14_gru, m15_transformer, m16_prophet, m17_xgb, m18_nbeats\"\n        )\n\n    return train_df_y_hat, test_df_y_hat\n</code></pre>"},{"location":"api_reference/#docs.notebooks.config.general_functions.remove_jump_df","title":"<code>remove_jump_df(train_df_y)</code>","text":"<p>Remove jump in the time series data Parameters:     train_df_y (pd.Series): Time series data</p> <p>Returns:</p> Name Type Description <code>train_df_y_updated</code> <code>Series</code> <p>Time series data with jump removed</p> Source code in <code>docs\\notebooks\\config\\general_functions.py</code> <pre><code>def remove_jump_df(train_df_y):\n    #make docstring with the same format like other cells\n    \"\"\"\n    Remove jump in the time series data\n    Parameters:\n        train_df_y (pd.Series): Time series data\n\n    Returns:\n        train_df_y_updated (pd.Series): Time series data with jump removed\n    \"\"\"\n\n    time_diff = train_df_y.index.to_series().diff().dt.total_seconds()\n    initial_freq = time_diff.iloc[1]\n    jump_indices = time_diff[time_diff &gt; initial_freq].index\n    if not jump_indices.empty:\n        jump_index = jump_indices[0]\n        jump_pos = train_df_y.index.get_loc(jump_index)\n        train_df_y_updated = train_df_y.iloc[:jump_pos]\n    else:\n        train_df_y_updated = train_df_y\n    return train_df_y_updated\n</code></pre>"},{"location":"api_reference/#docs.notebooks.config.general_functions.run_experiment","title":"<code>run_experiment(dataset, forecast_horizon, model_name, hyperparameter_no)</code>","text":"<p>Run the experiment with the specified parameters. Args:     dataset (str): Name of the dataset file.     forecast_horizon (int): Forecast horizon in minutes.     model_name (str): Model identifier (e.g., 'm6_lr').     hyperparameter_no (str): Hyperparameter set identifier.</p> <p>Returns:</p> Type Description <p>None. Results and models are saved to disk.</p> Source code in <code>docs\\notebooks\\config\\general_functions.py</code> <pre><code>def run_experiment(dataset, forecast_horizon, model_name, hyperparameter_no):\n    '''\n    Run the experiment with the specified parameters.\n    Args:\n        dataset (str): Name of the dataset file.\n        forecast_horizon (int): Forecast horizon in minutes.\n        model_name (str): Model identifier (e.g., 'm6_lr').\n        hyperparameter_no (str): Hyperparameter set identifier.\n\n    Returns:\n        None. Results and models are saved to disk.\n    '''\n    # PREPARE FOLDER\n    hyperparameter, experiment_no_str, filepath = prepare_directory(path_result, forecast_horizon, model_name, hyperparameter_no)\n    # INPUT DATA\n    block_length, holdout_df, df = input_and_process(path_data_cleaned, forecast_horizon, max_lag_day, n_block, hyperparameter)\n    # RUN MODEL\n    run_model(df, model_name, hyperparameter, filepath, forecast_horizon, experiment_no_str, block_length)\n</code></pre>"},{"location":"api_reference/#docs.notebooks.config.general_functions.run_model","title":"<code>run_model(df, model_name, hyperparameter, filepath, forecast_horizon, experiment_no_str, block_length)</code>","text":"<p>Run model! This will be updated so that it can adapt to any model  This consists of 1. Loop over all CV, then inside the loop, 2. Split df to train and test set 3. Train model on train set 4. Produce naive forecast on both trian and test set for benchmark 5. Produce forecast on both train and test set 6. Compute residual on both train and test set 7. Export the observation, naive, forecast, and residual of train and test set 8. Produce plots only on CV 1 9. Evaluate forecast performance on train and test set 10. Quit the loop, 11. Summarise the overall performance of the model using RMSE and its Stddev 12. Export all results     </p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>df</code> <p>df that will be used for training and validation (test) set, consists of X and Y</p> required <code>model_name</code> <code>string</code> <p>eg 'm06_lr'</p> required <code>hyperparameter</code> <code>pd series</code> <p>list of hyperparameter for that model</p> required <code>filepath</code> <code>dict</code> <p>dictionary of filepaths for exporting result</p> required <code>forecast_horizon </code> <p>the forecast horizon</p> required <code>experiment_no </code> <p>the experiment no. in string</p> required <code>block_length </code> <p>block length of one cross validation set</p> required Source code in <code>docs\\notebooks\\config\\general_functions.py</code> <pre><code>def run_model(df, model_name, hyperparameter, filepath, forecast_horizon, experiment_no_str, block_length):\n    \"\"\"Run model! This will be updated so that it can adapt to any model \n    This consists of\n    1. Loop over all CV, then inside the loop,\n    2. Split df to train and test set\n    3. Train model on train set\n    4. Produce naive forecast on both trian and test set for benchmark\n    5. Produce forecast on both train and test set\n    6. Compute residual on both train and test set\n    7. Export the observation, naive, forecast, and residual of train and test set\n    8. Produce plots only on CV 1\n    9. Evaluate forecast performance on train and test set\n    10. Quit the loop,\n    11. Summarise the overall performance of the model using RMSE and its Stddev\n    12. Export all results     \n\n    Args:\n        df (df): df that will be used for training and validation (test) set, consists of X and Y\n        model_name (string): eg 'm06_lr'\n        hyperparameter (pd series): list of hyperparameter for that model\n        filepath (dict): dictionary of filepaths for exporting result\n        forecast_horizon : the forecast horizon\n        experiment_no : the experiment no. in string\n        block_length : block length of one cross validation set\n\n\n    \"\"\"\n\n    import warnings\n    warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\n    cross_val_result_df = pd.DataFrame()\n\n    # Compute max_y for normalization later\n    max_y = df['y'].max()\n\n    # DO CROSS VALIDATION\n    for cv_no in range(1, k+1):\n        print(f'Processing CV {cv_no} / {k}....')\n\n        # SPLIT INTO TRAIN AND TEST X AND Y\n        train_df, test_df = split_time_series(df, cv_no)\n        train_df_X, train_df_y = split_xy(train_df)\n        test_df_X, test_df_y = split_xy(test_df)\n\n        # INITIALISE RESULT DF   \n        train_result = train_df_y.copy()\n        train_result = train_result.rename(columns={'y': 'observation'})\n\n        test_result = test_df_y.copy()\n        test_result = test_result.rename(columns={'y': 'observation'})\n\n        # PRODUCE NAIVE FORECAST\n        horizon_timedelta = pd.Timedelta(minutes=forecast_horizon)\n        last_observation = f'y_lag_{horizon_timedelta}m'\n        train_result['naive'] = train_df[last_observation]\n        test_result['naive'] = test_df[last_observation]\n\n        # TRAIN MODEL\n        start_time = time.time()\n        model = train_model(model_name, hyperparameter, train_df_X, train_df_y, forecast_horizon)\n        save_model(filepath, cv_no, model)\n        end_time = time.time()\n        runtime_ms = (end_time - start_time) * 1000  # Convert to milliseconds\n\n        # PRODUCE FORECAST\n        train_df_y_hat, test_df_y_hat = produce_forecast(model_name, model, train_df_X, test_df_X, train_df_y, forecast_horizon)\n        train_result['forecast'] = train_df_y_hat\n        test_result['forecast'] = test_df_y_hat\n\n        # EVALUATE FORECAST\n        train_result['residual'] = train_result['forecast'] - train_result['observation']\n        test_result['residual'] = test_result['forecast'] - test_result['observation']\n        train_R2 = compute_R2(train_result['forecast'], train_result['observation'])\n        test_R2 = compute_R2(test_result['forecast'], test_result['observation'])\n\n        train_RMSE = compute_RMSE(train_result['forecast'], train_result['observation'])\n        test_RMSE = compute_RMSE(test_result['forecast'], test_result['observation'])\n\n        train_nRMSE = 100*train_RMSE / max_y # in percent\n        test_nRMSE = 100*test_RMSE / max_y # in percent\n\n        cross_val_result = pd.DataFrame(\n        {\n            \"runtime_ms\": runtime_ms,\n            \"train_MBE\": compute_MBE(train_result['forecast'], train_result['observation']), \n            \"train_MAE\": compute_MAE(train_result['forecast'], train_result['observation']),\n            \"train_RMSE\": train_RMSE,\n            \"train_MAPE\": compute_MAPE(train_result['forecast'], train_result['observation']),\n            \"train_MASE\": compute_MASE(train_result['forecast'], train_result['observation'], train_result),\n            \"train_fskill\": compute_fskill(train_result['forecast'], train_result['observation'], train_result['naive']),\n            \"train_R2\": train_R2,\n            \"test_MBE\": compute_MBE(test_result['forecast'], test_result['observation']),\n            \"test_MAE\": compute_MAE(test_result['forecast'], test_result['observation']),\n            \"test_RMSE\": test_RMSE,\n            \"test_MAPE\": compute_MAPE(test_result['forecast'], test_result['observation']),\n            \"test_MASE\": compute_MASE(test_result['forecast'], test_result['observation'], train_result),\n            \"test_fskill\": compute_fskill(test_result['forecast'], test_result['observation'], test_result['naive']),\n            \"test_R2\": test_R2,\n            \"train_nRMSE\": train_nRMSE,\n            \"test_nRMSE\": test_nRMSE\n        }, \n        index=[cv_no]\n        )\n\n        if cross_val_result_df.empty:\n            cross_val_result_df = cross_val_result\n        else:\n            cross_val_result_df = pd.concat([cross_val_result_df, cross_val_result], ignore_index=False)\n        cross_val_result_df.index.name = 'cv_no'\n\n        # EXPORT RESULTS DF TO CSV\n        train_result.to_csv(filepath['train_cv'][cv_no])\n        test_result.to_csv(filepath['test_cv'][cv_no])\n\n        # IF CV_NO = 1, ALSO EXPORT SOME PLOTS\n        if cv_no == 1:\n            timeplot_forecast(train_result['observation'], train_result['forecast'], filepath['b1'])\n            timeplot_forecast(test_result['observation'], test_result['forecast'], filepath['c1'])\n            scatterplot_forecast(train_result['observation'], train_result['forecast'], train_R2, filepath['b2'])\n            scatterplot_forecast(test_result['observation'], test_result['forecast'], test_R2, filepath['c2'])\n            timeplot_residual(train_result['residual'], filepath['b3'])\n            timeplot_residual(test_result['residual'], filepath['c3'])\n            histogram_residual(train_result['residual'], df, filepath['b4'])\n            histogram_residual(test_result['residual'], df, filepath['c4'])\n        print()\n\n\n    cross_val_result = pd.DataFrame(\n        {\n            \"runtime_ms\": [cross_val_result_df['runtime_ms'].mean(), cross_val_result_df['runtime_ms'].std()],\n            \"train_MBE\": [cross_val_result_df['train_MBE'].mean(), cross_val_result_df['train_MBE'].std()], \n            \"train_MAE\": [cross_val_result_df['train_MAE'].mean(), cross_val_result_df['train_MAE'].std()],\n            \"train_RMSE\": [cross_val_result_df['train_RMSE'].mean(), cross_val_result_df['train_RMSE'].std()],\n            \"train_MAPE\": [cross_val_result_df['train_MAPE'].mean(), cross_val_result_df['train_MAPE'].std()],\n            \"train_MASE\": [cross_val_result_df['train_MASE'].mean(), cross_val_result_df['train_MASE'].std()],\n            \"train_fskill\": [cross_val_result_df['train_fskill'].mean(), cross_val_result_df['train_fskill'].std()],\n            \"train_R2\": [cross_val_result_df['train_R2'].mean(), cross_val_result_df['train_R2'].std()],\n            \"test_MBE\": [cross_val_result_df['test_MBE'].mean(), cross_val_result_df['test_MBE'].std()],\n            \"test_MAE\": [cross_val_result_df['test_MAE'].mean(), cross_val_result_df['test_MAE'].std()],\n            \"test_RMSE\": [cross_val_result_df['test_RMSE'].mean(), cross_val_result_df['test_RMSE'].std()],\n            \"test_MAPE\": [cross_val_result_df['test_MAPE'].mean(), cross_val_result_df['test_MAPE'].std()],\n            \"test_MASE\": [cross_val_result_df['test_MASE'].mean(), cross_val_result_df['test_MASE'].std()],\n            \"test_fskill\": [cross_val_result_df['test_fskill'].mean(), cross_val_result_df['test_fskill'].std()],\n            \"test_R2\": [cross_val_result_df['test_R2'].mean(), cross_val_result_df['test_R2'].std()],\n            \"train_nRMSE\": [cross_val_result_df['train_nRMSE'].mean(), cross_val_result_df['train_nRMSE'].std()],\n            \"test_nRMSE\": [cross_val_result_df['test_nRMSE'].mean(), cross_val_result_df['test_nRMSE'].std()]\n        }, \n        index=['mean', 'stddev']\n        )\n\n    cross_val_result_df = pd.concat([cross_val_result_df, cross_val_result], ignore_index=False)\n\n    data_a1 = {\n        \"experiment_no\": experiment_no_str,\n        \"exp_date\": datetime.today().strftime('%Y-%m-%d'), #today date in YYYY-MM-DD format\n        \"dataset_no\": dataset.split('_')[0],\n        \"dataset\": dataset.split('_')[1].split('.')[0],\n        \"dataset_freq_min\": int((df.index[1] - df.index[0]).total_seconds() / 60),\n        \"dataset_length_week\": block_length * (n_block - 1),\n        \"forecast_horizon_min\": forecast_horizon,\n        \"train_pct\": train_pct,\n        \"test_pct\": test_pct,\n        \"model_no\": model_name.split('_')[0],\n        \"hyperparameter_no\": hyperparameter_no,\n        \"model_name\": model_name + '_' + hyperparameter_no,\n        \"hyperparamter\": ', '.join(f\"{k}: {v}\" for k, v in hyperparameter.items()),  \n        \"runtime_ms\": cross_val_result_df.loc['mean', 'runtime_ms'],\n        \"train_RMSE\": cross_val_result_df.loc['mean', 'train_RMSE'],\n        \"train_RMSE_stddev\": cross_val_result_df.loc['stddev', 'train_RMSE'],\n        \"test_RMSE\": cross_val_result_df.loc['mean', 'test_RMSE'],\n        \"test_RMSE_stddev\": cross_val_result_df.loc['stddev', 'test_RMSE'],\n        \"train_nRMSE\": cross_val_result_df.loc['mean', 'train_nRMSE'],\n        \"train_nRMSE_stddev\": cross_val_result_df.loc['stddev', 'train_nRMSE'],\n        \"test_nRMSE\": cross_val_result_df.loc['mean', 'test_nRMSE'],\n        \"test_nRMSE_stddev\": cross_val_result_df.loc['stddev', 'test_nRMSE']\n    }\n\n    # Create a df of experiment result\n    df_a1_result = pd.DataFrame([data_a1])\n\n    export_result(filepath, df_a1_result, cross_val_result_df, hyperparameter)\n</code></pre>"},{"location":"api_reference/#docs.notebooks.config.general_functions.save_model","title":"<code>save_model(filepath, cv_no, model)</code>","text":"<p>Export model into binary file using pickle to a designated file</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>dictionary</code> <p>dictionary of the file path</p> required <code>cv_no (int) </code> <p>cv number</p> required <code>model</code> <code>dictionary</code> <p>trained model</p> required Source code in <code>docs\\notebooks\\config\\general_functions.py</code> <pre><code>def save_model(filepath, cv_no, model):\n    \"\"\"Export model into binary file using pickle to a designated file\n\n    Args:\n        filepath (dictionary): dictionary of the file path\n        cv_no (int) : cv number\n        model (dictionary): trained model\n    \"\"\"\n\n    with open(filepath['model'][cv_no], \"wb\") as model_file:\n        # pickle.dump(model, model_file)\n        dill.dump(model, model_file)\n</code></pre>"},{"location":"api_reference/#docs.notebooks.config.general_functions.scatterplot_forecast","title":"<code>scatterplot_forecast(observation, forecast, R2, pathname)</code>","text":"<p>Produce scatterplot observation vs forecast value and save it on the designated folder</p> <p>Parameters:</p> Name Type Description Default <code>observation</code> <code>df</code> <p>observed value</p> required <code>forecast</code> <code>df</code> <p>forecast value</p> required <code>pathname</code> <code>str</code> <p>filepath to save the figure</p> required Source code in <code>docs\\notebooks\\config\\general_functions.py</code> <pre><code>def scatterplot_forecast(observation, forecast, R2, pathname):\n    \"\"\"Produce scatterplot observation vs forecast value and save it on the designated folder\n\n    Args:\n        observation (df): observed value\n        forecast (df): forecast value\n        pathname (str): filepath to save the figure\n    \"\"\"\n    # Create the figure with specified size\n    plt.figure(figsize=(9, 9))\n\n    # Set background color\n    # plt.gcf().patch.set_facecolor(platinum)\n\n    # Plot the actual and forecast data\n    plt.scatter(forecast, observation, color=dark_blue, label='Actual', s=40, alpha=0.7)  # 's' sets the size of the points\n\n\n    # Set the font to Arial\n    plt.rcParams['font.family'] = 'Arial'\n\n    # Remove grid lines\n    plt.grid(False)\n\n    # Set tick marks for x and y axis\n    plt.xticks(fontsize=12, color=dark_blue, alpha=0.5, rotation=0)\n    plt.yticks(fontsize=12, color=dark_blue, alpha=0.5)\n\n    # Add borders to the plot\n    plt.gca().spines['top'].set_color(dark_blue)\n    plt.gca().spines['right'].set_color(dark_blue)\n    plt.gca().spines['bottom'].set_color(dark_blue)\n    plt.gca().spines['left'].set_color(dark_blue)\n\n    # Remove the tick markers (the small lines)\n    plt.tick_params(axis='x', which='both', length=0)  # Remove x-axis tick markers\n    plt.tick_params(axis='y', which='both', length=0)  # Remove y-axis tick markers\n\n    # Set axis titles\n    plt.xlabel('Net Load Forecast (kW)', fontsize=14, color=dark_blue)\n    plt.ylabel('Net Load Observation (kW)', fontsize=14, color=dark_blue)\n\n    # Remove title\n    plt.title('')\n\n    # Add R\u00b2 value at the top-left corner\n    plt.text(0.95, 0.05, f'R\u00b2 = {R2:.3f}', transform=plt.gca().transAxes, \n         fontsize=14, color=dark_blue, verticalalignment='bottom', horizontalalignment='right',\n         bbox=dict(facecolor='white', edgecolor=dark_blue, boxstyle='round,pad=0.5', linewidth=1))\n\n\n    plt.savefig(pathname, format='png', bbox_inches='tight')\n    plt.close()\n</code></pre>"},{"location":"api_reference/#docs.notebooks.config.general_functions.separate_holdout","title":"<code>separate_holdout(df, n_block)</code>","text":"<p>Separating df into two parts: 1. df : df that will be used for training and blocked k-fold cross validation.  The block is a multiple of a week because net load data has weekly seasonality 2. hold_out_df : this section is not used for now, but can be useful for final test of the chosen model if wanted, to show the generalized error. This is at least 1 block of data. </p> <p>By default, the chosen k for k-fold cross validation is 10.</p> <p>For example, the original df has 12 weeks worth of data.  In this case, new df is week 1-10, hold_out_df is week 11-12,</p> <p>the new df will be used for cross validation, for example CV1: training: week 1-9, validation (test) week 10 CV2: training: week 1-8, week 10, validation (test) week 9, etc. </p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>df</code> <p>cleaned df consisting of y and all predictors</p> required <code>n_block</code> <code>int</code> <p>number of blocks to divide the original df. This includes the block for hold_out_df, so if k=10, this n_block = k+1 = 11</p> required <p>Returns:</p> Type Description <p>block_length (int) : number of weeks per block</p> <p>hodout_df (df) : unused df, can be used later for unbiased estimate of final model performance</p> <p>df (df) : df that will be used for training and validation (test) set</p> Source code in <code>docs\\notebooks\\config\\general_functions.py</code> <pre><code>def separate_holdout(df, n_block):    \n    \"\"\"Separating df into two parts:\n    1. df : df that will be used for training and blocked k-fold cross validation. \n    The block is a multiple of a week because net load data has weekly seasonality\n    2. hold_out_df : this section is not used for now, but can be useful for final test of the chosen model\n    if wanted, to show the generalized error. This is at least 1 block of data. \n\n    By default, the chosen k for k-fold cross validation is 10.\n\n    For example, the original df has 12 weeks worth of data. \n    In this case,\n    new df is week 1-10,\n    hold_out_df is week 11-12,\n\n    the new df will be used for cross validation, for example\n    CV1: training: week 1-9, validation (test) week 10\n    CV2: training: week 1-8, week 10, validation (test) week 9,\n    etc. \n\n    Args:\n        df (df): cleaned df consisting of y and all predictors\n        n_block (int): number of blocks to divide the original df. This includes the block for hold_out_df, so if k=10, this n_block = k+1 = 11\n\n    Returns:\n        block_length (int) : number of weeks per block\n        hodout_df (df) : unused df, can be used later for unbiased estimate of final model performance\n        df (df) : df that will be used for training and validation (test) set\n    \"\"\"\n\n    dataset_length_week= ((df.index[-1] - df.index[0]).total_seconds() / 86400/7)\n    block_length = int(dataset_length_week / n_block)\n    consecutive_timedelta = df.index[1] - df.index[0]\n    n_timestep_per_week = int(one_week / consecutive_timedelta)\n    holdout_start = (n_block - 1)* block_length * n_timestep_per_week\n    holdout_df = df.iloc[holdout_start:]\n    df = df.drop(df.index[holdout_start:])\n\n    return block_length, holdout_df, df\n</code></pre>"},{"location":"api_reference/#docs.notebooks.config.general_functions.split_time_series","title":"<code>split_time_series(df, cv_no)</code>","text":"<p>Split df to train and test set using blocked cross validation.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>df</code> <p>df that will be used for training and validation (test) set, consists of X and Y</p> required <code>cv_no</code> <code>int</code> <p>number of current cv order.           cv_no=1 means the test set is at the last, cv_no = k means the test set is at the beginning</p> required <p>Returns:</p> Type Description <p>train_df (df) : df used for training</p> <p>test_df (df) : df used for validation, formal name is validation set / dev set.</p> Source code in <code>docs\\notebooks\\config\\general_functions.py</code> <pre><code>def split_time_series(df, cv_no):\n    \"\"\"Split df to train and test set using blocked cross validation.\n\n    Args:\n        df (df): df that will be used for training and validation (test) set, consists of X and Y\n        cv_no (int): number of current cv order. \n                     cv_no=1 means the test set is at the last, cv_no = k means the test set is at the beginning\n\n    Returns:\n        train_df (df) : df used for training\n        test_df (df) : df used for validation, formal name is validation set / dev set. \n    \"\"\"\n\n    n = len(df)\n    test_start = int(n*(1 - cv_no*test_pct))\n    test_end = int(n*(1 - (cv_no-1)*test_pct))\n\n    test_df = df.iloc[test_start:test_end]\n    train_df = df.drop(df.index[test_start:test_end])\n\n    return train_df, test_df\n</code></pre>"},{"location":"api_reference/#docs.notebooks.config.general_functions.split_xy","title":"<code>split_xy(df)</code>","text":"<p>separate forecast target y and all predictors X into two dfs</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>df</code> <p>df containing the forecast target y and all predictors X</p> required Return <p>df_X (df): df of all predictors X df_y (df): df of target forecast y</p> Source code in <code>docs\\notebooks\\config\\general_functions.py</code> <pre><code>def split_xy(df):\n    \"\"\"separate forecast target y and all predictors X into two dfs\n\n    Args:\n        df (df): df containing the forecast target y and all predictors X\n\n    Return:\n        df_X (df): df of all predictors X\n        df_y (df): df of target forecast y\n    \"\"\"\n\n    df_y = df[['y']]\n    df_X = df.drop(\"y\", axis=1)\n\n    return df_X, df_y\n</code></pre>"},{"location":"api_reference/#docs.notebooks.config.general_functions.timeplot_forecast","title":"<code>timeplot_forecast(observation, forecast, pathname)</code>","text":"<p>Produce time plot of observation vs forecast value and save it on the designated folder</p> <p>Parameters:</p> Name Type Description Default <code>observation</code> <code>df</code> <p>observed value</p> required <code>forecast</code> <code>df</code> <p>forecast value</p> required <code>pathname</code> <code>str</code> <p>filepath to save the figure</p> required Source code in <code>docs\\notebooks\\config\\general_functions.py</code> <pre><code>def timeplot_forecast(observation, forecast, pathname):\n    \"\"\"Produce time plot of observation vs forecast value and save it on the designated folder\n\n    Args:\n        observation (df): observed value\n        forecast (df): forecast value\n        pathname (str): filepath to save the figure\n    \"\"\"\n    consecutive_timedelta = observation.index[-1] - observation.index[-2]\n    # Calculate total minutes in a week\n    minutes_per_week = 7 * 24 * 60  # 7 days * 24 hours * 60 minutes\n\n    # Calculate the number of minutes per timestep\n    minutes_per_timestep = consecutive_timedelta.total_seconds() / 60  # convert seconds to minutes\n\n    # Compute the number of timesteps in a week\n    timesteps_per_week = int(minutes_per_week / minutes_per_timestep)\n\n    # Create the figure with specified size\n    plt.figure(figsize=(9, 9))\n\n    # Set background color\n    # plt.gcf().patch.set_facecolor(platinum)\n\n    # Plot the actual and forecast data\n    plt.plot(observation[-timesteps_per_week:], color=dark_blue, label='Actual')\n    plt.plot(forecast[-timesteps_per_week:], color=orange, label='Forecast')\n\n    # Set the font to Arial\n    plt.rcParams['font.family'] = 'Arial'\n\n    # Remove grid lines\n    plt.grid(False)\n\n    # Set tick marks for x and y axis\n    plt.xticks(fontsize=12, color=dark_blue, alpha=0.5, rotation=30)\n    plt.yticks(fontsize=12, color=dark_blue, alpha=0.5)\n\n    # Add borders to the plot\n    plt.gca().spines['top'].set_color(dark_blue)\n    plt.gca().spines['right'].set_color(dark_blue)\n    plt.gca().spines['bottom'].set_color(dark_blue)\n    plt.gca().spines['left'].set_color(dark_blue)\n\n    # Remove the tick markers (the small lines)\n    plt.tick_params(axis='x', which='both', length=0)  # Remove x-axis tick markers\n    plt.tick_params(axis='y', which='both', length=0)  # Remove y-axis tick markers\n\n    # Set axis titles\n    plt.xlabel('Time', fontsize=14, color=dark_blue)\n    plt.ylabel('Net Load (kW)', fontsize=14, color=dark_blue)\n\n    # Remove title\n    plt.title('')\n\n    plt.legend(loc='upper left', fontsize=12, frameon=False, labelspacing=1, bbox_to_anchor=(1, 1))\n\n    plt.savefig(pathname, format='png', bbox_inches='tight')\n    plt.close()\n</code></pre>"},{"location":"api_reference/#docs.notebooks.config.general_functions.timeplot_residual","title":"<code>timeplot_residual(residual, pathname)</code>","text":"<p>Produce time plot of resodia; value and save it on the designated folder</p> <p>Parameters:</p> Name Type Description Default <code>residual</code> <code>df</code> <p>forecast - observation</p> required <code>pathname</code> <code>str</code> <p>filepath to save the figure</p> required Source code in <code>docs\\notebooks\\config\\general_functions.py</code> <pre><code>def timeplot_residual(residual, pathname):\n    \"\"\"Produce time plot of resodia; value and save it on the designated folder\n\n    Args:\n        residual (df): forecast - observation\n        pathname (str): filepath to save the figure\n    \"\"\"\n    consecutive_timedelta = residual.index[-1] - residual.index[-2]\n    # Calculate total minutes in a week\n    minutes_per_week = 7 * 24 * 60  # 7 days * 24 hours * 60 minutes\n\n    # Calculate the number of minutes per timestep\n    minutes_per_timestep = consecutive_timedelta.total_seconds() / 60  # convert seconds to minutes\n\n    # Compute the number of timesteps in a week\n    timesteps_per_week = int(minutes_per_week / minutes_per_timestep)\n\n    # Create the figure with specified size\n    plt.figure(figsize=(9, 9))\n\n    # Set background color\n    # plt.gcf().patch.set_facecolor(platinum)\n\n    # Plot the actual and forecast data\n    plt.plot(residual[-timesteps_per_week:], color=dark_blue, label='Actual')\n\n    # Set the font to Arial\n    plt.rcParams['font.family'] = 'Arial'\n\n    # Remove grid lines\n    plt.grid(False)\n\n    # Set tick marks for x and y axis\n    plt.xticks(fontsize=12, color=dark_blue, alpha=0.5, rotation=30)\n    plt.yticks(fontsize=12, color=dark_blue, alpha=0.5)\n\n    # Add borders to the plot\n    plt.gca().spines['top'].set_color(dark_blue)\n    plt.gca().spines['right'].set_color(dark_blue)\n    plt.gca().spines['bottom'].set_color(dark_blue)\n    plt.gca().spines['left'].set_color(dark_blue)\n\n    # Remove the tick markers (the small lines)\n    plt.tick_params(axis='x', which='both', length=0)  # Remove x-axis tick markers\n    plt.tick_params(axis='y', which='both', length=0)  # Remove y-axis tick markers\n\n    # Set axis titles\n    plt.xlabel('Time', fontsize=14, color=dark_blue)\n    plt.ylabel('Forecast Residual (kW)', fontsize=14, color=dark_blue)\n\n    # Remove title\n    plt.title('')\n\n    plt.savefig(pathname, format='png', bbox_inches='tight')\n    plt.close()\n</code></pre>"},{"location":"api_reference/#docs.notebooks.config.general_functions.train_model","title":"<code>train_model(model_name, hyperparameter, train_df_X, train_df_y, forecast_horizon)</code>","text":"<p>train model based on the model choice, hyperparamter, predictors X and target y</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>string</code> <p>eg 'm06_lr'</p> required <code>hyperparameter</code> <code>pd series</code> <p>list of hyperparameter for that model</p> required <code>train_df_X</code> <code>df</code> <p>matrix of predictors</p> required <code>train_df_y</code> <code>df</code> <p>target forecast y</p> required <code>forecast_horizon</code> <code>int</code> <p>Forecast horizon in minutes.</p> required <p>Returns:</p> Type Description <p>model (dict) : general object storing all models info including the predictor, feature selection, </p> <p>and all other relevant features</p> Source code in <code>docs\\notebooks\\config\\general_functions.py</code> <pre><code>def train_model(model_name, hyperparameter, train_df_X, train_df_y, forecast_horizon):\n    \"\"\"train model based on the model choice, hyperparamter, predictors X and target y\n\n    Args:\n        model_name (string): eg 'm06_lr'\n        hyperparameter (pd series): list of hyperparameter for that model\n        train_df_X (df): matrix of predictors\n        train_df_y (df): target forecast y\n        forecast_horizon (int): Forecast horizon in minutes.\n\n    Returns:\n        model (dict) : general object storing all models info including the predictor, feature selection, \n        and all other relevant features\n    \"\"\"\n\n    if model_name == 'm1_naive':\n        model = train_model_m1_naive(hyperparameter, train_df_X, train_df_y)\n    elif model_name == 'm2_snaive':\n        model = train_model_m2_snaive(hyperparameter, train_df_X, train_df_y)\n    elif model_name == 'm3_ets':\n        model = train_model_m3_ets(hyperparameter, train_df_X, train_df_y, forecast_horizon)\n    elif model_name == 'm4_arima':\n        model = train_model_m4_arima(hyperparameter, train_df_X, train_df_y, forecast_horizon)\n    elif model_name == 'm5_sarima':\n        model = train_model_m5_sarima(hyperparameter, train_df_X, train_df_y, forecast_horizon)\n    elif model_name == 'm6_lr':\n        model = train_model_m6_lr(hyperparameter, train_df_X, train_df_y)\n    elif model_name == 'm7_ann':\n        model = train_model_m7_ann(hyperparameter, train_df_X, train_df_y)\n    elif model_name == 'm8_dnn':\n        model = train_model_m8_dnn(hyperparameter, train_df_X, train_df_y)\n    elif model_name == 'm9_rt':\n        model = train_model_m9_rt(hyperparameter, train_df_X, train_df_y)\n    elif model_name == 'm10_rf':\n        model = train_model_m10_rf(hyperparameter, train_df_X, train_df_y)\n    elif model_name == 'm11_svr':\n        model = train_model_m11_svr(hyperparameter, train_df_X, train_df_y)\n    elif model_name == 'm12_rnn':\n        model = train_model_m12_rnn(hyperparameter, train_df_X, train_df_y)\n    elif model_name == 'm13_lstm':\n        model = train_model_m13_lstm(hyperparameter, train_df_X, train_df_y)\n    elif model_name == 'm14_gru':\n        model = train_model_m14_gru(hyperparameter, train_df_X, train_df_y)\n    elif model_name == 'm15_transformer':\n        model = train_model_m15_transformer(hyperparameter, train_df_X, train_df_y)\n    elif model_name == 'm16_prophet':\n        model = train_model_m16_prophet(hyperparameter, train_df_X, train_df_y, forecast_horizon)\n    elif model_name == 'm17_xgb':\n        model = train_model_m17_xgb(hyperparameter, train_df_X, train_df_y)\n    elif model_name == 'm18_nbeats':\n        model = train_model_m18_nbeats(hyperparameter, train_df_X, train_df_y)\n    else:\n        raise ValueError(\n            \"Wrong Model Choice! Available models are: m1_naive, m2_snaive, m3_ets, m4_arima, m5_sarima, m6_lr, m7_ann, m8_dnn, m9_rt, m10_rf, m11_svr, m12_rnn, m13_lstm, m14_gru, m15_transformer, m16_prophet, m17_xgb, m18_nbeats\"\n        )\n\n    return model\n</code></pre>"},{"location":"api_reference/#folder-model","title":"Folder <code>Model/</code>","text":""},{"location":"api_reference/#docs.notebooks.model.m1_naive.produce_forecast_m1_naive","title":"<code>produce_forecast_m1_naive(model, train_df_X, test_df_X)</code>","text":"<p>Create forecast at the train and test set using the trained model</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>dictionary</code> <p>all parameters of the trained model</p> required <code>train_df_X</code> <code>df</code> <p>predictors of train set</p> required <code>test_df_X</code> <code>df</code> <p>predictors of test set</p> required <p>Returns:</p> Type Description <p>train_df_y_hat (df) : forecast result at train set</p> <p>test_df_y_hat (df) : forecast result at test set</p> Source code in <code>docs\\notebooks\\model\\m1_naive.py</code> <pre><code>def produce_forecast_m1_naive(model, train_df_X, test_df_X):\n    \"\"\"Create forecast at the train and test set using the trained model\n\n    Args:\n        model (dictionary): all parameters of the trained model\n        train_df_X (df): predictors of train set\n        test_df_X (df): predictors of test set\n\n    Returns:\n        train_df_y_hat (df) : forecast result at train set\n        test_df_y_hat (df) : forecast result at test set\n\n    \"\"\"\n\n    # PRODUCE FORECAST\n    horizon_timedelta = pd.Timedelta(minutes=forecast_horizon)\n    last_observation = f'y_lag_{horizon_timedelta}m'\n    train_df_y_hat = train_df_X[last_observation]\n    test_df_y_hat = test_df_X[last_observation]\n\n    return train_df_y_hat, test_df_y_hat\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m1_naive.train_model_m1_naive","title":"<code>train_model_m1_naive(hyperparameter, train_df_X, train_df_y)</code>","text":"<p>Train and test a naive model for point forecasting. </p> <p>Parameters:</p> Name Type Description Default <code>hyperparameter (df) </code> <p>hyperparameter value of the model consisting of number of features</p> required <code>train_df_X (df) </code> <p>features matrix for training</p> required <code>train_df_y (df) </code> <p>target matrix for training</p> required <p>Returns:</p> Type Description <p>model (model) : trained model with all features</p> Source code in <code>docs\\notebooks\\model\\m1_naive.py</code> <pre><code>def train_model_m1_naive(hyperparameter, train_df_X, train_df_y):\n    ''' Train and test a naive model for point forecasting. \n\n    Args:\n        hyperparameter (df) : hyperparameter value of the model consisting of number of features\n        train_df_X (df) : features matrix for training\n        train_df_y (df) : target matrix for training\n\n\n    Returns:\n        model (model) : trained model with all features\n    '''\n\n    #UNPACK HYPERPARAMETER\n    #no hyperparameter for naive model\n\n    #TRAIN MODEL\n    #no training is required for naive model\n\n    # PACK MODEL\n    model = {}\n\n\n    return model\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m2_snaive.produce_forecast_m2_snaive","title":"<code>produce_forecast_m2_snaive(model, train_df_X, test_df_X)</code>","text":"<p>Create forecast at the train and test set using the trained model</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>dictionary</code> <p>all parameters of the trained model</p> required <code>train_df_X</code> <code>df</code> <p>predictors of train set</p> required <code>test_df_X</code> <code>df</code> <p>predictors of test set</p> required <p>Returns:</p> Type Description <p>train_df_y_hat (df) : forecast result at train set</p> <p>test_df_y_hat (df) : forecast result at test set</p> Source code in <code>docs\\notebooks\\model\\m2_snaive.py</code> <pre><code>def produce_forecast_m2_snaive(model, train_df_X, test_df_X):\n    \"\"\"Create forecast at the train and test set using the trained model\n\n    Args:\n        model (dictionary): all parameters of the trained model\n        train_df_X (df): predictors of train set\n        test_df_X (df): predictors of test set\n\n    Returns:\n        train_df_y_hat (df) : forecast result at train set\n        test_df_y_hat (df) : forecast result at test set\n\n    \"\"\"\n\n    # UNPACK MODEL\n    col_name = model['col_name']  #this depends on the lag day\n\n    # PRODUCE FORECAST\n    train_df_y_hat = train_df_X[col_name]\n    test_df_y_hat = test_df_X[col_name]\n\n    return train_df_y_hat, test_df_y_hat\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m2_snaive.train_model_m2_snaive","title":"<code>train_model_m2_snaive(hyperparameter, train_df_X, train_df_y)</code>","text":"<p>Train and test a seasonal model for point forecasting. </p> <p>Parameters:</p> Name Type Description Default <code>hyperparameter (df) </code> <p>hyperparameter value of the model consisting of number of features</p> required <code>train_df_X (df) </code> <p>features matrix for training</p> required <code>train_df_y (df) </code> <p>target matrix for training</p> required <p>Returns:</p> Type Description <p>model (model) : trained model with all features</p> Source code in <code>docs\\notebooks\\model\\m2_snaive.py</code> <pre><code>def train_model_m2_snaive(hyperparameter, train_df_X, train_df_y):\n    ''' Train and test a seasonal model for point forecasting. \n\n    Args:\n        hyperparameter (df) : hyperparameter value of the model consisting of number of features\n        train_df_X (df) : features matrix for training\n        train_df_y (df) : target matrix for training\n\n\n    Returns:\n        model (model) : trained model with all features\n    '''\n\n    #UNPACK HYPERPARAMETER\n    days = hyperparameter['days']\n    col_name = f'y_lag_{days} days 00:00:00m'\n\n    #TRAIN MODEL\n    #no training is required for seasonal naive model\n\n    # PACK MODEL\n    model = {\"col_name\": col_name }\n\n\n    return model\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m3_ets.produce_forecast_m3_ets","title":"<code>produce_forecast_m3_ets(model, train_df_X, test_df_X, forecast_horizon)</code>","text":"<p>Create forecast at the train and test set using the trained model</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>dictionary</code> <p>all parameters of the trained model</p> required <code>train_df_X</code> <code>df</code> <p>predictors of train set</p> required <code>test_df_X</code> <code>df</code> <p>predictors of test set</p> required <code>forecast_horizon</code> <code>int</code> <p>forecast horizon in mins</p> required <p>Returns:</p> Type Description <p>train_df_y_hat (df) : forecast result at train set</p> <p>test_df_y_hat (df) : forecast result at test set</p> Source code in <code>docs\\notebooks\\model\\m3_ets.py</code> <pre><code>def produce_forecast_m3_ets(model, train_df_X, test_df_X, forecast_horizon):\n    \"\"\"Create forecast at the train and test set using the trained model\n\n    Args:\n        model (dictionary): all parameters of the trained model\n        train_df_X (df): predictors of train set\n        test_df_X (df): predictors of test set\n        forecast_horizon (int): forecast horizon in mins\n\n    Returns:\n        train_df_y_hat (df) : forecast result at train set\n        test_df_y_hat (df) : forecast result at test set\n\n    \"\"\"\n\n    timestep_frequency = test_df_X.index[1] - test_df_X.index[0]\n    n_timestep_forecast_horizon = int(forecast_horizon / (timestep_frequency.total_seconds() / 60))\n\n\n    train_df_X_updated = remove_jump_df(train_df_X)\n    test_df_X_updated = remove_jump_df(test_df_X)\n\n    # UNPACK MODEL\n    model_fitted = model['model_fitted']\n\n    # PRODUCE FORECAST FOR TRAIN SET\n    train_df_y_hat = pd.DataFrame(model_fitted.fittedvalues)\n    train_df_y_hat.columns = ['y']\n\n    # train_df_y_hat_2 = pd.DataFrame(model_fitted.forecast(n_timestep_forecast_horizon-1))\n    # train_df_y_hat_2.columns = ['y']\n    # train_df_y_hat = pd.concat([train_df_y_hat, train_df_y_hat_2])\n\n    train_df_y_hat.index.name = 'datetime'\n\n    # TRANSFORM test_df_X to a series with only the last lag\n    horizon_timedelta = pd.Timedelta(minutes=forecast_horizon)\n    last_observation = f'y_lag_{horizon_timedelta}m'\n    test_df_y_last = test_df_X[last_observation]\n\n\n    # REFIT THE MODEL AND PRODUCE NEW FORECAST FOR TEST SET\n    # THIS CODE RESULTS IN 2 MINS\n    test_df_y_hat = pd.DataFrame(index = test_df_X.index)\n    test_df_y_hat['y_hat'] = np.nan\n\n\n    # in the case of CV 10, which is when test df &lt; train df\n    # don't compute the test forecast\n    if (test_df_X.index[-1] &lt; train_df_X.index[0]):\n    # this is the case when we use CV10, where the test set is before the train set\n        print(\"Test set is before train set / CV 10, no test forecast can be made\")\n        return train_df_y_hat, test_df_y_hat\n\n    for i in range(len(test_df_y_last)):\n    # for i in range(2): #for test only\n        print('Processing i = ', i + 1, ' out of ', len(test_df_y_last)),\n        if i == 0:\n            test_df_y_hat.iloc[i, 0] = model_fitted.forecast(steps=n_timestep_forecast_horizon).iloc[-1]\n        else:\n            new_row = pd.DataFrame([test_df_y_last.values[i]], columns=['y'], index=[test_df_y_last.index[i] - dt.timedelta(minutes=forecast_horizon)])\n            new_row = new_row.asfreq(test_df_X_updated.index.freq)\n\n            model_fitted = model_fitted.append(new_row)\n            test_df_y_hat.iloc[i, 0] = model_fitted.forecast(steps=n_timestep_forecast_horizon).iloc[-1] # to update based on the forecast horizon\n\n    # test_df_y_hat = m06_lr.predict(test_df_X)\n\n    return train_df_y_hat, test_df_y_hat\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m3_ets.train_model_m3_ets","title":"<code>train_model_m3_ets(hyperparameter, train_df_X, train_df_y, forecast_horizon)</code>","text":"<p>Train and test a linear model for point forecasting.  https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.exponential_smoothing.ExponentialSmoothing.html</p> <p>Parameters:</p> Name Type Description Default <code>hyperparameter (df) </code> <p>hyperparameter value of the model consisting of number of features</p> required <code>train_df_X (df) </code> <p>features matrix for training</p> required <code>train_df_y (df) </code> <p>target matrix for training</p> required <code>forecast_horizon (int) </code> <p>forecast horizon in mins</p> required <p>Returns:</p> Type Description <p>model (model) : trained model with all features</p> Source code in <code>docs\\notebooks\\model\\m3_ets.py</code> <pre><code>def train_model_m3_ets(hyperparameter, train_df_X, train_df_y, forecast_horizon):\n    ''' Train and test a linear model for point forecasting. \n    https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.exponential_smoothing.ExponentialSmoothing.html\n\n    Args:\n        hyperparameter (df) : hyperparameter value of the model consisting of number of features\n        train_df_X (df) : features matrix for training\n        train_df_y (df) : target matrix for training\n        forecast_horizon (int) : forecast horizon in mins\n\n    Returns:\n        model (model) : trained model with all features\n    '''\n\n    #UNPACK HYPERPARAMETER\n    trend = hyperparameter['trend']\n    damped_trend = hyperparameter['damped_trend']\n    seasonal_periods_days = hyperparameter['seasonal_periods_days']\n\n    # UPDATE train_df_y to exclude all rows after a sudden jump in the timestep\n    train_df_y_updated = remove_jump_df(train_df_y)\n\n    # TRAIN MODEL\n    # Calculate the frequency of the timesteps using the first and second index values\n    timestep_frequency = train_df_y_updated.index[1] - train_df_y_updated.index[0]\n    inferred_frequency = pd.infer_freq(train_df_y_updated.index)\n    train_df_y_updated = train_df_y_updated.asfreq(inferred_frequency) \n\n    # INTRODUCE GAP BETWEEN TRAIN AND TEST SET TO AVOID DATA LEAKAGE\n    n_timestep_forecast_horizon = int(forecast_horizon / (timestep_frequency.total_seconds() / 60))\n    if n_timestep_forecast_horizon == 1:\n        pass\n    else:\n        train_df_y_updated = train_df_y_updated[:-(n_timestep_forecast_horizon - 1)]\n\n    # Assuming train_df_y_updated is your dataframe and 'y' is the column with the training series\n    y = train_df_y_updated['y']\n\n   # Build and fit the state-space Exponential Smoothing model\n    model_fitted = ExponentialSmoothing(\n        y,\n        trend=trend,\n        seasonal=None, #can be updated later\n        damped_trend=damped_trend\n    ).fit()\n\n\n    # Print the model summary\n    # print(model_fitted.summary())\n\n    # PACK MODEL\n    model = {\"model_fitted\": model_fitted}\n\n\n    return model\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m4_arima.produce_forecast_m4_arima","title":"<code>produce_forecast_m4_arima(model, train_df_X, test_df_X, forecast_horizon)</code>","text":"<p>Create forecast at the train and test set using the trained model</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>dictionary</code> <p>all parameters of the trained model</p> required <code>train_df_X</code> <code>df</code> <p>predictors of train set</p> required <code>test_df_X</code> <code>df</code> <p>predictors of test set</p> required <code>forecast_horizon</code> <code>int</code> <p>forecast horizon in mins</p> required <p>Returns:</p> Type Description <p>train_df_y_hat (df) : forecast result at train set</p> <p>test_df_y_hat (df) : forecast result at test set</p> Source code in <code>docs\\notebooks\\model\\m4_arima.py</code> <pre><code>def produce_forecast_m4_arima(model, train_df_X, test_df_X, forecast_horizon):\n    \"\"\"Create forecast at the train and test set using the trained model\n\n    Args:\n        model (dictionary): all parameters of the trained model\n        train_df_X (df): predictors of train set\n        test_df_X (df): predictors of test set\n        forecast_horizon (int): forecast horizon in mins\n\n    Returns:\n        train_df_y_hat (df) : forecast result at train set\n        test_df_y_hat (df) : forecast result at test set\n\n    \"\"\"\n    timestep_frequency = test_df_X.index[1] - test_df_X.index[0]\n    n_timestep_forecast_horizon = int(forecast_horizon / (timestep_frequency.total_seconds() / 60))\n\n    train_df_X_updated = remove_jump_df(train_df_X)\n    test_df_X_updated = remove_jump_df(test_df_X)\n\n    # UNPACK MODEL\n    model_fitted = model['model_fitted']\n\n    # PRODUCE FORECAST FOR TRAIN SET\n    train_df_y_hat = pd.DataFrame(model_fitted.fittedvalues)\n    train_df_y_hat.columns = ['y']\n\n    # train_df_y_hat_2 = pd.DataFrame(model_fitted.forecast(n_timestep_forecast_horizon-1))\n    # train_df_y_hat_2.columns = ['y']\n    # train_df_y_hat = pd.concat([train_df_y_hat, train_df_y_hat_2])\n\n    train_df_y_hat.index.name = 'datetime'\n\n    # TRANSFORM test_df_X to a series with only the last lag\n    horizon_timedelta = pd.Timedelta(minutes=forecast_horizon)\n    last_observation = f'y_lag_{horizon_timedelta}m'\n    test_df_y_last = test_df_X[last_observation]\n\n\n    # REFIT THE MODEL AND PRODUCE NEW FORECAST FOR TEST SET\n    # THIS CODE RESULTS IN 2 MINS\n    test_df_y_hat = pd.DataFrame(index = test_df_X.index)\n    test_df_y_hat['y_hat'] = np.nan\n\n    # in the case of CV 10, which is when test df &lt; train df\n    # don't compute the test forecast\n    if (test_df_X.index[-1] &lt; train_df_X.index[0]):\n    # this is the case when we use CV10, where the test set is before the train set\n        print(\"Test set is before train set / CV 10, no test forecast can be made\")\n        return train_df_y_hat, test_df_y_hat\n\n    for i in range(len(test_df_y_last)):\n    # for i in range(2): #for test only\n        print('Processing i = ', i + 1, ' out of ', len(test_df_y_last)),\n        if i == 0:\n            test_df_y_hat.iloc[i, 0] = model_fitted.forecast(steps=n_timestep_forecast_horizon).iloc[-1]\n        else:\n            new_row = pd.DataFrame([test_df_y_last.values[i]], columns=['y'], index=[test_df_y_last.index[i] - dt.timedelta(minutes=forecast_horizon)])\n            new_row = new_row.asfreq(test_df_X_updated.index.freq)\n\n            model_fitted = model_fitted.append(new_row)\n            test_df_y_hat.iloc[i, 0] = model_fitted.forecast(steps=n_timestep_forecast_horizon).iloc[-1] # to update based on the forecast horizon\n\n\n    # test_df_y_hat = m06_lr.predict(test_df_X)\n\n    return train_df_y_hat, test_df_y_hat\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m4_arima.train_model_m4_arima","title":"<code>train_model_m4_arima(hyperparameter, train_df_X, train_df_y, forecast_horizon)</code>","text":"<p>Train and test a linear model for point forecasting. </p> <p>Parameters:</p> Name Type Description Default <code>hyperparameter (df) </code> <p>hyperparameter value of the model consisting of number of features</p> required <code>train_df_X (df) </code> <p>features matrix for training</p> required <code>train_df_y (df) </code> <p>target matrix for training</p> required <code>forecast_horizon (int) </code> <p>forecast horizon in mins</p> required <p>Returns:</p> Type Description <p>model (model) : trained model with all features</p> Source code in <code>docs\\notebooks\\model\\m4_arima.py</code> <pre><code>def train_model_m4_arima(hyperparameter, train_df_X, train_df_y, forecast_horizon):\n    ''' Train and test a linear model for point forecasting. \n\n    Args:\n        hyperparameter (df) : hyperparameter value of the model consisting of number of features\n        train_df_X (df) : features matrix for training\n        train_df_y (df) : target matrix for training\n        forecast_horizon (int) : forecast horizon in mins\n\n\n    Returns:\n        model (model) : trained model with all features\n    '''\n\n    #UNPACK HYPERPARAMETER\n    p = hyperparameter['p']\n    d = hyperparameter['d']\n    q = hyperparameter['q']\n\n\n    # UPDATE train_df_y to exclude all rows after a sudden jump in the timestep\n    train_df_y_updated = remove_jump_df(train_df_y)\n\n    # TRAIN MODEL\n    # Calculate the frequency of the timesteps using the first and second index values\n    timestep_frequency = train_df_y_updated.index[1] - train_df_y_updated.index[0]\n    inferred_frequency = pd.infer_freq(train_df_y_updated.index)\n    train_df_y_updated = train_df_y_updated.asfreq(inferred_frequency)\n\n    # INTRODUCE GAP BETWEEN TRAIN AND TEST SET TO AVOID DATA LEAKAGE\n    n_timestep_forecast_horizon = int(forecast_horizon / (timestep_frequency.total_seconds() / 60))\n    if n_timestep_forecast_horizon == 1:\n        pass\n    else:\n        train_df_y_updated = train_df_y_updated[:-(n_timestep_forecast_horizon - 1)]\n\n    # Assuming train_df_y_updated is your dataframe and 'y' is the column with the training series\n    y = train_df_y_updated['y']\n\n    # Build and fit the state-space ARIMA model\n    model_fitted = ARIMA(y, order=(p, d, q), freq=inferred_frequency).fit()\n\n    # PACK MODEL\n    model = {\"model_fitted\": model_fitted}\n\n\n    return model\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m5_sarima.produce_forecast_m5_sarima","title":"<code>produce_forecast_m5_sarima(model, train_df_X, test_df_X, forecast_horizon)</code>","text":"<p>Create forecast at the train and test set using the trained model</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>dictionary</code> <p>all parameters of the trained model</p> required <code>train_df_X</code> <code>df</code> <p>predictors of train set</p> required <code>test_df_X</code> <code>df</code> <p>predictors of test set</p> required <code>forecast_horizon</code> <code>int</code> <p>forecast horizon in mins</p> required <p>Returns:</p> Type Description <p>train_df_y_hat (df) : forecast result at train set</p> <p>test_df_y_hat (df) : forecast result at test set</p> Source code in <code>docs\\notebooks\\model\\m5_sarima.py</code> <pre><code>def produce_forecast_m5_sarima(model, train_df_X, test_df_X, forecast_horizon):\n    \"\"\"Create forecast at the train and test set using the trained model\n\n    Args:\n        model (dictionary): all parameters of the trained model\n        train_df_X (df): predictors of train set\n        test_df_X (df): predictors of test set\n        forecast_horizon (int): forecast horizon in mins\n\n    Returns:\n        train_df_y_hat (df) : forecast result at train set\n        test_df_y_hat (df) : forecast result at test set\n\n    \"\"\"\n    timestep_frequency = test_df_X.index[1] - test_df_X.index[0]\n    n_timestep_forecast_horizon = int(forecast_horizon / (timestep_frequency.total_seconds() / 60))\n\n    train_df_X_updated = remove_jump_df(train_df_X)\n    test_df_X_updated = remove_jump_df(test_df_X)\n\n    # UNPACK MODEL\n    model_fitted = model['model_fitted']\n\n    # PRODUCE FORECAST FOR TRAIN SET\n    train_df_y_hat = pd.DataFrame(model_fitted.fittedvalues)\n    train_df_y_hat.columns = ['y']\n\n    # train_df_y_hat_2 = pd.DataFrame(model_fitted.forecast(n_timestep_forecast_horizon-1))\n    # train_df_y_hat_2.columns = ['y']\n    # train_df_y_hat = pd.concat([train_df_y_hat, train_df_y_hat_2])\n\n    train_df_y_hat.index.name = 'datetime'\n\n    # TRANSFORM test_df_X to a series with only the last lag\n    horizon_timedelta = pd.Timedelta(minutes=forecast_horizon)\n    last_observation = f'y_lag_{horizon_timedelta}m'\n    test_df_y_last = test_df_X[last_observation]\n\n\n    # REFIT THE MODEL AND PRODUCE NEW FORECAST FOR TEST SET\n    # THIS CODE RESULTS IN 2 MINS\n    test_df_y_hat = pd.DataFrame(index = test_df_X.index)\n    test_df_y_hat['y_hat'] = np.nan\n\n    # in the case of CV 10, which is when test df &lt; train df\n    # don't compute the test forecast\n    if (test_df_X.index[-1] &lt; train_df_X.index[0]):\n    # this is the case when we use CV10, where the test set is before the train set\n        print(\"Test set is before train set / CV 10, no test forecast can be made\")\n        return train_df_y_hat, test_df_y_hat\n\n    for i in range(len(test_df_y_last)):\n    # for i in range(2): #for test only\n        print('Processing i = ', i + 1, ' out of ', len(test_df_y_last)),\n        if i == 0:\n            test_df_y_hat.iloc[i, 0] = model_fitted.forecast(steps=n_timestep_forecast_horizon).iloc[-1]\n        else:\n            new_row = pd.DataFrame([test_df_y_last.values[i]], columns=['y'], index=[test_df_y_last.index[i] - dt.timedelta(minutes=forecast_horizon)])\n            new_row = new_row.asfreq(test_df_X_updated.index.freq)\n\n            model_fitted = model_fitted.append(new_row)\n            test_df_y_hat.iloc[i, 0] = model_fitted.forecast(steps=n_timestep_forecast_horizon).iloc[-1] # to update based on the forecast horizon\n\n\n    # test_df_y_hat = m06_lr.predict(test_df_X)\n\n    return train_df_y_hat, test_df_y_hat\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m5_sarima.train_model_m5_sarima","title":"<code>train_model_m5_sarima(hyperparameter, train_df_X, train_df_y, forecast_horizon)</code>","text":"<p>Train and test a linear model for point forecasting. </p> <p>Parameters:</p> Name Type Description Default <code>hyperparameter (df) </code> <p>hyperparameter value of the model consisting of number of features</p> required <code>train_df_X (df) </code> <p>features matrix for training</p> required <code>train_df_y (df) </code> <p>target matrix for training</p> required <code>forecast_horizon (int) </code> <p>forecast horizon in mins</p> required <p>Returns:</p> Type Description <p>model (model) : trained model with all features</p> Source code in <code>docs\\notebooks\\model\\m5_sarima.py</code> <pre><code>def train_model_m5_sarima(hyperparameter, train_df_X, train_df_y, forecast_horizon):\n    ''' Train and test a linear model for point forecasting. \n\n    Args:\n        hyperparameter (df) : hyperparameter value of the model consisting of number of features\n        train_df_X (df) : features matrix for training\n        train_df_y (df) : target matrix for training\n        forecast_horizon (int) : forecast horizon in mins\n\n\n    Returns:\n        model (model) : trained model with all features\n    '''\n\n    #UNPACK HYPERPARAMETER\n    p = hyperparameter['p']\n    d = hyperparameter['d']\n    q = hyperparameter['q']\n    P = hyperparameter['P']\n    D = hyperparameter['D']\n    Q = hyperparameter['Q']\n    seasonal_period_days = hyperparameter['seasonal_period_days']\n\n\n    # UPDATE train_df_y to exclude all rows after a sudden jump in the timestep\n    train_df_y_updated = remove_jump_df(train_df_y)\n\n    # TRAIN MODEL\n    # Calculate the frequency of the timesteps using the first and second index values\n    timestep_frequency = train_df_y_updated.index[1] - train_df_y_updated.index[0]\n    s = int(seasonal_period_days * 24 * 60 / (timestep_frequency.seconds / 60))\n    inferred_frequency = pd.infer_freq(train_df_y_updated.index)\n    train_df_y_updated = train_df_y_updated.asfreq(inferred_frequency)\n\n    # INTRODUCE GAP BETWEEN TRAIN AND TEST SET TO AVOID DATA LEAKAGE\n    n_timestep_forecast_horizon = int(forecast_horizon / (timestep_frequency.total_seconds() / 60))\n    if n_timestep_forecast_horizon == 1:\n        pass\n    else:\n        train_df_y_updated = train_df_y_updated[:-(n_timestep_forecast_horizon - 1)]\n\n    # Assuming train_df_y_updated is your dataframe and 'y' is the column with the training series\n    y = train_df_y_updated['y']\n\n    # Build and fit the state-space ARIMA model\n    model_fitted = SARIMAX(y, order=(p, d, q), seasonal_order = (P, D, Q, s), freq=inferred_frequency).fit()\n\n    # PACK MODEL\n    model = {\"model_fitted\": model_fitted}\n\n\n    return model\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m6_lr.produce_forecast_m6_lr","title":"<code>produce_forecast_m6_lr(model, train_df_X, test_df_X)</code>","text":"<p>Create forecast at the train and test set using the trained model</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>dictionary</code> <p>all parameters of the trained model</p> required <code>train_df_X</code> <code>df</code> <p>predictors of train set</p> required <code>test_df_X</code> <code>df</code> <p>predictors of test set</p> required <p>Returns:</p> Type Description <p>train_df_y_hat (df) : forecast result at train set</p> <p>test_df_y_hat (df) : forecast result at test set</p> Source code in <code>docs\\notebooks\\model\\m6_lr.py</code> <pre><code>def produce_forecast_m6_lr(model, train_df_X, test_df_X):\n    \"\"\"Create forecast at the train and test set using the trained model\n\n    Args:\n        model (dictionary): all parameters of the trained model\n        train_df_X (df): predictors of train set\n        test_df_X (df): predictors of test set\n\n    Returns:\n        train_df_y_hat (df) : forecast result at train set\n        test_df_y_hat (df) : forecast result at test set\n\n    \"\"\"\n    fs_lr = model['feature_selector']\n    m06_lr = model['regression_model']\n\n    # SELECT K BEST FEATURES\n    train_df_X = fs_lr.transform(train_df_X)\n    test_df_X = fs_lr.transform(test_df_X)\n\n    # PRODUCE FORECAST\n    train_df_y_hat = m06_lr.predict(train_df_X)\n    test_df_y_hat = m06_lr.predict(test_df_X)\n\n    return train_df_y_hat, test_df_y_hat\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m6_lr.train_model_m6_lr","title":"<code>train_model_m6_lr(hyperparameter, train_df_X, train_df_y)</code>","text":"<p>Train and test a linear model for point forecasting. </p> <p>Parameters:</p> Name Type Description Default <code>hyperparameter (df) </code> <p>hyperparameter value of the model consisting of number of features</p> required <code>train_df_X (df) </code> <p>features matrix for training</p> required <code>train_df_y (df) </code> <p>target matrix for training</p> required <p>Returns:</p> Type Description <p>model (dictionary) : trained model with all features</p> Source code in <code>docs\\notebooks\\model\\m6_lr.py</code> <pre><code>def train_model_m6_lr(hyperparameter, train_df_X, train_df_y):\n    ''' Train and test a linear model for point forecasting. \n\n    Args:\n        hyperparameter (df) : hyperparameter value of the model consisting of number of features\n        train_df_X (df) : features matrix for training\n        train_df_y (df) : target matrix for training\n\n\n    Returns:\n        model (dictionary) : trained model with all features\n    '''\n\n    #UNPACK HYPERPARAMETER\n    num_feature = int(hyperparameter['num_features'])\n\n    # FEATURE SELECTOR\n    def select_features(train_df_X, train_df_y, num_feature):\n        ''' Make model to select K best feature. \n\n        Args:\n            train_df_X (df) : features matrix for training\n            train_df_y (df) : target matrix for training\n\n        Returns:\n            fs_lr (model) : feature selector\n        '''\n\n        train_df_y = train_df_y.values.ravel()\n        fs_lr = SelectKBest(f_regression, k = num_feature)\n        fs_lr.fit(train_df_X, train_df_y)\n\n        return fs_lr\n\n    fs_lr = select_features(train_df_X, train_df_y, num_feature)\n\n    #TRAIN MODEL\n    train_df_X = fs_lr.transform(train_df_X)\n    m06_lr = LinearRegression()\n    m06_lr.fit(train_df_X, train_df_y)\n\n    # PACK MODEL\n    model = {\"feature_selector\": fs_lr, \"regression_model\": m06_lr}    \n\n    return model\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m7_ann.produce_forecast_m7_ann","title":"<code>produce_forecast_m7_ann(model, train_df_X, test_df_X)</code>","text":"<p>Create forecast at the train and test set using the trained model</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>dictionary</code> <p>all parameters of the trained model</p> required <code>train_df_X</code> <code>df</code> <p>predictors of train set</p> required <code>test_df_X</code> <code>df</code> <p>predictors of test set</p> required <p>Returns:</p> Type Description <p>train_df_y_hat (df) : forecast result at train set</p> <p>test_df_y_hat (df) : forecast result at test set</p> Source code in <code>docs\\notebooks\\model\\m7_ann.py</code> <pre><code>def produce_forecast_m7_ann(model, train_df_X, test_df_X):\n    \"\"\"Create forecast at the train and test set using the trained model\n\n    Args:\n        model (dictionary): all parameters of the trained model\n        train_df_X (df): predictors of train set\n        test_df_X (df): predictors of test set\n\n    Returns:\n        train_df_y_hat (df) : forecast result at train set\n        test_df_y_hat (df) : forecast result at test set\n\n    \"\"\"\n\n    # UNPACK MODEL\n    model_ann = model[\"model_ann\"]\n\n    # PREPARE FORMAT\n    train_df_X_tensor = torch.tensor(train_df_X.values, dtype=torch.float32)\n    test_df_X_tensor = torch.tensor(test_df_X.values, dtype=torch.float32)\n\n    # PRODUCE FORECAST\n    # Switch model to evaluation mode for inference\n    model_ann.eval()\n\n    # TRAIN SET FORECAST\n    with torch.no_grad():  # Disable gradient calculation to save memory\n        train_df_y_hat_tensor = model_ann(train_df_X_tensor)\n\n    # TEST SET FORECAST\n    with torch.no_grad():  # Disable gradient calculation to save memory\n        test_df_y_hat_tensor = model_ann(test_df_X_tensor)\n\n    # Create DataFrames of result\n    train_df_y_hat = pd.DataFrame(train_df_y_hat_tensor, index=train_df_X.index, columns=['y_hat'])\n    test_df_y_hat = pd.DataFrame(test_df_y_hat_tensor, index=test_df_X.index, columns=['y_hat'])\n\n    return train_df_y_hat, test_df_y_hat\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m7_ann.train_model_m7_ann","title":"<code>train_model_m7_ann(hyperparameter, train_df_X, train_df_y)</code>","text":"<p>Train and test a linear model for point forecasting. </p> <p>Parameters:</p> Name Type Description Default <code>hyperparameter (df) </code> <p>hyperparameter value of the model consisting of number of features</p> required <code>train_df_X (df) </code> <p>features matrix for training</p> required <code>train_df_y (df) </code> <p>target matrix for training</p> required <p>Returns:</p> Type Description <p>model (model) : trained model with all features</p> Source code in <code>docs\\notebooks\\model\\m7_ann.py</code> <pre><code>def train_model_m7_ann(hyperparameter, train_df_X, train_df_y):\n    ''' Train and test a linear model for point forecasting. \n\n    Args:\n        hyperparameter (df) : hyperparameter value of the model consisting of number of features\n        train_df_X (df) : features matrix for training\n        train_df_y (df) : target matrix for training\n\n\n    Returns:\n        model (model) : trained model with all features\n    '''\n\n    #UNPACK HYPERPARAMETER\n\n    # Set random seed for reproducibility\n    def set_seed(seed):\n        random.seed(seed)\n        os.environ[\"PYTHONHASHSEED\"] = str(seed)\n        np.random.seed(seed)\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed(seed)\n        torch.backends.cudnn.deterministic = True\n\n    seed = int(hyperparameter['seed'])\n\n    hidden_size = hyperparameter['hidden_size']\n    activation_function = hyperparameter['activation_function']\n    learning_rate = hyperparameter['learning_rate']\n    # learning_rate = 0.001\n    solver = hyperparameter['solver']\n    epochs = hyperparameter['epochs']\n\n    # Use proper format for X and y\n    X = torch.tensor(train_df_X.values, dtype=torch.float32)\n    y = torch.tensor(train_df_y.values, dtype=torch.float32).view(-1, 1) \n\n    # Define the ANN model\n    class ANNModel(nn.Module):\n        def __init__(self, input_size, hidden_size, output_size):\n            super(ANNModel, self).__init__()\n            self.fc1 = nn.Linear(input_size, hidden_size)\n            self.fc2 = nn.Linear(hidden_size, output_size)\n            self.relu = nn.ReLU()  # Activation function\n\n        def forward(self, x):\n            x = self.fc1(x)\n            if activation_function == 'relu':\n                x = self.relu(x)\n            elif activation_function == 'sigmoid':\n                x = torch.sigmoid(x)\n            else:\n                x = torch.tanh(x)\n            x = self.fc2(x)\n            return x\n\n    # Model initialization\n    input_size = X.shape[1]\n    output_size = y.shape[1]\n\n    set_seed(seed)\n\n    model_ann = ANNModel(input_size, hidden_size, output_size)\n    if solver == 'adam':\n        optimizer = optim.Adam(model_ann.parameters(), lr=learning_rate)\n    elif solver == 'sgd':\n        optimizer = optim.SGD(model_ann.parameters(), lr=learning_rate)\n    else:\n        raise ValueError('Solver not found')\n\n    # Loss function\n    criterion = nn.MSELoss()  # Mean Squared Error loss for regression\n\n    #TRAIN MODEL\n    # Training loop\n    for epoch in range(epochs):\n        model_ann.train()\n\n        # Forward pass\n        output = model_ann(X)\n        loss = criterion(output, y)\n\n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n\n        # Update weights\n        optimizer.step()\n\n        if epoch % 10 == 0:\n            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n\n    # PACK MODEL\n    model = {\"model_ann\": model_ann}\n\n\n    return model\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m8_dnn.produce_forecast_m8_dnn","title":"<code>produce_forecast_m8_dnn(model, train_df_X, test_df_X)</code>","text":"<p>Create forecast at the train and test set using the trained model</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>dict</code> <p>all parameters of the trained model</p> required <code>train_df_X</code> <code>DataFrame</code> <p>predictors of train set</p> required <code>test_df_X</code> <code>DataFrame</code> <p>predictors of test set</p> required <p>Returns:</p> Type Description <p>train_df_y_hat (DataFrame) : forecast result at train set</p> <p>test_df_y_hat (DataFrame) : forecast result at test set</p> Source code in <code>docs\\notebooks\\model\\m8_dnn.py</code> <pre><code>def produce_forecast_m8_dnn(model, train_df_X, test_df_X):\n    \"\"\"Create forecast at the train and test set using the trained model\n\n    Args:\n        model (dict): all parameters of the trained model\n        train_df_X (DataFrame): predictors of train set\n        test_df_X (DataFrame): predictors of test set\n\n    Returns:\n        train_df_y_hat (DataFrame) : forecast result at train set\n        test_df_y_hat (DataFrame) : forecast result at test set\n    \"\"\"\n\n    # UNPACK MODEL\n    model_dnn = model[\"model_dnn\"]\n\n    # PREPARE FORMAT\n    train_df_X_tensor = torch.tensor(train_df_X.values, dtype=torch.float32)\n    test_df_X_tensor = torch.tensor(test_df_X.values, dtype=torch.float32)\n\n    # PRODUCE FORECAST\n    # Switch model to evaluation mode for inference\n    model_dnn.eval()\n\n    # TRAIN SET FORECAST\n    with torch.no_grad():  # Disable gradient calculation to save memory\n        train_df_y_hat_tensor = model_dnn(train_df_X_tensor)\n\n    # TEST SET FORECAST\n    with torch.no_grad():  # Disable gradient calculation to save memory\n        test_df_y_hat_tensor = model_dnn(test_df_X_tensor)\n\n    # Create DataFrames of result\n    train_df_y_hat = pd.DataFrame(train_df_y_hat_tensor.numpy(), index=train_df_X.index, columns=['y_hat'])\n    test_df_y_hat = pd.DataFrame(test_df_y_hat_tensor.numpy(), index=test_df_X.index, columns=['y_hat'])\n\n    return train_df_y_hat, test_df_y_hat\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m8_dnn.train_model_m8_dnn","title":"<code>train_model_m8_dnn(hyperparameter, train_df_X, train_df_y)</code>","text":"<p>Train and test a linear model for point forecasting. </p> <p>Parameters:</p> Name Type Description Default <code>hyperparameter (dict) </code> <p>hyperparameter value of the model consisting of number of features</p> required <code>train_df_X (DataFrame) </code> <p>features matrix for training</p> required <code>train_df_y (DataFrame) </code> <p>target matrix for training</p> required <p>Returns:</p> Type Description <p>model (dict) : trained model with all features</p> Source code in <code>docs\\notebooks\\model\\m8_dnn.py</code> <pre><code>def train_model_m8_dnn(hyperparameter, train_df_X, train_df_y):\n    ''' Train and test a linear model for point forecasting. \n\n    Args:\n        hyperparameter (dict) : hyperparameter value of the model consisting of number of features\n        train_df_X (DataFrame) : features matrix for training\n        train_df_y (DataFrame) : target matrix for training\n\n    Returns:\n        model (dict) : trained model with all features\n    '''\n\n    # UNPACK HYPERPARAMETER\n    seed = hyperparameter['seed']\n    torch.manual_seed(seed)  # Set seed for PyTorch\n\n    n_hidden = hyperparameter['n_hidden']\n    hidden_size = hyperparameter['hidden_size']\n    activation_function = hyperparameter['activation_function']\n    learning_rate = hyperparameter['learning_rate']\n    solver = hyperparameter['solver']\n    epochs = hyperparameter['epochs']\n\n    # Use proper format for X and y\n    X = torch.tensor(train_df_X.values, dtype=torch.float32)\n    y = torch.tensor(train_df_y.values, dtype=torch.float32).view(-1, 1) \n\n    # Define the DNN model\n    class DNNModel(nn.Module):\n        def __init__(self, input_size, hidden_size, output_size, n_hidden, activation_function):\n            super(DNNModel, self).__init__()\n            self.layers = nn.ModuleList()\n            self.activation_function = activation_function\n\n            # Input layer\n            self.layers.append(nn.Linear(input_size, hidden_size))\n\n            # Hidden layers\n            for _ in range(n_hidden - 1):\n                self.layers.append(nn.Linear(hidden_size, hidden_size))\n\n            # Output layer\n            self.layers.append(nn.Linear(hidden_size, output_size))\n\n        def forward(self, x):\n            for i, layer in enumerate(self.layers[:-1]):  # Iterate through hidden layers\n                x = layer(x)\n                if self.activation_function == 'relu':\n                    x = nn.ReLU()(x)\n                elif self.activation_function == 'sigmoid':\n                    x = torch.sigmoid(x)\n                elif self.activation_function == 'tanh':\n                    x = torch.tanh(x)\n\n            # Apply the output layer without activation function\n            x = self.layers[-1](x)\n            return x\n\n    # Model initialization\n    input_size = X.shape[1]\n    output_size = y.shape[1]\n    model_dnn = DNNModel(input_size, hidden_size, output_size, n_hidden, activation_function)\n\n    if solver == 'adam':\n        optimizer = optim.Adam(model_dnn.parameters(), lr=learning_rate)\n    elif solver == 'sgd':\n        optimizer = optim.SGD(model_dnn.parameters(), lr=learning_rate)\n    else:\n        raise ValueError('Solver not found')\n\n    # Loss function\n    criterion = nn.MSELoss()  # Mean Squared Error loss for regression\n\n    # TRAIN MODEL\n    # Training loop\n    for epoch in range(epochs):\n        model_dnn.train()\n\n        # Forward pass\n        output = model_dnn(X)\n        loss = criterion(output, y)\n\n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n\n        # Update weights\n        optimizer.step()\n\n        if epoch % 10 == 0:\n            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n\n    # PACK MODEL\n    model = {\"model_dnn\": model_dnn}\n\n    return model\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m9_rt.produce_forecast_m9_rt","title":"<code>produce_forecast_m9_rt(model, train_df_X, test_df_X)</code>","text":"<p>Create forecast at the train and test set using the trained model</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>dictionary</code> <p>all parameters of the trained model</p> required <code>train_df_X</code> <code>df</code> <p>predictors of train set</p> required <code>test_df_X</code> <code>df</code> <p>predictors of test set</p> required <p>Returns:</p> Type Description <p>train_df_y_hat (df) : forecast result at train set</p> <p>test_df_y_hat (df) : forecast result at test set</p> Source code in <code>docs\\notebooks\\model\\m9_rt.py</code> <pre><code>def produce_forecast_m9_rt(model, train_df_X, test_df_X):\n    \"\"\"Create forecast at the train and test set using the trained model\n\n    Args:\n        model (dictionary): all parameters of the trained model\n        train_df_X (df): predictors of train set\n        test_df_X (df): predictors of test set\n\n    Returns:\n        train_df_y_hat (df) : forecast result at train set\n        test_df_y_hat (df) : forecast result at test set\n\n    \"\"\"\n\n    # UNPACK MODEL\n    regressor = model['rt']\n\n    # PRODUCE FORECAST\n    train_df_y_hat = pd.DataFrame(regressor.predict(train_df_X), index = train_df_X.index, columns = ['y_hat'])\n    test_df_y_hat = pd.DataFrame(regressor.predict(test_df_X), index = test_df_X.index, columns = ['y_hat'])\n\n    # print('I am here after training the model')\n    # print('train_df_y_hat', train_df_y_hat)\n    # print('test_df_y_hat', test_df_y_hat)\n\n    return train_df_y_hat, test_df_y_hat\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m9_rt.train_model_m9_rt","title":"<code>train_model_m9_rt(hyperparameter, train_df_X, train_df_y)</code>","text":"<p>Train and test a regression tree model for point forecasting. </p> <p>Parameters:</p> Name Type Description Default <code>hyperparameter (df) </code> <p>hyperparameter value of the model consisting of number of features</p> required <code>train_df_X (df) </code> <p>features matrix for training</p> required <code>train_df_y (df) </code> <p>target matrix for training</p> required <p>Returns:</p> Type Description <p>model (model) : trained model with all features</p> Source code in <code>docs\\notebooks\\model\\m9_rt.py</code> <pre><code>def train_model_m9_rt(hyperparameter, train_df_X, train_df_y):\n    ''' Train and test a regression tree model for point forecasting. \n\n    Args:\n        hyperparameter (df) : hyperparameter value of the model consisting of number of features\n        train_df_X (df) : features matrix for training\n        train_df_y (df) : target matrix for training\n\n\n    Returns:\n        model (model) : trained model with all features\n    '''\n\n    #UNPACK HYPERPARAMETER\n    seed = hyperparameter['seed']\n    max_depth = hyperparameter['max_depth']\n    min_samples_split = hyperparameter['min_samples_split']\n    min_samples_leaf = hyperparameter['min_samples_leaf']\n    max_features = hyperparameter['max_features']\n\n    #TRAIN MODEL\n    # Initialize the regression tree model with important hyperparameters\n    regressor = DecisionTreeRegressor(\n        criterion='squared_error',\n        max_depth=max_depth,\n        min_samples_split = min_samples_split,\n        min_samples_leaf = min_samples_leaf,\n        max_features = max_features,\n        random_state = seed\n    )\n\n    # Train the model\n    regressor.fit(train_df_X, train_df_y)\n\n    # PACK MODEL\n    model = {\"rt\": regressor}\n\n    # print('I am here after training the model')\n\n    return model\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m10_rf.produce_forecast_m10_rf","title":"<code>produce_forecast_m10_rf(model, train_df_X, test_df_X)</code>","text":"<p>Create forecast at the train and test set using the trained model</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>dictionary</code> <p>all parameters of the trained model</p> required <code>train_df_X</code> <code>df</code> <p>predictors of train set</p> required <code>test_df_X</code> <code>df</code> <p>predictors of test set</p> required <p>Returns:</p> Type Description <p>train_df_y_hat (df) : forecast result at train set</p> <p>test_df_y_hat (df) : forecast result at test set</p> Source code in <code>docs\\notebooks\\model\\m10_rf.py</code> <pre><code>def produce_forecast_m10_rf(model, train_df_X, test_df_X):\n    \"\"\"Create forecast at the train and test set using the trained model\n\n    Args:\n        model (dictionary): all parameters of the trained model\n        train_df_X (df): predictors of train set\n        test_df_X (df): predictors of test set\n\n    Returns:\n        train_df_y_hat (df) : forecast result at train set\n        test_df_y_hat (df) : forecast result at test set\n\n    \"\"\"\n\n    # UNPACK MODEL\n    rf = model['rf']\n\n    # PRODUCE FORECAST\n    train_df_y_hat = pd.DataFrame(rf.predict(train_df_X), index = train_df_X.index, columns = ['y_hat'])\n    test_df_y_hat = pd.DataFrame(rf.predict(test_df_X), index = test_df_X.index, columns = ['y_hat'])\n\n    return train_df_y_hat, test_df_y_hat\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m10_rf.train_model_m10_rf","title":"<code>train_model_m10_rf(hyperparameter, train_df_X, train_df_y)</code>","text":"<p>Train and test a random forest model for point forecasting. </p> <p>Parameters:</p> Name Type Description Default <code>hyperparameter (df) </code> <p>hyperparameter value of the model consisting of number of features</p> required <code>train_df_X (df) </code> <p>features matrix for training</p> required <code>train_df_y (df) </code> <p>target matrix for training</p> required <p>Returns:</p> Type Description <p>model (model) : trained model with all features</p> Source code in <code>docs\\notebooks\\model\\m10_rf.py</code> <pre><code>def train_model_m10_rf(hyperparameter, train_df_X, train_df_y):\n    ''' Train and test a random forest model for point forecasting. \n\n    Args:\n        hyperparameter (df) : hyperparameter value of the model consisting of number of features\n        train_df_X (df) : features matrix for training\n        train_df_y (df) : target matrix for training\n\n\n    Returns:\n        model (model) : trained model with all features\n    '''\n\n    #UNPACK HYPERPARAMETER\n    seed = int(hyperparameter['seed'])\n    n_estimators = int(hyperparameter['n_estimators'])\n    max_depth = int(hyperparameter['max_depth'])\n    min_samples_split = int(hyperparameter['min_samples_split'])\n    min_samples_leaf = int(hyperparameter['min_samples_leaf'])\n\n\n    #TRAIN MODEL\n    rf = RandomForestRegressor(\n        n_estimators=n_estimators,       # number of trees\n        max_depth=max_depth,           # maximum depth of a tree\n        min_samples_split=min_samples_split,    # min samples to split a node\n        min_samples_leaf=min_samples_leaf,     # min samples in a leaf\n        random_state=seed\n    )\n\n    rf.fit(train_df_X, train_df_y) # fit the model to the training data\n\n    # PACK MODEL\n    model = {\"rf\": rf}\n\n\n    return model\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m11_svr.produce_forecast_m11_svr","title":"<code>produce_forecast_m11_svr(model, train_df_X, test_df_X)</code>","text":"<p>Create forecast at the train and test set using the trained model</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>dictionary</code> <p>all parameters of the trained model</p> required <code>train_df_X</code> <code>df</code> <p>predictors of train set</p> required <code>test_df_X</code> <code>df</code> <p>predictors of test set</p> required <p>Returns:</p> Type Description <p>train_df_y_hat (df) : forecast result at train set</p> <p>test_df_y_hat (df) : forecast result at test set</p> Source code in <code>docs\\notebooks\\model\\m11_svr.py</code> <pre><code>def produce_forecast_m11_svr(model, train_df_X, test_df_X):\n    \"\"\"Create forecast at the train and test set using the trained model\n\n    Args:\n        model (dictionary): all parameters of the trained model\n        train_df_X (df): predictors of train set\n        test_df_X (df): predictors of test set\n\n    Returns:\n        train_df_y_hat (df) : forecast result at train set\n        test_df_y_hat (df) : forecast result at test set\n\n    \"\"\"\n\n    svr = model['svr']\n    train_df_y_hat = svr.predict(train_df_X)\n    test_df_y_hat = svr.predict(test_df_X)\n\n    return train_df_y_hat, test_df_y_hat\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m11_svr.train_model_m11_svr","title":"<code>train_model_m11_svr(hyperparameter, train_df_X, train_df_y)</code>","text":"<p>Train and test a linear model for point forecasting. </p> <p>Parameters:</p> Name Type Description Default <code>hyperparameter (df) </code> <p>hyperparameter value of the model consisting of number of features</p> required <code>train_df_X (df) </code> <p>features matrix for training</p> required <code>train_df_y (df) </code> <p>target matrix for training</p> required <p>Returns:</p> Type Description <p>model (model) : trained model with all features</p> Source code in <code>docs\\notebooks\\model\\m11_svr.py</code> <pre><code>def train_model_m11_svr(hyperparameter, train_df_X, train_df_y):\n    ''' Train and test a linear model for point forecasting. \n\n    Args:\n        hyperparameter (df) : hyperparameter value of the model consisting of number of features\n        train_df_X (df) : features matrix for training\n        train_df_y (df) : target matrix for training\n\n\n    Returns:\n        model (model) : trained model with all features\n    '''\n\n    from sklearn.svm import SVR\n\n    #UNPACK HYPERPARAMETER\n    seed = hyperparameter['seed'] #seem we can't use this using sklearn\n    kernel = hyperparameter['kernel']\n    C = hyperparameter['C']\n    gamma = hyperparameter['gamma']\n    epsilon = hyperparameter['epsilon']\n\n    #TRAIN MODEL\n    train_df_y = train_df_y.values.ravel()  # Flatten the target array if necessary\n    svr = SVR(kernel=kernel, C=C, gamma=gamma, epsilon=epsilon)\n    svr.fit(train_df_X, train_df_y)\n\n    # PACK MODEL\n    model = {\"svr\": svr}\n\n\n    return model\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m12_rnn.produce_forecast_m12_rnn","title":"<code>produce_forecast_m12_rnn(model, train_df_X, test_df_X)</code>","text":"<p>Create forecast at the train and test set using the trained RNN model</p> Source code in <code>docs\\notebooks\\model\\m12_rnn.py</code> <pre><code>def produce_forecast_m12_rnn(model, train_df_X, test_df_X):\n    \"\"\"Create forecast at the train and test set using the trained RNN model\"\"\"\n\n    # UNPACK MODEL\n    rnn = model['rnn']\n    hyperparameter = model['hyperparameter']\n\n    # UNPACK HYPERPARAMETER\n    input_size = int(hyperparameter['input_size'])\n    batch_size = int(hyperparameter['batch_size'])\n\n    # PRODUCE FORECAST\n    def produce_forecast(rnn, X):\n        X_lags, X_exog = separate_lag_and_exogenous_features(X)\n        X_lags_tensor = torch.tensor(X_lags.values, dtype=torch.float32)\n        X_exog_tensor = torch.tensor(X_exog.values, dtype=torch.float32)\n\n        total_lag_features = X_lags_tensor.shape[1]\n        sequence_length = total_lag_features // input_size\n        X_lags_tensor = X_lags_tensor.view(-1, sequence_length, input_size)\n\n        predictions = []\n        for i in range(0, len(X_lags_tensor), batch_size):\n            batch_X_lags = X_lags_tensor[i:i+batch_size]\n            batch_X_exog = X_exog_tensor[i:i+batch_size]\n\n            with torch.no_grad():\n                batch_pred = rnn(batch_X_lags, batch_X_exog)\n\n            predictions.append(batch_pred)\n\n        predictions = torch.cat(predictions, dim=0)\n        return predictions.detach().numpy()\n\n    train_df_y_hat = produce_forecast(rnn, train_df_X)\n    test_df_y_hat = produce_forecast(rnn, test_df_X)\n\n    return train_df_y_hat, test_df_y_hat\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m12_rnn.separate_lag_and_exogenous_features","title":"<code>separate_lag_and_exogenous_features(train_df_X, target_column='y', lag_prefix='y_lag')</code>","text":"<p>This function separates the lag features and exogenous variables from the training dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>train_df_X</code> <code>DataFrame</code> <p>The dataframe containing both lag features and exogenous variables.</p> required <code>target_column</code> <code>str</code> <p>The name of the target column (e.g., 'y').</p> <code>'y'</code> <code>lag_prefix</code> <code>str</code> <p>The prefix used for lag columns (e.g., 'y_lag').</p> <code>'y_lag'</code> <p>Returns:</p> Name Type Description <code>X_lags</code> <code>DataFrame</code> <p>DataFrame containing only the lag features.</p> <code>X_exog</code> <code>DataFrame</code> <p>DataFrame containing only the exogenous variables.</p> Source code in <code>docs\\notebooks\\model\\m12_rnn.py</code> <pre><code>def separate_lag_and_exogenous_features(train_df_X, target_column='y', lag_prefix='y_lag'):\n    '''\n    This function separates the lag features and exogenous variables from the training dataframe.\n\n    Args:\n        train_df_X (pd.DataFrame): The dataframe containing both lag features and exogenous variables.\n        target_column (str): The name of the target column (e.g., 'y').\n        lag_prefix (str): The prefix used for lag columns (e.g., 'y_lag').\n\n    Returns:\n        X_lags (pd.DataFrame): DataFrame containing only the lag features.\n        X_exog (pd.DataFrame): DataFrame containing only the exogenous variables.\n    '''\n\n    # Identify lag features (columns that start with 'y_lag')\n    lag_features = [col for col in train_df_X.columns if col.startswith(lag_prefix)]\n\n    # Identify exogenous variables (everything except the target and lag features)\n    exog_features = [col for col in train_df_X.columns if col not in [target_column] + lag_features]\n\n    # Create dataframes for lag features and exogenous features\n    X_lags = train_df_X[lag_features]\n    X_exog = train_df_X[exog_features]\n\n    return X_lags, X_exog\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m12_rnn.train_model_m12_rnn","title":"<code>train_model_m12_rnn(hyperparameter, train_df_X, train_df_y)</code>","text":"<p>Train and test an RNN model for point forecasting.  Essentially we use the RNN block to learn the temporal patterns of the time series,  and then use a fully connected layer to learn the relationship between the lag features. We take the last hidden state of the RNN as the output, and concatenate it with the exogenous features  (like calendar) to make the final prediction using a fully connected layer.</p> Source code in <code>docs\\notebooks\\model\\m12_rnn.py</code> <pre><code>def train_model_m12_rnn(hyperparameter, train_df_X, train_df_y):\n    ''' Train and test an RNN model for point forecasting. \n    Essentially we use the RNN block to learn the temporal patterns of the time series, \n    and then use a fully connected layer to learn the relationship between the lag features.\n    We take the last hidden state of the RNN as the output, and concatenate it with the exogenous features \n    (like calendar) to make the final prediction using a fully connected layer.\n    '''\n\n    # UNPACK HYPERPARAMETER\n    seed = int(hyperparameter['seed'])\n    input_size = int(hyperparameter['input_size'])\n    hidden_size = int(hyperparameter['hidden_size'])\n    num_layers = int(hyperparameter['num_layers'])\n    output_size = int(hyperparameter['output_size'])\n    batch_size = int(hyperparameter['batch_size'])\n    epochs = int(hyperparameter['epochs'])\n    learning_rate = hyperparameter['learning_rate']\n\n    # DEFINE MODEL AND TRAINING FUNCTION\n    class RNNModel(nn.Module):\n        def __init__(self, input_size, hidden_size, num_layers, exog_size, output_size=1):\n            super(RNNModel, self).__init__()\n\n            # Define the RNN layer\n            self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n\n            # Define the Fully Connected (FC) layer\n            self.fc = nn.Linear(hidden_size + exog_size, output_size)\n\n        def forward(self, x, exogenous_data):\n            # Pass the input through the RNN\n            out, h_n = self.rnn(x)\n\n            # Get the last timestep hidden state\n            last_hidden_state = out[:, -1, :]  # Shape: (batch_size, hidden_size)\n\n            # Concatenate hidden state with exogenous vars\n            combined_input = torch.cat((last_hidden_state, exogenous_data), dim=1)\n\n            # Pass through FC\n            out = self.fc(combined_input)\n            return out\n\n    def train_rnn_with_minibatches(model, train_loader, epochs, learning_rate=0.001):\n        criterion = nn.MSELoss()\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n        for epoch in range(epochs):\n            print(f'Epoch [{epoch+1}/{epochs}]')\n            start_time = time.time()\n\n            model.train()\n            batch_no = 1\n            for X_lags_batch, X_exog_batch, y_batch in train_loader:\n                print(f'Epoch [{epoch+1}/{epochs}] and batch [{batch_no}/{len(train_loader)}]')\n                batch_no += 1\n\n                # Forward pass\n                predictions = model(X_lags_batch, X_exog_batch)\n                loss = criterion(predictions, y_batch)\n\n                # Backward pass\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n            end_time = time.time()\n            epoch_time = end_time - start_time\n            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}, time taken: {epoch_time:.2f} seconds')\n\n    def set_seed(seed=seed):\n        random.seed(seed)\n        np.random.seed(seed)\n        torch.manual_seed(seed)\n        os.environ[\"PYTHONHASHSEED\"] = str(seed)\n\n    # PREPARE TRAIN DATA\n    X_lags, X_exog = separate_lag_and_exogenous_features(train_df_X)\n    X_lags_tensor = torch.tensor(X_lags.values, dtype=torch.float32)\n    X_exog_tensor = torch.tensor(X_exog.values, dtype=torch.float32)\n    y_tensor = torch.tensor(train_df_y.values, dtype=torch.float32).view(-1, 1)\n\n    total_lag_features = X_lags_tensor.shape[1]\n    sequence_length = total_lag_features // input_size\n    exog_size = X_exog_tensor.shape[1]\n\n    # Reshape to 3D\n    X_lags_tensor = X_lags_tensor.view(-1, sequence_length, input_size)\n\n    # INITIALIZE MODEL + DATALOADER\n    set_seed(seed=seed)\n    rnn = RNNModel(input_size, hidden_size, num_layers, exog_size, output_size)\n    train_data = TensorDataset(X_lags_tensor, X_exog_tensor, y_tensor)\n    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n\n    # TRAIN MODEL\n    train_rnn_with_minibatches(rnn, train_loader, epochs=epochs, learning_rate=learning_rate)\n\n    # PACK MODEL\n    model = {\"rnn\": rnn, 'hyperparameter': hyperparameter, \"train_df_X\": train_df_X, \"train_df_y\": train_df_y}\n    return model\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m13_lstm.produce_forecast_m13_lstm","title":"<code>produce_forecast_m13_lstm(model, train_df_X, test_df_X)</code>","text":"<p>Create forecast at the train and test set using the trained model</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>dictionary</code> <p>all parameters of the trained model</p> required <code>train_df_X</code> <code>df</code> <p>predictors of train set</p> required <code>test_df_X</code> <code>df</code> <p>predictors of test set</p> required <p>Returns:</p> Type Description <p>train_df_y_hat (df) : forecast result at train set</p> <p>test_df_y_hat (df) : forecast result at test set</p> Source code in <code>docs\\notebooks\\model\\m13_lstm.py</code> <pre><code>def produce_forecast_m13_lstm(model, train_df_X, test_df_X):\n    \"\"\"Create forecast at the train and test set using the trained model\n\n    Args:\n        model (dictionary): all parameters of the trained model\n        train_df_X (df): predictors of train set\n        test_df_X (df): predictors of test set\n\n    Returns:\n        train_df_y_hat (df) : forecast result at train set\n        test_df_y_hat (df) : forecast result at test set\n\n    \"\"\"\n\n    # UNPACK MODEL\n    lstm = model['lstm']\n    hyperparameter = model['hyperparameter']\n\n    #UNPACK HYPERPARAMETER\n    seed = int(hyperparameter['seed'])\n    input_size = int(hyperparameter['input_size']) #this is one since we only use lag features to be fed into the LSTM. The exogenous features like calenndar are fed to the fully connected layer, together with the last hidden state of LSTM.\n    hidden_size = int(hyperparameter['hidden_size']) #this is the size of hidden state, and we aim to use many to one architecture. Meaning we only take the last hidden state as output, and fed into the fully connected layer.\n    num_layers = int(hyperparameter['num_layers']) # we use 1 by default to make it simple. \n    output_size = int(hyperparameter['output_size']) #this is one since we only predict one value.\n    batch_size = int(hyperparameter['batch_size']) #using minibatch is important cuz if we train all samples at once, the memory is not enough.\n    epochs = int(hyperparameter['epochs'])\n    learning_rate = hyperparameter['learning_rate']  # No change for learning rate\n\n    # PRODUCE FORECAST\n    def produce_forecast(lstm, X):\n        # Convert X into X_lag and X_exog\n        X_lags, X_exog = separate_lag_and_exogenous_features(X)\n        X_lags_tensor = torch.tensor(X_lags.values, dtype=torch.float32)  # Shape: (batch_size, sequence_length, input_size)\n        X_exog_tensor = torch.tensor(X_exog.values, dtype=torch.float32)  # Shape: (batch_size, exog_size)\n        # y_tensor = torch.tensor(train_df_y.values, dtype=torch.float32).view(-1, 1) to be deleted.\n\n        total_lag_features = X_lags_tensor.shape[1]  # Number of lag features (columns)\n        sequence_length = total_lag_features // input_size\n        exog_size = X_exog_tensor.shape[1]  # Number of exogenous features\n\n        # Reshaping X_lags_tensor to 3D: (batch_size, sequence_length, input_size)\n        X_lags_tensor = X_lags_tensor.view(-1, sequence_length, input_size)\n\n        #predictions = lstm(X_lags_tensor, X_exog_tensor) #this doesn't work because of the batch size is too big, not enough memory.\n        predictions = []\n        for i in range(0, len(X_lags_tensor), batch_size):\n            # Get the current minibatch for both X_lags_tensor and X_exog_tensor\n            batch_X_lags = X_lags_tensor[i:i+batch_size]\n            batch_X_exog = X_exog_tensor[i:i+batch_size]\n\n            with torch.no_grad():\n                # Make predictions for the minibatch\n                batch_pred = lstm(batch_X_lags, batch_X_exog)\n\n            # Store the predictions for the current batch\n            predictions.append(batch_pred)\n\n        # Concatenate all predictions to get the full result\n        predictions = torch.cat(predictions, dim=0)\n\n\n        return predictions.detach().numpy()\n\n    train_df_y_hat = produce_forecast(lstm, train_df_X)\n    test_df_y_hat = produce_forecast(lstm, test_df_X)\n\n    return train_df_y_hat, test_df_y_hat\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m13_lstm.separate_lag_and_exogenous_features","title":"<code>separate_lag_and_exogenous_features(train_df_X, target_column='y', lag_prefix='y_lag')</code>","text":"<p>This function separates the lag features and exogenous variables from the training dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>train_df_X</code> <code>DataFrame</code> <p>The dataframe containing both lag features and exogenous variables.</p> required <code>target_column</code> <code>str</code> <p>The name of the target column (e.g., 'y').</p> <code>'y'</code> <code>lag_prefix</code> <code>str</code> <p>The prefix used for lag columns (e.g., 'y_lag').</p> <code>'y_lag'</code> <p>Returns:</p> Name Type Description <code>X_lags</code> <code>DataFrame</code> <p>DataFrame containing only the lag features.</p> <code>X_exog</code> <code>DataFrame</code> <p>DataFrame containing only the exogenous variables.</p> Source code in <code>docs\\notebooks\\model\\m13_lstm.py</code> <pre><code>def separate_lag_and_exogenous_features(train_df_X, target_column='y', lag_prefix='y_lag'):\n    '''\n    This function separates the lag features and exogenous variables from the training dataframe.\n\n    Args:\n        train_df_X (pd.DataFrame): The dataframe containing both lag features and exogenous variables.\n        target_column (str): The name of the target column (e.g., 'y').\n        lag_prefix (str): The prefix used for lag columns (e.g., 'y_lag').\n\n    Returns:\n        X_lags (pd.DataFrame): DataFrame containing only the lag features.\n        X_exog (pd.DataFrame): DataFrame containing only the exogenous variables.\n    '''\n\n    # Identify lag features (columns that start with 'y_lag')\n    lag_features = [col for col in train_df_X.columns if col.startswith(lag_prefix)]\n\n    # Identify exogenous variables (everything except the target and lag features)\n    exog_features = [col for col in train_df_X.columns if col not in [target_column] + lag_features]\n\n    # Create dataframes for lag features and exogenous features\n    X_lags = train_df_X[lag_features]\n    X_exog = train_df_X[exog_features]\n\n    return X_lags, X_exog\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m13_lstm.train_model_m13_lstm","title":"<code>train_model_m13_lstm(hyperparameter, train_df_X, train_df_y)</code>","text":"<p>Train and test an LSTM model for point forecasting.  Essentially we use the LSTM block to learn the temporal patterns of the time series, and then use a fully connected layer to learn the relationship between the lag features. We take the last hidden state of the LSTM as the output, and concatenate it with the exogenous features (like calendar) to make the final prediction using a fully connected layer. Future imporvement maybe to improve the architecture of the fully connected layer after LSTM. </p> <p>Parameters:</p> Name Type Description Default <code>hyperparameter (df) </code> <p>hyperparameter value of the model consisting of number of features</p> required <code>train_df_X (df) </code> <p>features matrix for training</p> required <code>train_df_y (df) </code> <p>target matrix for training</p> required <p>Returns:</p> Type Description <p>model (model) : trained model with all features</p> Source code in <code>docs\\notebooks\\model\\m13_lstm.py</code> <pre><code>def train_model_m13_lstm(hyperparameter, train_df_X, train_df_y):\n    ''' Train and test an LSTM model for point forecasting. \n    Essentially we use the LSTM block to learn the temporal patterns of the time series, and then use a fully connected layer to learn the relationship between the lag features.\n    We take the last hidden state of the LSTM as the output, and concatenate it with the exogenous features (like calendar) to make the final prediction using a fully connected layer.\n    Future imporvement maybe to improve the architecture of the fully connected layer after LSTM. \n\n    Args:\n        hyperparameter (df) : hyperparameter value of the model consisting of number of features\n        train_df_X (df) : features matrix for training\n        train_df_y (df) : target matrix for training\n\n\n    Returns:\n        model (model) : trained model with all features\n    '''\n\n    #UNPACK HYPERPARAMETER\n    seed = int(hyperparameter['seed'])\n    input_size = int(hyperparameter['input_size']) #this is one since we only use lag features to be fed into the LSTM. The exogenous features like calenndar are fed to the fully connected layer, together with the last hidden state of LSTM.\n    hidden_size = int(hyperparameter['hidden_size']) #this is the size of hidden state, and we aim to use many to one architecture. Meaning we only take the last hidden state as output, and fed into the fully connected layer.\n    num_layers = int(hyperparameter['num_layers']) # we use 1 by default to make it simple. \n    output_size = int(hyperparameter['output_size']) #this is one since we only predict one value.\n    batch_size = int(hyperparameter['batch_size']) #using minibatch is important cuz if we train all samples at once, the memory is not enough.\n    epochs = int(hyperparameter['epochs'])\n    learning_rate = hyperparameter['learning_rate']  # No change for learning rate\n\n\n    #DEFINE MODEL AND TRAINING FUNCTION\n    class LSTMModel(nn.Module):\n        def __init__(self, input_size, hidden_size, num_layers, exog_size, output_size=1):\n            super(LSTMModel, self).__init__()\n\n            # Define the LSTM layer\n            self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n\n            # Define the Fully Connected (FC) layer\n            # The FC layer input size is the concatenation of LSTM output and exogenous variables\n            self.fc = nn.Linear(hidden_size + exog_size, output_size)  # exog_size is the number of exogenous features\n\n        def forward(self, x, exogenous_data):\n            # Pass the input through the LSTM\n            out, (h_n, c_n) = self.lstm(x)\n\n            # Get the last timestep hidden state (h3)\n            last_hidden_state = out[:, -1, :]  # Shape: (batch_size, hidden_size)\n\n            # Concatenate the LSTM output (h3) with the exogenous variables (for timestep t+100)\n            combined_input = torch.cat((last_hidden_state, exogenous_data), dim=1)  # Shape: (batch_size, hidden_size + exog_size)\n\n            # Pass the combined input through the FC layer\n            out = self.fc(combined_input)\n            return out\n\n    def train_lstm_with_minibatches(model, train_loader, epochs, learning_rate=0.001):\n        # Define the loss function (Mean Squared Error)\n        criterion = nn.MSELoss()\n\n        # Define the optimizer (Adam)\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n        for epoch in range(epochs):\n            print(f'Epoch [{epoch+1}/{epochs}]')\n            start_time = time.time()\n\n            model.train()  # Set model to training mode\n            # print(f'I am here')\n\n            # Iterate over mini-batches\n            batch_no = 1\n            for X_lags_batch, X_exog_batch, y_batch in train_loader:\n                # print(f'I am here now')\n                # Print the loss and time taken for this epoch\n                print(f'Epoch [{epoch+1}/{epochs}] and batch [{batch_no}/{len(train_loader)}]')\n                batch_no += 1\n                # Forward pass\n                predictions = model(X_lags_batch, X_exog_batch)\n                loss = criterion(predictions, y_batch)\n\n                # Backward pass\n                optimizer.zero_grad()  # Zero gradients from previous step\n                loss.backward()  # Backpropagate the error\n                optimizer.step()  # Update the model's weights\n\n\n\n            end_time = time.time()\n            epoch_time = end_time - start_time\n            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}, time taken: {epoch_time:.2f} seconds')\n\n    def set_seed(seed=seed):\n        random.seed(seed)\n        np.random.seed(seed)\n        torch.manual_seed(seed)\n        os.environ[\"PYTHONHASHSEED\"] = str(seed)\n\n    # PREPARE TRAIN DATA\n    # SEPARATE LAG AND EXOGENOUS FEATURES\n    X_lags, X_exog = separate_lag_and_exogenous_features(train_df_X)\n    X_lags_tensor = torch.tensor(X_lags.values, dtype=torch.float32)  # Shape: (batch_size, sequence_length, input_size)\n    X_exog_tensor = torch.tensor(X_exog.values, dtype=torch.float32)  # Shape: (batch_size, exog_size)\n    y_tensor = torch.tensor(train_df_y.values, dtype=torch.float32).view(-1, 1)\n\n    total_lag_features = X_lags_tensor.shape[1]  # Number of lag features (columns)\n    sequence_length = total_lag_features // input_size\n    exog_size = X_exog_tensor.shape[1]  # Number of exogenous features\n\n    # Reshaping X_lags_tensor to 3D: (batch_size, sequence_length, input_size)\n    X_lags_tensor = X_lags_tensor.view(-1, sequence_length, input_size)\n\n\n    # INITIALIZE MODEL and MAKE TRAINING BATCHES\n    set_seed(seed = seed) # Set random seed for reproducibility\n    lstm = LSTMModel(input_size, hidden_size, num_layers, exog_size, output_size)\n    # Create a TensorDataset with your features and target\n    train_data = TensorDataset(X_lags_tensor, X_exog_tensor, y_tensor)\n    # Create a DataLoader to handle mini-batching\n    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n\n    # TRAIN MODEL\n    train_lstm_with_minibatches(lstm, train_loader, epochs=epochs, learning_rate=learning_rate)\n\n\n    # PACK MODEL\n    model = {\"lstm\": lstm, 'hyperparameter': hyperparameter, \"train_df_X\": train_df_X, \"train_df_y\": train_df_y}\n\n\n    return model\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m14_gru.produce_forecast_m14_gru","title":"<code>produce_forecast_m14_gru(model, train_df_X, test_df_X)</code>","text":"<p>Create forecast at the train and test set using the trained GRU model Args:     model (dictionary): all parameters of the trained model     train_df_X (df): predictors of train set     test_df_X (df): predictors of test set</p> <p>Returns:</p> Type Description <p>train_df_y_hat (df) : forecast result at train set</p> <p>test_df_y_hat (df) : forecast result at test set</p> Source code in <code>docs\\notebooks\\model\\m14_gru.py</code> <pre><code>def produce_forecast_m14_gru(model, train_df_X, test_df_X):\n    \"\"\"Create forecast at the train and test set using the trained GRU model\n    Args:\n        model (dictionary): all parameters of the trained model\n        train_df_X (df): predictors of train set\n        test_df_X (df): predictors of test set\n\n    Returns:\n        train_df_y_hat (df) : forecast result at train set\n        test_df_y_hat (df) : forecast result at test set\n    \"\"\"\n    gru = model['gru']\n    hyperparameter = model['hyperparameter']\n    input_size = int(hyperparameter['input_size'])\n    batch_size = int(hyperparameter['batch_size'])\n\n    def produce_forecast(gru, X):\n        X_lags, X_exog = separate_lag_and_exogenous_features(X)\n        X_lags_tensor = torch.tensor(X_lags.values, dtype=torch.float32)\n        X_exog_tensor = torch.tensor(X_exog.values, dtype=torch.float32)\n\n        total_lag_features = X_lags_tensor.shape[1]\n        sequence_length = total_lag_features // input_size\n        X_lags_tensor = X_lags_tensor.view(-1, sequence_length, input_size)\n\n        predictions = []\n        for i in range(0, len(X_lags_tensor), batch_size):\n            batch_X_lags = X_lags_tensor[i:i+batch_size]\n            batch_X_exog = X_exog_tensor[i:i+batch_size]\n            with torch.no_grad():\n                batch_pred = gru(batch_X_lags, batch_X_exog)\n            predictions.append(batch_pred)\n        return torch.cat(predictions, dim=0).detach().numpy()\n\n    train_df_y_hat = produce_forecast(gru, train_df_X)\n    test_df_y_hat = produce_forecast(gru, test_df_X)\n    return train_df_y_hat, test_df_y_hat\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m14_gru.train_model_m14_gru","title":"<code>train_model_m14_gru(hyperparameter, train_df_X, train_df_y)</code>","text":"<p>Train and test a GRU model for point forecasting.  Uses GRU for temporal patterns, FC layer for lag+exogenous features. Args:     hyperparameter (df) : hyperparameter value of the model consisting of number of features     train_df_X (df) : features matrix for training     train_df_y (df) : target matrix for training</p> <p>Returns:</p> Type Description <p>model (model) : trained model with all features</p> Source code in <code>docs\\notebooks\\model\\m14_gru.py</code> <pre><code>def train_model_m14_gru(hyperparameter, train_df_X, train_df_y):\n    ''' Train and test a GRU model for point forecasting. \n    Uses GRU for temporal patterns, FC layer for lag+exogenous features.\n    Args:\n        hyperparameter (df) : hyperparameter value of the model consisting of number of features\n        train_df_X (df) : features matrix for training\n        train_df_y (df) : target matrix for training\n\n\n    Returns:\n        model (model) : trained model with all features\n    '''\n\n    # UNPACK HYPERPARAMETER\n    seed = int(hyperparameter['seed'])\n    input_size = int(hyperparameter['input_size'])\n    hidden_size = int(hyperparameter['hidden_size'])\n    num_layers = int(hyperparameter['num_layers'])\n    output_size = int(hyperparameter['output_size'])\n    batch_size = int(hyperparameter['batch_size'])\n    epochs = int(hyperparameter['epochs'])\n    learning_rate = hyperparameter['learning_rate']\n\n    # DEFINE MODEL\n    class GRUModel(nn.Module):\n        def __init__(self, input_size, hidden_size, num_layers, exog_size, output_size=1):\n            super(GRUModel, self).__init__()\n            self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n            self.fc = nn.Linear(hidden_size + exog_size, output_size)\n\n        def forward(self, x, exogenous_data):\n            out, h_n = self.gru(x)\n            last_hidden_state = out[:, -1, :]\n            combined_input = torch.cat((last_hidden_state, exogenous_data), dim=1)\n            out = self.fc(combined_input)\n            return out\n\n    def train_gru_with_minibatches(model, train_loader, epochs, learning_rate=0.001):\n        criterion = nn.MSELoss()\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n        for epoch in range(epochs):\n            print(f'Epoch [{epoch+1}/{epochs}]')\n            start_time = time.time()\n            model.train()\n            batch_no = 1\n            for X_lags_batch, X_exog_batch, y_batch in train_loader:\n                print(f'Epoch [{epoch+1}/{epochs}] and batch [{batch_no}/{len(train_loader)}]')\n                batch_no += 1\n\n                predictions = model(X_lags_batch, X_exog_batch)\n                loss = criterion(predictions, y_batch)\n\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}, time taken: {time.time() - start_time:.2f}s')\n\n    def set_seed(seed=seed):\n        random.seed(seed)\n        np.random.seed(seed)\n        torch.manual_seed(seed)\n        os.environ[\"PYTHONHASHSEED\"] = str(seed)\n\n    # PREPARE TRAIN DATA\n    X_lags, X_exog = separate_lag_and_exogenous_features(train_df_X)\n    X_lags_tensor = torch.tensor(X_lags.values, dtype=torch.float32)\n    X_exog_tensor = torch.tensor(X_exog.values, dtype=torch.float32)\n    y_tensor = torch.tensor(train_df_y.values, dtype=torch.float32).view(-1, 1)\n\n    total_lag_features = X_lags_tensor.shape[1]\n    sequence_length = total_lag_features // input_size\n    exog_size = X_exog_tensor.shape[1]\n\n    X_lags_tensor = X_lags_tensor.view(-1, sequence_length, input_size)\n\n    # INITIALIZE MODEL + DATALOADER\n    set_seed(seed=seed)\n    gru = GRUModel(input_size, hidden_size, num_layers, exog_size, output_size)\n    train_data = TensorDataset(X_lags_tensor, X_exog_tensor, y_tensor)\n    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n\n    # TRAIN MODEL\n    train_gru_with_minibatches(gru, train_loader, epochs=epochs, learning_rate=learning_rate)\n\n    # PACK MODEL\n    model = {\"gru\": gru, 'hyperparameter': hyperparameter, \"train_df_X\": train_df_X, \"train_df_y\": train_df_y}\n    return model\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m15_transformer.produce_forecast_m15_transformer","title":"<code>produce_forecast_m15_transformer(model, train_df_X, test_df_X)</code>","text":"<p>Create forecast at the train and test set using the trained Transformer model Args:     model (dictionary): all parameters of the trained model     train_df_X (df): predictors of train set     test_df_X (df): predictors of test set</p> <p>Returns:</p> Type Description <p>train_df_y_hat (df) : forecast result at train set</p> <p>test_df_y_hat (df) : forecast result at test set</p> Source code in <code>docs\\notebooks\\model\\m15_transformer.py</code> <pre><code>def produce_forecast_m15_transformer(model, train_df_X, test_df_X):\n    \"\"\"Create forecast at the train and test set using the trained Transformer model\n    Args:\n        model (dictionary): all parameters of the trained model\n        train_df_X (df): predictors of train set\n        test_df_X (df): predictors of test set\n\n    Returns:\n        train_df_y_hat (df) : forecast result at train set\n        test_df_y_hat (df) : forecast result at test set\n    \"\"\"\n    transformer = model['transformer']\n    hyperparameter = model['hyperparameter']\n    batch_size = int(hyperparameter['batch_size'])\n    input_size = int(hyperparameter['input_size'])\n\n    def produce_forecast(transformer, X):\n        X_lags, X_exog = separate_lag_and_exogenous_features(X)\n        X_lags_tensor = torch.tensor(X_lags.values, dtype=torch.float32)\n        X_exog_tensor = torch.tensor(X_exog.values, dtype=torch.float32)\n        total_lag_features = X_lags_tensor.shape[1]\n        sequence_length = total_lag_features // input_size\n        X_lags_tensor = X_lags_tensor.view(-1, sequence_length, input_size)\n        predictions = []\n        for i in range(0, len(X_lags_tensor), batch_size):\n            batch_X_lags = X_lags_tensor[i:i+batch_size]\n            batch_X_exog = X_exog_tensor[i:i+batch_size]\n            with torch.no_grad():\n                batch_pred = transformer(batch_X_lags, batch_X_exog)\n            predictions.append(batch_pred)\n        return torch.cat(predictions, dim=0).detach().numpy()\n\n    train_df_y_hat = produce_forecast(transformer, train_df_X)\n    test_df_y_hat = produce_forecast(transformer, test_df_X)\n    return train_df_y_hat, test_df_y_hat\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m15_transformer.train_model_m15_transformer","title":"<code>train_model_m15_transformer(hyperparameter, train_df_X, train_df_y)</code>","text":"<p>Train and test a Transformer model for point forecasting.  Uses Transformer for temporal patterns, FC layer for lag+exogenous features. Args:     hyperparameter (df) : hyperparameter value of the model consisting of number of features     train_df_X (df) : features matrix for training     train_df_y (df) : target matrix for training</p> <p>Returns:</p> Type Description <p>model (model) : trained model with all features</p> Source code in <code>docs\\notebooks\\model\\m15_transformer.py</code> <pre><code>def train_model_m15_transformer(hyperparameter, train_df_X, train_df_y):\n    ''' Train and test a Transformer model for point forecasting. \n    Uses Transformer for temporal patterns, FC layer for lag+exogenous features.\n    Args:\n        hyperparameter (df) : hyperparameter value of the model consisting of number of features\n        train_df_X (df) : features matrix for training\n        train_df_y (df) : target matrix for training\n\n    Returns:\n        model (model) : trained model with all features\n    '''\n\n    # UNPACK HYPERPARAMETER\n    seed = int(hyperparameter['seed'])\n    input_size = int(hyperparameter['input_size'])\n    hidden_size = int(hyperparameter['hidden_size'])\n    num_layers = int(hyperparameter['num_layers'])\n    output_size = int(hyperparameter['output_size'])\n    batch_size = int(hyperparameter['batch_size'])\n    epochs = int(hyperparameter['epochs'])\n    nhead = int(hyperparameter['nhead'])\n    learning_rate = hyperparameter['learning_rate']\n\n    import torch\n    import torch.nn as nn\n    import torch.optim as optim\n    import random, numpy as np, os, time\n    from torch.utils.data import DataLoader, TensorDataset\n\n    # TRANSFORMER MODEL\n    class TransformerModel(nn.Module):\n        def __init__(self, input_size, hidden_size, num_layers, exog_size, output_size=1):\n            super(TransformerModel, self).__init__()\n            # Transformer embedding\n            self.embedding = nn.Linear(input_size, hidden_size)\n            encoder_layer = nn.TransformerEncoderLayer(\n                d_model=hidden_size,\n                nhead=nhead,\n                dim_feedforward=hidden_size * 2,\n                batch_first=True\n            )\n            self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n            # Fully connected output layer\n            self.fc = nn.Linear(hidden_size + exog_size, output_size)\n\n        def forward(self, x, exogenous_data):\n            x = self.embedding(x) \n            x = self.transformer_encoder(x)\n            last_hidden_state = x[:, -1, :]\n            combined_input = torch.cat((last_hidden_state, exogenous_data), dim=1)\n            out = self.fc(combined_input)\n            return out\n\n    def train_transformer_with_minibatches(model, train_loader, epochs, learning_rate=learning_rate):\n        criterion = nn.MSELoss()\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n        for epoch in range(epochs):\n            print(f'Epoch [{epoch+1}/{epochs}]')\n            start_time = time.time()\n            model.train()\n            batch_no = 1\n            for X_lags_batch, X_exog_batch, y_batch in train_loader:\n                print(f'Epoch [{epoch+1}/{epochs}] batch [{batch_no}/{len(train_loader)}]')\n                batch_no += 1\n                predictions = model(X_lags_batch, X_exog_batch)\n                loss = criterion(predictions, y_batch)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            end_time = time.time()\n            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}, time: {end_time - start_time:.2f}s')\n\n    def set_seed(seed=seed):\n        random.seed(seed)\n        np.random.seed(seed)\n        torch.manual_seed(seed)\n        os.environ[\"PYTHONHASHSEED\"] = str(seed)\n\n    # --- DATA PREP ---\n    X_lags, X_exog = separate_lag_and_exogenous_features(train_df_X)\n    X_lags_tensor = torch.tensor(X_lags.values, dtype=torch.float32)\n    X_exog_tensor = torch.tensor(X_exog.values, dtype=torch.float32)\n    y_tensor = torch.tensor(train_df_y.values, dtype=torch.float32).view(-1, 1)\n    total_lag_features = X_lags_tensor.shape[1]\n    sequence_length = total_lag_features // input_size\n    exog_size = X_exog_tensor.shape[1]\n    X_lags_tensor = X_lags_tensor.view(-1, sequence_length, input_size)\n\n    # --- INIT MODEL AND DATALOADER ---\n    set_seed(seed)\n    transformer = TransformerModel(input_size, hidden_size, num_layers, exog_size, output_size)\n    train_data = TensorDataset(X_lags_tensor, X_exog_tensor, y_tensor)\n    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n    train_transformer_with_minibatches(transformer, train_loader, epochs=epochs, learning_rate=learning_rate)\n\n    model = {\"transformer\": transformer, 'hyperparameter': hyperparameter, \"train_df_X\": train_df_X, \"train_df_y\": train_df_y}\n    return model\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m16_prophet.produce_forecast_m16_prophet","title":"<code>produce_forecast_m16_prophet(model, train_df_X, test_df_X, train_df_y, forecast_horizon)</code>","text":"<p>Create forecast at the train and test set using the trained model</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>dictionary</code> <p>all parameters of the trained model</p> required <code>train_df_X</code> <code>df</code> <p>predictors of train set</p> required <code>test_df_X</code> <code>df</code> <p>predictors of test set</p> required <code>train_df_y</code> <code>df</code> <p>target of train set</p> required <code>forecast_horizon</code> <code>int</code> <p>forecast horizon for the model</p> required <p>Returns:</p> Type Description <p>train_df_y_hat (df) : forecast result at train set</p> <p>test_df_y_hat (df) : forecast result at test set</p> Source code in <code>docs\\notebooks\\model\\m16_prophet.py</code> <pre><code>def produce_forecast_m16_prophet(model, train_df_X, test_df_X, train_df_y, forecast_horizon):\n    \"\"\"Create forecast at the train and test set using the trained model\n\n    Args:\n        model (dictionary): all parameters of the trained model\n        train_df_X (df): predictors of train set\n        test_df_X (df): predictors of test set\n        train_df_y (df): target of train set\n        forecast_horizon (int): forecast horizon for the model\n\n    Returns:\n        train_df_y_hat (df) : forecast result at train set\n        test_df_y_hat (df) : forecast result at test set\n\n    \"\"\"\n\n    # UNPACK MODEL\n    prophet_model = model['prophet']\n    y = model['y']\n    hyperparameter = model['hyperparameter']\n\n    #UNPACK HYPERPARAMETER\n    seasonality_prior_scale = hyperparameter[\"seasonality_prior_scale\"]\n    seasonality_mode = hyperparameter[\"seasonality_mode\"]\n    weekly_seasonality = hyperparameter[\"weekly_seasonality\"]\n    daily_seasonality = hyperparameter[\"daily_seasonality\"]\n    growth = hyperparameter[\"growth\"]\n\n    # Set up X_exog which is used for prediction\n    timestep_frequency = test_df_X.index[1] - test_df_X.index[0]\n    n_timestep_forecast_horizon = int(forecast_horizon / (timestep_frequency.total_seconds() / 60))\n\n    train_df_X_updated = remove_jump_df(train_df_X)\n    test_df_X_updated = remove_jump_df(test_df_X)\n\n    X_lags, X_exog = separate_lag_and_exogenous_features(train_df_X_updated)\n\n    X_exog.reset_index(inplace=True)\n    X_exog.rename(columns={'datetime': 'ds'}, inplace=True)\n\n    # Forecast train set\n    train_df_y_hat = prophet_model.predict(X_exog)\n\n    train_df_y_hat = train_df_y_hat[['ds', 'yhat']]\n\n    train_df_y_hat.set_index('ds', inplace=True)\n    train_df_y_hat.index.name = 'datetime'\n\n    # Set up function to warm start the model for updating the fit\n    def warm_start_params(m):\n        \"\"\"\n        Retrieve parameters from a trained model in the format used to initialize a new Stan model.\n        Note that the new Stan model must have these same settings:\n            n_changepoints, seasonality features, mcmc sampling\n        for the retrieved parameters to be valid for the new model.\n\n        Parameters\n        ----------\n        m: A trained model of the Prophet class.\n\n        Returns\n        -------\n        A Dictionary containing retrieved parameters of m.\n        \"\"\"\n        res = {}\n        for pname in ['k', 'm', 'sigma_obs']:\n            if m.mcmc_samples == 0:\n                res[pname] = m.params[pname][0][0]\n            else:\n                res[pname] = np.mean(m.params[pname])\n        for pname in ['delta', 'beta']:\n            if m.mcmc_samples == 0:\n                res[pname] = m.params[pname][0]\n            else:\n                res[pname] = np.mean(m.params[pname], axis=0)\n        return res\n\n    # PRODUCE FORECASTFOR TEST SET\n\n    # REFIT THE MODEL AND PRODUCE NEW FORECAST FOR TEST SET\n    # The model is refitted for 100 times only so there will be only 100 forecast results. \n\n    test_df_y_hat = pd.DataFrame(index = test_df_X.index)\n    test_df_y_hat['y_hat'] = np.nan\n\n\n    # in the case of CV 10, which is when test df &lt; train df\n    # don't compute the test forecast\n    if (test_df_X.index[-1] &lt; train_df_X.index[0]):\n    # this is the case when we use CV10, where the test set is before the train set\n        print(\"Test set is before train set / CV 10, no test forecast can be made\")\n        return train_df_y_hat, test_df_y_hat\n\n    _, X_test = separate_lag_and_exogenous_features(test_df_X)\n    X_test.reset_index(inplace=True)\n    X_test.rename(columns={'datetime': 'ds'}, inplace=True)\n\n    n_update = 100\n    n_timesteps_per_update = int(len(test_df_y_hat) / (n_update + 1))\n\n    # TRANSFORM test_df_X to a series with only the last lag\n    horizon_timedelta = pd.Timedelta(minutes=forecast_horizon)\n    last_observation = f'y_lag_{horizon_timedelta}m'\n    test_df_y_last = test_df_X[last_observation]\n\n    new_y = pd.DataFrame(test_df_y_last)\n    new_y.rename(columns={new_y.columns[0]: 'y'}, inplace=True)\n    new_y.insert(0, 'ds', new_y.index - pd.Timedelta(minutes=forecast_horizon))\n    new_y.reset_index(drop = True, inplace=True)\n\n    new_y = new_y.drop(0, axis=0).reset_index(drop=True)\n    X_exog_complete = pd.concat([X_exog, X_test], axis=0)\n    X_exog_complete = X_exog_complete.drop(0, axis=0).reset_index(drop=True)\n    new_y = pd.merge(new_y, X_exog_complete, on='ds', how='left')\n\n    for i in range(n_update):\n    # for i in range(2): #for test only\n        print('Processing i = ', i + 1, ' out of ', n_update),\n        if i == 0:\n            X_test_curr = X_test.iloc[:1,:]\n            test_df_y_hat.iloc[i, 0] = prophet_model.predict(X_test_curr)['yhat'].values[0]\n        else:\n            new_rows = new_y.iloc[(i-1)*n_timesteps_per_update : i*n_timesteps_per_update, :]\n            y = pd.concat([y, new_rows], ignore_index=True)\n\n            current_params = warm_start_params(prophet_model)\n\n            prophet_model = Prophet(\n                seasonality_prior_scale=seasonality_prior_scale,  # Example hyperparameter for seasonality strength\n                seasonality_mode=seasonality_mode,  # Use multiplicative seasonality\n                weekly_seasonality=weekly_seasonality,  # Enable weekly seasonality\n                daily_seasonality=daily_seasonality,  # Enable daily seasonality\n                growth=growth,  # Choose between 'linear' or 'logistic' growth\n            )\n\n            prophet_model = prophet_model.fit(y, init=current_params)  # Adding the last day, warm-starting from the prev model\n            X_test_curr = X_test.iloc[i*n_timesteps_per_update : (1+i*n_timesteps_per_update),:]\n            test_df_y_hat.iloc[i*n_timesteps_per_update, 0] = prophet_model.predict(X_test_curr)['yhat'].values[0]\n\n    return train_df_y_hat, test_df_y_hat\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m16_prophet.train_model_m16_prophet","title":"<code>train_model_m16_prophet(hyperparameter, train_df_X, train_df_y, forecast_horizon)</code>","text":"<p>Train a prophet model for point forecasting. </p> <p>Parameters:</p> Name Type Description Default <code>hyperparameter (df) </code> <p>hyperparameter value of the model consisting of number of features</p> required <code>train_df_X (df) </code> <p>features matrix for training</p> required <code>train_df_y (df) </code> <p>target matrix for training</p> required <code>forecast_horizon (int) </code> <p>forecast horizon for the model</p> required <p>Returns:</p> Type Description <p>model (model) : trained model with all features</p> Source code in <code>docs\\notebooks\\model\\m16_prophet.py</code> <pre><code>def train_model_m16_prophet(hyperparameter, train_df_X, train_df_y, forecast_horizon):\n    ''' Train a prophet model for point forecasting. \n\n    Args:\n        hyperparameter (df) : hyperparameter value of the model consisting of number of features\n        train_df_X (df) : features matrix for training\n        train_df_y (df) : target matrix for training\n        forecast_horizon (int) : forecast horizon for the model\n\n    Returns:\n        model (model) : trained model with all features\n    '''\n\n    #UNPACK HYPERPARAMETER\n    seed = hyperparameter[\"seed\"]\n    seasonality_prior_scale = hyperparameter[\"seasonality_prior_scale\"]\n    seasonality_mode = hyperparameter[\"seasonality_mode\"]\n    weekly_seasonality = hyperparameter[\"weekly_seasonality\"]\n    daily_seasonality = hyperparameter[\"daily_seasonality\"]\n    growth = hyperparameter[\"growth\"]\n\n\n    # UPDATE train_df to exclude all rows after a sudden jump in the timestep\n    train_df_y_updated = remove_jump_df(train_df_y)\n    train_df_X_updated = remove_jump_df(train_df_X)\n\n    # Calculate the frequency of the timesteps using the first and second index values\n    timestep_frequency = train_df_y_updated.index[1] - train_df_y_updated.index[0]\n    inferred_frequency = pd.infer_freq(train_df_y_updated.index)\n    train_df_y_updated = train_df_y_updated.asfreq(inferred_frequency) \n\n    # INTRODUCE GAP BETWEEN TRAIN AND TEST SET TO AVOID DATA LEAKAGE\n    n_timestep_forecast_horizon = int(forecast_horizon / (timestep_frequency.total_seconds() / 60))\n    if n_timestep_forecast_horizon == 1:\n        pass\n    else:\n        train_df_y_updated = train_df_y_updated[:-(n_timestep_forecast_horizon - 1)]\n        train_df_X_updated = train_df_X_updated[:-(n_timestep_forecast_horizon - 1)]\n\n    # Assuming train_df_y_updated is your dataframe and 'y' is the column with the training series\n    y = train_df_y_updated.copy()\n    X_lags, X_exog = separate_lag_and_exogenous_features(train_df_X_updated)\n\n    #Initialize the Prophet model with hyperparameters\n    prophet_model = Prophet(\n        seasonality_prior_scale=seasonality_prior_scale,  # Example hyperparameter for seasonality strength\n        seasonality_mode=seasonality_mode,  # Use multiplicative seasonality\n        weekly_seasonality=weekly_seasonality,  # Enable weekly seasonality\n        daily_seasonality=daily_seasonality,  # Enable daily seasonality\n        growth=growth  # Choose between 'linear' or 'logistic' growth\n        # random_state =  seed,  # cannot set seed in prophet\n    )\n    for col in X_exog.columns:\n        prophet_model.add_regressor(col)\n\n    # Add exogenous features to the y DataFrame\n    y = y.merge(X_exog, on='datetime')\n    y.reset_index(inplace=True)\n    y.rename(columns={'datetime': 'ds'}, inplace=True)\n\n    # Train model\n    prophet_model.fit(y)\n\n    # PACK MODEL\n    model = {\"prophet\": prophet_model, \"y\": y, \"hyperparameter\": hyperparameter}\n\n\n    return model\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m17_xgb.produce_forecast_m17_xgb","title":"<code>produce_forecast_m17_xgb(model, train_df_X, test_df_X)</code>","text":"<p>Create forecast at the train and test set using the trained model</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>dictionary</code> <p>all parameters of the trained model</p> required <code>train_df_X</code> <code>df</code> <p>predictors of train set</p> required <code>test_df_X</code> <code>df</code> <p>predictors of test set</p> required <p>Returns:</p> Type Description <p>train_df_y_hat (df) : forecast result at train set</p> <p>test_df_y_hat (df) : forecast result at test set</p> Source code in <code>docs\\notebooks\\model\\m17_xgb.py</code> <pre><code>def produce_forecast_m17_xgb(model, train_df_X, test_df_X):\n    \"\"\"Create forecast at the train and test set using the trained model\n\n    Args:\n        model (dictionary): all parameters of the trained model\n        train_df_X (df): predictors of train set\n        test_df_X (df): predictors of test set\n\n    Returns:\n        train_df_y_hat (df) : forecast result at train set\n        test_df_y_hat (df) : forecast result at test set\n\n    \"\"\"\n\n    # UNPACK MODEL\n    xgb = model[\"xgb\"]\n\n    # PRODUCE FORECAST\n    train_df_y_hat = xgb.predict(train_df_X)\n    test_df_y_hat = xgb.predict(test_df_X)\n\n    return train_df_y_hat, test_df_y_hat\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m17_xgb.train_model_m17_xgb","title":"<code>train_model_m17_xgb(hyperparameter, train_df_X, train_df_y)</code>","text":"<p>Train and test a xgb model for point forecasting. </p> <p>Parameters:</p> Name Type Description Default <code>hyperparameter (df) </code> <p>hyperparameter value of the model consisting of number of features</p> required <code>train_df_X (df) </code> <p>features matrix for training</p> required <code>train_df_y (df) </code> <p>target matrix for training</p> required <p>Returns:</p> Type Description <p>model (model) : trained model with all features</p> Source code in <code>docs\\notebooks\\model\\m17_xgb.py</code> <pre><code>def train_model_m17_xgb(hyperparameter, train_df_X, train_df_y):\n    ''' Train and test a xgb model for point forecasting. \n\n    Args:\n        hyperparameter (df) : hyperparameter value of the model consisting of number of features\n        train_df_X (df) : features matrix for training\n        train_df_y (df) : target matrix for training\n\n\n    Returns:\n        model (model) : trained model with all features\n    '''\n\n    #UNPACK HYPERPARAMETER\n    xgb_seed = int(hyperparameter[\"xgb_seed\"])\n    n_estimators=hyperparameter[\"n_estimators\"]\n    learning_rate=hyperparameter[\"learning_rate\"]\n    max_depth=hyperparameter[\"max_depth\"]\n    subsample=hyperparameter[\"subsample\"]\n    colsample_bytree=hyperparameter[\"colsample_bytree\"]\n\n    #INITIALIZE AND TRAIN MODEL\n    xgb = XGBRegressor(n_estimators=200, learning_rate=0.1, max_depth=6, subsample=0.8, colsample_bytree=0.8, random_state=xgb_seed)   \n    xgb.fit(train_df_X, train_df_y)   \n\n    # PACK MODEL\n    model = {\"xgb\": xgb}\n\n\n    return model\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m18_nbeats.produce_forecast_m18_nbeats","title":"<code>produce_forecast_m18_nbeats(model, train_df_X, test_df_X)</code>","text":"<p>Create forecast at the train and test set using the trained NBeats model.</p> <p>Parameters:</p> Name Type Description Default <code>model </code> <p>trained NBeats PyTorch model</p> required <code>train_df_X (DataFrame) </code> <p>predictors of train set</p> required <code>test_df_X (DataFrame) </code> <p>predictors of test set</p> required <p>Returns:</p> Type Description <p>train_df_y_hat (DataFrame) : forecast result at train set</p> <p>test_df_y_hat (DataFrame) : forecast result at test set</p> Source code in <code>docs\\notebooks\\model\\m18_nbeats.py</code> <pre><code>def produce_forecast_m18_nbeats(model, train_df_X, test_df_X):\n    \"\"\"\n    Create forecast at the train and test set using the trained NBeats model.\n\n    Args:\n        model : trained NBeats PyTorch model\n        train_df_X (DataFrame) : predictors of train set\n        test_df_X (DataFrame) : predictors of test set\n\n    Returns:\n        train_df_y_hat (DataFrame) : forecast result at train set\n        test_df_y_hat (DataFrame) : forecast result at test set\n    \"\"\"\n    import torch\n    model.eval()\n\n    with torch.no_grad():\n        X_train_tensor = torch.tensor(train_df_X.values, dtype=torch.float32)\n        X_test_tensor = torch.tensor(test_df_X.values, dtype=torch.float32)\n\n        y_train_hat = model(X_train_tensor).numpy()\n        y_test_hat = model(X_test_tensor).numpy()\n\n    import pandas as pd\n    train_df_y_hat = pd.DataFrame(y_train_hat, index=train_df_X.index, columns=['y_hat'])\n    test_df_y_hat = pd.DataFrame(y_test_hat, index=test_df_X.index, columns=['y_hat'])\n\n    return train_df_y_hat, test_df_y_hat\n</code></pre>"},{"location":"api_reference/#docs.notebooks.model.m18_nbeats.train_model_m18_nbeats","title":"<code>train_model_m18_nbeats(hyperparameter, train_df_X, train_df_y)</code>","text":"<p>Train and test an NBeats model for point forecasting. Uses NBeats architecture for predicting time series with lag+exogenous features.</p> <p>Parameters:</p> Name Type Description Default <code>hyperparameter (dict) </code> <p>model hyperparameters</p> required <code>train_df_X (DataFrame) </code> <p>predictors for training</p> required <code>train_df_y (DataFrame) </code> <p>target for training</p> required <p>Returns:</p> Name Type Description <code>model</code> <p>trained PyTorch NBeats model</p> Source code in <code>docs\\notebooks\\model\\m18_nbeats.py</code> <pre><code>def train_model_m18_nbeats(hyperparameter, train_df_X, train_df_y):\n    \"\"\"\n    Train and test an NBeats model for point forecasting.\n    Uses NBeats architecture for predicting time series with lag+exogenous features.\n\n    Args:\n        hyperparameter (dict) : model hyperparameters\n        train_df_X (DataFrame) : predictors for training\n        train_df_y (DataFrame) : target for training\n\n    Returns:\n        model : trained PyTorch NBeats model\n    \"\"\"\n    # ---- Unpack hyperparameters ----\n    input_size = train_df_X.shape[1]\n    output_size = int(hyperparameter['output_size'])\n    hidden_size = int(hyperparameter['hidden_size'])\n    num_blocks = int(hyperparameter['num_blocks'])\n    num_layers = int(hyperparameter['num_layers'])\n    lr = hyperparameter['lr']\n    epochs = int(hyperparameter['epochs'])\n    seed = int(hyperparameter['seed'])\n\n    # ---- Set seeds for reproducibility ----\n    import torch, numpy as np, random\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n    # ---- Define NBeats model inside the function ----\n    import torch.nn as nn\n    class NBeatsModel(nn.Module):\n        def __init__(self, input_size, output_size, hidden_size, num_blocks, num_layers):\n            super(NBeatsModel, self).__init__()\n            blocks = []\n            for _ in range(num_blocks):\n                block = []\n                for l in range(num_layers):\n                    block.append(nn.Linear(input_size if l==0 else hidden_size, hidden_size))\n                    block.append(nn.ReLU())\n                block.append(nn.Linear(hidden_size, output_size))\n                blocks.append(nn.Sequential(*block))\n            self.blocks = nn.ModuleList(blocks)\n\n        def forward(self, x):\n            out = 0\n            for block in self.blocks:\n                out += block(x)\n            return out\n\n    model = NBeatsModel(input_size, output_size, hidden_size, num_blocks, num_layers)\n\n    # ---- Training setup ----\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    criterion = nn.MSELoss()\n    X_tensor = torch.tensor(train_df_X.values, dtype=torch.float32)\n    y_tensor = torch.tensor(train_df_y.values, dtype=torch.float32)\n\n    # ---- Training loop ----\n    model.train()\n    for epoch in range(epochs):\n        optimizer.zero_grad()\n        output = model(X_tensor)\n        loss = criterion(output, y_tensor)\n        loss.backward()\n        optimizer.step()\n\n    return model\n</code></pre>"},{"location":"contributors/","title":"Contributors","text":"<ul> <li>Samhan Samhan (m.samhan@unsw.edu.au): Lead developer and researcher. Responsible for conceptualization, implementation, documentation, and experimentation.</li> <li>Assoc Prof Anna Bruce: Supervisor. Provided guidance on research direction and methodology.</li> <li>Dr Baran Yildiz: Supervisor. Provided guidance on research direction and methodology.</li> </ul>"},{"location":"datasets_models/","title":"Datasets & Models","text":""},{"location":"datasets_models/#dataset","title":"Dataset","text":"<p>The available datasets are listed in the <code>data</code> folder, with metadata provided in <code>data/metadata.xlsx</code>. Below are some of the datasets currently included in the library:</p>"},{"location":"datasets_models/#ausgrid-solar-home-datasets","title":"Ausgrid Solar Home Datasets","text":"<p>This dataset has been widely used in net load forecasting research since 2016. It includes data from 300 solar-equipped households within the Ausgrid network in Sydney. The original source is here.</p> Dataset Name Description <code>ds1_ahsd.csv</code> Ausgrid Solar Home Dataset, aggregate of 300 household data in Ausgrid network <code>ds4_ashd_with_weather.csv</code> <code>ds1</code> enhanced with temperature, relative humidity, and wind speed data <code>ds13_ashd_with_cloud_solcast.csv</code> <code>ds4</code> further enriched with cloud data from Solcast"},{"location":"datasets_models/#australia-energy-data-platform-aedp-datasets","title":"Australia Energy Data Platform (AEDP) Datasets","text":"<p>These datasets were compiled by UNSW Sydney using data from Solar Analytics and Wattwatchers. Sensitive information such as customer addresses, names, and NMIs has been removed. The original source is  here.</p> Dataset Name Description <code>ds10_aedp_cluster2_30min.csv</code> AEDP dataset for Cluster 2 with 30-minute resolution <code>ds11_aedp_cluster2_30min_with_weather.csv</code> <code>ds10</code> enhanced with weather data including temperature, humidity, etc."},{"location":"datasets_models/#ausgrid-zone-substation-zs-datasets-mascot","title":"Ausgrid Zone Substation (ZS) Datasets \u2013 Mascot","text":"<p>Unlike the previous household-focused datasets, this dataset covers a zone substation, which includes residential, commercial &amp; industrial (C&amp;I), and major customers. The original source is here.</p> Dataset Name Description <code>ds14_ausgrid_zs_mascot.csv</code> Zone Substation data for Mascot <code>ds15_ausgrid_zs_mascot_30min_with_weather.csv</code> <code>ds14</code> enhanced with weather data at 30-minute resolution"},{"location":"datasets_models/#model","title":"Model","text":""},{"location":"datasets_models/#forecasting-models-overview","title":"Forecasting Models Overview","text":"Model ID Model Name Short Description <code>m1_naive</code> Naive Forecast equals the last observed value <code>m2_snaive</code> Seasonal Naive Forecast equals the value from the same season in the previous cycle"},{"location":"datasets_models/#statistical-models","title":"Statistical Models","text":"Model ID Model Name Short Description <code>m3_ets</code> ETS Exponential smoothing model with error, trend, and seasonality components <code>m4_arima</code> ARIMA Autoregressive Integrated Moving Average model for time series forecasting <code>m5_sarima</code> SARIMA Seasonal ARIMA model with seasonal components <code>m6_lr</code> Linear Regression Predicts future values using a linear combination of input features"},{"location":"datasets_models/#machine-learning-models","title":"Machine Learning Models","text":"Model ID Model Name Short Description <code>m7_ann</code> ANN Basic Artificial Neural Network with one hidden layers <code>m8_dnn</code> Deep Neural Network ANN with more than one hidden layer <code>m9_rt</code> Regression Tree Decision tree model for regression tasks <code>m10_rf</code> Random Forest Ensemble of regression trees for improved accuracy and robustness <code>m11_svr</code> Support Vector Regression Uses support vectors to perform regression with margin of tolerance <code>m12_rnn</code> Recurrent Neural Network Neural network with feedback loops for sequential data <code>m13_lstm</code> Long Short-Term Memory RNN variant designed to capture long-term dependencies <code>m14_gru</code> Gated Recurrent Unit Simplified LSTM with fewer parameters <code>m15_transformer</code> Transformer Attention-based model for sequence modeling without recurrence <code>m16_prophet</code> Prophet Time series model developed by Facebook for business forecasting <code>m17_xgb</code> XGBoost Gradient boosting framework optimized for speed and performance <code>m18_nbeats</code> N-BEATS Deep learning model for univariate time series forecasting"},{"location":"datasets_models/#hyperparameter","title":"Hyperparameter","text":"<p>The list of available model and its hyperparameter can be seen on <code>config/model_hyperparameters.ipynb</code>. The values currently available are the hyperparameter values mostly used in academic literature, but not necessarily the optimum value. </p>"},{"location":"examples/","title":"User Input","text":"<p>Suppose we perform a simple test using the inputs below, which should take less than 1 minute in the <code>run_experiment.ipynb</code> file: </p> <pre><code># 1. RUN CONFIG\n%run \"../config/config.ipynb\"\n\n# 2. SETUP FORECAST PROBLEM AND MODEL SPECIFICATION (USER TO INPUT)\n# FORECAST PROBLEM\ndataset = ds0\nforecast_horizon = fh1 # fh1 = 30 minutes ahead, fh9 = 2 days ahead\n# MODEL SPECIFICATION\nmodel_name = m6\nhyperparameter_no = 'hp1'\n\n# 3. RUN EXPERIMENT\nrun_experiment(dataset, forecast_horizon, model_name, hyperparameter_no)\n</code></pre>"},{"location":"examples/#output","title":"Output","text":"<p>The tool generates the following outputs.</p> Name Type Description <code>E00001_cv_test/</code> Folder Time series of observation, forecast, and residual for each cross-validation split <code>E00001_cv_train/</code> Folder Time series of observation, forecast, and residual for each cross-validation split <code>E00001_cv1_plots/</code> Folder Plots for the first cross-validation fold: time plot, scatter plot, residual plot, histogram <code>E00001_models/</code> Folder Saved models used or generated during the experiment <code>E00001_a1_experiment_result.csv</code> File Accuracy (cross-validated test n-RMSE), stability, and training time <code>E00001_a2_hyperparameter.csv</code> File Hyperparameters used for each model <code>E00001_a3_cross_validation_result.csv</code> File Detailed results for each cross-validation split <p>The file <code>a1_experiment_result.csv</code> summarises the results, including the cross validated nRMSE &amp; its standard deviation</p> experiment_no exp_date dataset_no dataset dataset_freq_min dataset_length_week forecast_horizon_min train_pct test_pct model_no hyperparameter_no model_name hyperparameter runtime_ms train_RMSE train_RMSE_stddev test_RMSE test_RMSE_stddev train_nRMSE train_nRMSE_stddev test_nRMSE test_nRMSE_stddev E00001 15/09/2025 ds0 test 30 10 30 0.9 0.1 m6 hp1 m6_lr_hp1 num_features: 50 201.769185 17.33 0.206421 17.7066 1.82726 2.98206 0.03552 3.04686 0.31443 <p>The file <code>a3_cross_validation_result.csv</code> provides the detailed cross-validation (CV) results, from CV1 to CV10.</p>"},{"location":"examples/#experiment-metrics","title":"Experiment Metrics","text":"# runtime_ms train_MBE train_MAE train_RMSE train_MAPE train_MASE train_fskill train_R2 test_MBE test_MAE test_RMSE test_MAPE test_MASE test_fskill test_R2 train_nRMSE test_nRMSE 1 193.2564 0 12.208 17.122 27.366 0.422 55.471 0.988 -0.1804 13.278 19.679 22.808 0.459 43.531 0.981 2.9463 3.3863 2 224.8719 0 12.337 17.495 27.347 0.423 54.817 0.988 -0.1443 12.332 16.346 18.905 0.423 49.006 0.987 3.0105 2.8127 3 251.0867 0 12.361 17.477 22.314 0.433 54.031 0.987 0.3318 11.965 16.321 60.409 0.419 58.022 0.989 3.0074 2.8084 4 174.1974 0 12.401 17.573 28.635 0.434 53.775 0.987 -0.8563 11.654 15.427 13.623 0.408 60.346 0.992 3.0239 2.6546 5 232.7249 0 12.281 17.020 22.091 0.435 54.565 0.988 1.2783 12.536 20.375 70.615 0.444 53.147 0.986 2.9287 3.5060 6 173.2767 0 12.322 17.467 28.530 0.436 53.491 0.987 -0.6225 12.436 16.567 12.690 0.440 61.224 0.991 3.0056 2.8508 7 233.0346 0 12.171 17.004 26.500 0.427 55.250 0.988 0.8754 13.685 20.516 27.756 0.480 47.485 0.981 2.9260 3.5303 8 212.3849 0 12.286 17.390 26.759 0.428 54.516 0.988 -0.7814 12.809 17.191 23.091 0.447 53.467 0.985 2.9924 2.9581 9 157.8457 0 12.330 17.417 28.242 0.435 53.965 0.988 -1.0170 12.308 16.923 10.919 0.434 58.189 0.989 2.9970 2.9120 10 165.0126 0 12.269 17.335 28.311 0.421 55.265 0.988 1.0148 12.840 17.721 11.193 0.441 44.110 0.979 2.9829 3.0493 mean 201.7692 0 12.2966 17.330 26.6095 0.4294 54.5146 0.9877 -0.0102 12.5843 17.7066 27.2009 0.4395 52.8527 0.986 2.9821 3.0469 stddev 33.2319 0 0.0693 0.2064 2.4352 0.00591 0.6866 0.00048 0.8405 0.5983 1.8273 21.106 0.02059 6.5729 0.00447 0.03552 0.31443 <p>Below are some plots on the test set:</p> Figure 1: Observation vs Forecast (Time Plot) Figure 2: Observation vs Forecast (Scatter Plot) Figure 3: Residuals Over Time Figure 4: Residual Histogram"},{"location":"features_limitations/","title":"Features & Limitations","text":"Type Title Description Feature Adjustable User can specify the dataset, forecast horizon, model, and hyperparameters Feature Systematic Each experiment outputs metadata such as date, experiment ID, forecast problem, and model specs Feature Flexible Users can add or modify models, datasets, hyperparameters, and general functions Limitation Basic User Interface Users must manually modify code to change models Limitation Limited Dataset and Model Library Many public net load datasets and models are not yet included. Contributions are welcome!"},{"location":"getting_started/","title":"Installation Instructions","text":"<ol> <li>Clone the repository to your local machine.  </li> <li>Create a Python virtual environment.  </li> <li>Install the required packages:</li> </ol> <pre><code>pip install -r requirements.txt\n</code></pre> <p>\u26a0\ufe0f This may take ~10 minutes. Although newer Python versions may work, the tool was tested on Python 3.12.3.</p>"},{"location":"getting_started/#how-to-use-the-tool","title":"How to Use The Tool","text":"<ol> <li>Open the notebook: <code>notebooks/model/run_experiments.ipynb.</code></li> <li>Fill in the input values for your forecast problem and model specification. </li> <li>Run the notebook.</li> <li>The tool outputs evaluation results to the <code>experiment_result/folder</code>.</li> <li>To evaluate multiple forecast problems and model specifications at once, use <code>notebooks/model/run_experiments_batch.ipynb</code></li> </ol> <p>Example input values:</p> <pre><code>dataset = \"ds0\"            # Dataset for testing\nforecast_horizon = \"fh1\"   # fh1 = 30 minutes ahead\nmodel_name = \"m6\"          # Linear regression\nhyperparameter_no = \"hp1\"  # Hyperparameter ID\n</code></pre> <p>For the list of available datasets &amp; models, how to modify model hyperparameter, how to add a model, how to add a dataset, and exhaustive list of API Reference see the Detailed Guide page.</p>"},{"location":"modify_hyper/","title":"Model Hyperparameter","text":"<p>All hyperparameter values are stored separately from the model code in a designated notebook: <code>notebooks/config/model_hyperparameters.ipynb</code>. This design avoids hard-coded values and provides a centralized location for managing all hyperparameters.</p>"},{"location":"modify_hyper/#format","title":"Format","text":"<p>Hyperparameter values are stored as a list of dictionaries. For example, for <code>m7_ann</code>, the values are stored in the <code>m7_hp_table</code> list. Each dictionary contains an <code>hp_no</code> (hyperparameter set ID) and key-value pairs for the parameters.</p> <pre><code>m7_hp_table = [\n    {\n        \"hp_no\": \"hp1\",\n        \"seed\": 99, # we will use the same seed for reproducibility\n        \"hidden_size\": 10, #number of neurons in hidden layers\n        \"activation_function\": \"relu\",\n        \"learning_rate\" : 0.001,\n        \"solver\" : \"adam\",\n        \"epochs\" : 500\n    },\n    {\n        \"hp_no\": \"hp2\",\n        \"seed\": 99, # we will use the same seed for reproducibility\n        \"hidden_size\": 10, #number of neurons in hidden layers\n        \"activation_function\": \"relu\",\n        \"learning_rate\" : 0.01,\n        \"solver\" : \"adam\",\n        \"epochs\" : 500\n    },\n]\n</code></pre>"},{"location":"modify_hyper/#how-to-modify-model-hyperparameter","title":"How to Modify Model Hyperparameter","text":"<p>If for example we want to modify the learning rate of the ANN model we can create a new hyperparameter set, and below is the update result:</p> <pre><code>m7_hp_table = [\n    {\n        \"hp_no\": \"hp1\",\n        \"seed\": 99, # we will use the same seed for reproducibility\n        \"hidden_size\": 10, #number of neurons in hidden layers\n        \"activation_function\": \"relu\",\n        \"learning_rate\" : 0.001,\n        \"solver\" : \"adam\",\n        \"epochs\" : 500\n    },\n    {\n        \"hp_no\": \"hp2\",\n        \"seed\": 99, # we will use the same seed for reproducibility\n        \"hidden_size\": 10, #number of neurons in hidden layers\n        \"activation_function\": \"relu\",\n        \"learning_rate\" : 0.01,\n        \"solver\" : \"adam\",\n        \"epochs\" : 500\n    },\n    {\n        \"hp_no\": \"hp3\",\n        \"seed\": 99, # we will use the same seed for reproducibility\n        \"hidden_size\": 10, #number of neurons in hidden layers\n        \"activation_function\": \"relu\",\n        \"learning_rate\" : 0.1, **Modified learning rate**\n        \"solver\" : \"adam\",\n        \"epochs\" : 500\n    },\n]\n</code></pre> <p>When running experiments, you can select the desired hyperparameter set by referencing its <code>hp_no</code>, such as <code>hp3</code>.</p>"},{"location":"repo_structure/","title":"Repo structure","text":""},{"location":"repo_structure/#folder-data","title":"Folder: <code>data/</code>","text":"<p>Contains all datasets available for experiments. Metadata is stored in <code>data/metadata.xlsx</code>.</p>"},{"location":"repo_structure/#folder-docs","title":"Folder: <code>docs/</code>","text":"<p>Includes all files required to build the documentation page on GitHub Desktop, including images.</p>"},{"location":"repo_structure/#folder-experiment_result","title":"Folder: <code>experiment_result/</code>","text":"<p>Stores all experiment outputs generated by the PyNNLF tool. Testing results are saved in <code>experiment_result/Archive/Testing Result</code>.</p> <ul> <li><code>result_summary.xlsx</code>: Summary of all experiment results, generated by <code>notebooks/result_aggregation.ipynb</code>.</li> <li><code>testing_benchmark.xlsx</code>: Benchmark results from three different computers.</li> </ul>"},{"location":"repo_structure/#folder-notebooks","title":"Folder: <code>notebooks/</code>","text":"<p>Contains all source code for the tool.</p>"},{"location":"repo_structure/#subfolder-notebooksconfig","title":"Subfolder: <code>notebooks/config/</code>","text":"<p>Stores configuration files with hard-coded values.</p> <ul> <li><code>config.ipynb</code>: Contains tool parameters that rarely change (e.g., cross-validation folds, lookback period, train-test split ratio).</li> <li><code>constant.ipynb</code>: Stores constants like month names.</li> <li><code>general_functions.ipynb</code>: Core backend functions for PyNNLF, including lag feature creation, train-test splitting, cross-validation, evaluation, etc.</li> <li><code>hyperparameters.ipynb</code>: Stores all model hyperparameters.</li> </ul>"},{"location":"repo_structure/#subfolder-notebooksmodel","title":"Subfolder: <code>notebooks/model/</code>","text":"<p>Contains all model files and experiment notebooks.</p>"},{"location":"repo_structure/#model-files","title":"Model Files","text":"<p>Each model file follows the naming format <code>[model_id]_[model_name].ipynb</code>.</p>"},{"location":"repo_structure/#run-experiments","title":"Run Experiments","text":"<ul> <li><code>run_experiment.ipynb</code>: Runs a single experiment.</li> <li><code>run_experiment_batch.ipynb</code>: Runs batch experiments across multiple forecast problems or model configurations.</li> </ul>"},{"location":"repo_structure/#result-aggregation","title":"Result Aggregation","text":"<ul> <li><code>result_aggregation.ipynb</code>: Aggregates results from all experiments in <code>experiment_result/</code> into <code>result_summary.xlsx</code>.</li> </ul>"},{"location":"repo_structure/#run-tests","title":"Run Tests","text":"<ul> <li><code>run_experiment_batch.ipynb</code>: Also used for testing the tool.   Important to run full tests after major updates, especially those affecting the backend.</li> </ul>"},{"location":"run_experiment/","title":"How to Use The Tool","text":"<ul> <li>Open the notebook: <code>notebooks/model/run_experiments.ipynb.</code></li> <li>Fill in the input values for your forecast problem and model specification. Example:</li> </ul> <pre><code>dataset = \"ds0\"            # Dataset for testing\nforecast_horizon = \"fh1\"   # fh1 = 30 minutes ahead\nmodel_name = \"m6\"          # Linear regression\nhyperparameter_no = \"hp1\"  # Hyperparameter ID\n</code></pre> <ul> <li> <p>Run the notebook.</p> </li> <li> <p>The tool outputs evaluation results to the <code>experiment_result/folder</code>.</p> </li> <li> <p>To evaluate multiple forecast problems and model specifications at once, use <code>notebooks/model/run_experiments_batch.ipynb</code></p> </li> </ul> <p>For the list of available datasets &amp; models, how to modify model hyperparameter, how to add a model, how to add a dataset, and exhaustive list of API Reference see the Detailed Guide page.</p>"},{"location":"support_contributing/","title":"Support & Contributing","text":"<p>To report bugs, request features, or suggest improvements, please use the GitHub Issues feature. For contributing or seeking support, contact m.samhan@unsw.edu.au.</p>"},{"location":"tool_testing/","title":"Simple Testing","text":"<p>To quickly test the tool, follow the steps in the Getting Started section before. You can try it out with the following example inputs:</p> <pre><code>dataset = ds0              # Sample dataset for testing\nforecast_horizon = fh1     # fh1 = 30 minutes ahead\nmodel_name = m6            # m6 = Linear Regression\nhyperparameter_no = 'hp1'  # Example hyperparameter set\n</code></pre> <p>This is a great way to get familiar with how the tool works before running full-scale tests.</p>"},{"location":"tool_testing/#full-model-testing-all-18-models","title":"Full Model Testing (All 18 Models)","text":"<p>For a complete evaluation and checking the result against benchmark values, which is available on <code>experiment_result/Archive/Testing Result/testing_benchmark</code>.  To test it, follow these steps:</p> <ol> <li>Open the file <code>run_tests.ipynb</code>.</li> <li>Run all cells without making any changes.</li> <li>The notebook will automatically: Run all 18 models, compare their outputs against three benchmark results. The results are available in: <code>experiment_result/Archive/Testing Result/</code></li> </ol> <p>\u23f1 Estimated time: ~1 hour on a personal computer with an Intel i5 processor and 32GB RAM. This tool has undergone full model testing on three different computers, and the result can be seen on </p> <pre><code>experiment_result/Archive/Testing Result/20250821_test_result_CEEM Computer.csv\nexperiment_result/Archive/Testing Result/20250821_test_result_UNSW Laptop.csv\nexperiment_result/Archive/Testing Result/20250822_test_result_SS Personal Laptop\n</code></pre>"}]}