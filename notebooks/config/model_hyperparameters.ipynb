{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = 'm1_naive' #custom\n",
    "m2 = 'm2_snaive' #custom\n",
    "m3 = 'm3_ets' #statsmodel\n",
    "m4 = 'm4_arima' #statsmodel\n",
    "m5 = 'm5_sarima' #statsmodel\n",
    "m6 = 'm6_lr' #scikit-learn\n",
    "m7 = 'm7_ann' #scikit-learn\n",
    "m8 = 'm8_dnn' #pytorch\n",
    "m9 = 'm9_rt' #pytorch\n",
    "m10 = 'm10_rf' #scikit-learn\n",
    "m11 = 'm11_svr' #scikit-learn\n",
    "m12 = 'm12_rnn' #pytorch\n",
    "m13 = 'm13_lstm' #pytorch\n",
    "m14 = 'm14_gru' #pytorch\n",
    "m15 = 'm15_transformer' #pytorch\n",
    "m16 = 'm16_prophet' #fb prophet\n",
    "m17 = 'm17_xgb' #xgboost\n",
    "m18 = 'm18_nbeats' #pytorch forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1. Naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter List:\n",
    "# version\n",
    "m1_hp_table = [\n",
    "    {\n",
    "        \"hp_no\": \"hp1\",\n",
    "        \"hyperparameter_1\": \"No hyperparameter\",\n",
    "    }\n",
    "]\n",
    "\n",
    "m1_hp_table = pd.DataFrame(m1_hp_table)\n",
    "m1_hp_table.set_index('hp_no', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2. Seasonal Naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter List:\n",
    "# We can also add initial states or damping \n",
    "\n",
    "# version\n",
    "\n",
    "m2_hp_table = [\n",
    "    {\n",
    "        \"hp_no\": \"hp1\",\n",
    "        \"days\": 1, #1 day behind\n",
    "    },\n",
    "    {\n",
    "        \"hp_no\": \"hp2\",\n",
    "        \"days\": 7, #a week behind\n",
    "    }\n",
    "]\n",
    "\n",
    "m2_hp_table = pd.DataFrame(m2_hp_table)\n",
    "m2_hp_table.set_index('hp_no', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3. Exponential Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to inlcude seed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter List:\n",
    "# We don't use seasonal period 7 days because it failed to converge (tried 250109)\n",
    "m3_hp_table = [\n",
    "    {\n",
    "        \"hp_no\": \"hp1\",\n",
    "        \"trend\": False,\n",
    "        \"damped_trend\": False,\n",
    "        \"seasonal_periods_days\": None\n",
    "    },\n",
    "    {\n",
    "        \"hp_no\": \"hp2\",\n",
    "        \"trend\": False,\n",
    "        \"damped_trend\": False,\n",
    "        \"seasonal_periods_days\": 1\n",
    "    },\n",
    "    {\n",
    "        \"hp_no\": \"hp3\",\n",
    "        \"trend\": \"add\",\n",
    "        \"damped_trend\": False,\n",
    "        \"seasonal_periods_days\": None\n",
    "    },\n",
    "]\n",
    "\n",
    "m3_hp_table = pd.DataFrame(m3_hp_table)\n",
    "m3_hp_table.set_index('hp_no', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 4. ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter List:\n",
    "m4_hp_table = [\n",
    "    {\n",
    "        \"hp_no\": \"hp1\",\n",
    "        \"p\": 1,\n",
    "        \"d\": 0,\n",
    "        \"q\": 1\n",
    "    },\n",
    "    {\n",
    "        \"hp_no\": \"hp2\",\n",
    "        \"p\": 1,\n",
    "        \"d\": 1,\n",
    "        \"q\": 1\n",
    "    }\n",
    "]\n",
    "\n",
    "m4_hp_table = pd.DataFrame(m4_hp_table)\n",
    "m4_hp_table.set_index('hp_no', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 5. SARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter List:\n",
    "# this model is not used because it is very slow to converge somehow\n",
    "m5_hp_table = [\n",
    "    {\n",
    "        \"hp_no\": \"hp1\",\n",
    "        \"p\": 1,\n",
    "        \"d\": 0,\n",
    "        \"q\": 1,\n",
    "        \"P\": 0,\n",
    "        \"D\": 1,\n",
    "        \"Q\": 0,\n",
    "        \"seasonal_period_days\": 1\n",
    "    },\n",
    "    {\n",
    "        \"hp_no\": \"hp2\",\n",
    "        \"p\": 1,\n",
    "        \"d\": 0,\n",
    "        \"q\": 1,\n",
    "        \"P\": 0,\n",
    "        \"D\": 1,\n",
    "        \"Q\": 0,\n",
    "        \"seasonal_period_days\": 7\n",
    "    }\n",
    "]\n",
    "\n",
    "m5_hp_table = pd.DataFrame(m5_hp_table)\n",
    "m5_hp_table.set_index('hp_no', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 6. Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no need to set up seed because deterministic\n",
    "m6_hp_table = [\n",
    "    {\n",
    "        \"hp_no\": \"hp1\",\n",
    "        \"num_features\": 50,\n",
    "    },\n",
    "]\n",
    "\n",
    "m6_hp_table = pd.DataFrame(m6_hp_table)\n",
    "m6_hp_table.set_index('hp_no', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 7. ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to inlcude seed!\n",
    "Include n of epoch, batch size, optimiser used, learning rate, regularization technique, early stopping etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m7_hp_table = [\n",
    "    {\n",
    "        \"hp_no\": \"hp1\",\n",
    "        \"seed\": 99, # we will use the same seed for reproducibility\n",
    "        \"hidden_size\": 10, #number of neurons in hidden layers\n",
    "        \"activation_function\": \"relu\",\n",
    "        \"learning_rate\" : 0.001,\n",
    "        \"solver\" : \"adam\",\n",
    "        \"epochs\" : 500\n",
    "    },\n",
    "    {\n",
    "        \"hp_no\": \"hp2\",\n",
    "        \"seed\": 99, # we will use the same seed for reproducibility\n",
    "        \"hidden_size\": 10, #number of neurons in hidden layers\n",
    "        \"activation_function\": \"relu\",\n",
    "        \"learning_rate\" : 0.01,\n",
    "        \"solver\" : \"adam\",\n",
    "        \"epochs\" : 500\n",
    "    },\n",
    "]\n",
    "\n",
    "m7_hp_table = pd.DataFrame(m7_hp_table)\n",
    "m7_hp_table.set_index('hp_no', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 8. DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m8_hp_table = [\n",
    "    {\n",
    "        \"hp_no\": \"hp1\",\n",
    "        \"seed\": 1, # we will use the same seed for reproducibility #I tried 99 but it failed! It somehow gives non negative result.\n",
    "        \"n_hidden\" : 3, #number of hidden layers\n",
    "        \"hidden_size\": 10, #number of neurons in hidden layers\n",
    "        \"activation_function\": \"relu\",\n",
    "        \"learning_rate\" : 0.001,\n",
    "        \"solver\" : \"adam\",\n",
    "        \"epochs\" : 500\n",
    "    },\n",
    "    {\n",
    "        \"hp_no\": \"hp2\",\n",
    "        \"seed\": 1, # we will use the same seed for reproducibility\n",
    "        \"n_hidden\" : 3, #number of hidden layers\n",
    "        \"hidden_size\": 10, #number of neurons in hidden layers\n",
    "        \"activation_function\": \"relu\",\n",
    "        \"learning_rate\" : 0.01,\n",
    "        \"solver\" : \"adam\",\n",
    "        \"epochs\" : 500\n",
    "    },\n",
    "    {\n",
    "        \"hp_no\": \"hp3\",\n",
    "        \"seed\": 1, # we will use the same seed for reproducibility\n",
    "        \"n_hidden\" : 4, #number of hidden layers\n",
    "        \"hidden_size\": 10, #number of neurons in hidden layers\n",
    "        \"activation_function\": \"relu\",\n",
    "        \"learning_rate\" : 0.01,\n",
    "        \"solver\" : \"adam\",\n",
    "        \"epochs\" : 500\n",
    "    },\n",
    "]\n",
    "\n",
    "m8_hp_table = pd.DataFrame(m8_hp_table)\n",
    "m8_hp_table.set_index('hp_no', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 9. Regression Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m9_hp_table = [\n",
    "    {\n",
    "        \"hp_no\": \"hp1\",\n",
    "        \"seed\": 1, \n",
    "        \"max_depth\" : 3,\n",
    "        \"min_samples_split\" : 2,\n",
    "        \"min_samples_leaf\" : 1,\n",
    "        \"max_features\" : \"sqrt\",\n",
    "    },\n",
    "    {\n",
    "        \"hp_no\": \"hp2\",\n",
    "        \"seed\": 1, \n",
    "        \"max_depth\" : 30,\n",
    "        \"min_samples_split\" : 2,\n",
    "        \"min_samples_leaf\" : 1,\n",
    "        \"max_features\" : \"sqrt\",\n",
    "    },\n",
    "    {\n",
    "        \"hp_no\": \"hp3\",\n",
    "        \"seed\": 1, \n",
    "        \"max_depth\" : 15,\n",
    "        \"min_samples_split\" : 2,\n",
    "        \"min_samples_leaf\" : 1,\n",
    "        \"max_features\" : \"sqrt\",\n",
    "    }\n",
    "]\n",
    "\n",
    "m9_hp_table = pd.DataFrame(m9_hp_table)\n",
    "m9_hp_table.set_index('hp_no', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 10. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m10_hp_table = [\n",
    "    {\n",
    "        \"hp_no\": \"hp1\",\n",
    "        \"seed\": 1, \n",
    "        \"n_estimators\" : 100,\n",
    "        \"max_depth\" : 3,\n",
    "        \"min_samples_split\" : 2,\n",
    "        \"min_samples_leaf\" : 1\n",
    "    },\n",
    "    {\n",
    "        \"hp_no\": \"hp2\",\n",
    "        \"seed\": 1, \n",
    "        \"n_estimators\" : 50,\n",
    "        \"max_depth\" : 3,\n",
    "        \"min_samples_split\" : 2,\n",
    "        \"min_samples_leaf\" : 1\n",
    "    }\n",
    "]\n",
    "\n",
    "m10_hp_table = pd.DataFrame(m10_hp_table)\n",
    "m10_hp_table.set_index('hp_no', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 11. SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m11_hp_table = [\n",
    "    {\n",
    "        \"hp_no\": \"hp1\", #this is only for testing, with epochs = 2 just to ensure code is working.\n",
    "        \"seed\": 1,              # Seed for reproducibility\n",
    "        \"kernel\": 'rbf', # kernel\n",
    "        \"C\": 100, # regularization strength, higher = less regularization\n",
    "     \"gamma\": 0.001, # kernel coefficient, higher = more complex\n",
    "        \"epsilon\": 0.3 #margin of tolerance \n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "m11_hp_table = pd.DataFrame(m11_hp_table)\n",
    "m11_hp_table.set_index('hp_no', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 12. RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m12_hp_table = [\n",
    "    {\n",
    "        \"hp_no\": \"hp1\", #this is only for testing, with epochs = 2 just to ensure code is working.\n",
    "        \"seed\": 1,              # Seed for reproducibility\n",
    "        \"input_size\": 1,        # One feature per timestep (e.g., lag)\n",
    "        \"hidden_size\": 64,      # RNN units\n",
    "        \"num_layers\": 1,        # RNN layers\n",
    "        \"output_size\": 1,       # One-step prediction\n",
    "        \"batch_size\": 4096,     # Training batch size\n",
    "        \"epochs\": 2,            # Training epochs\n",
    "        \"learning_rate\": 0.001  # Optimizer learning rate\n",
    "    },\n",
    "    {\n",
    "        \"hp_no\": \"hp2\",\n",
    "        \"seed\": 1,              # Seed for reproducibility\n",
    "        \"input_size\": 1,        # One feature per timestep (e.g., lag)\n",
    "        \"hidden_size\": 64,     # RNN units\n",
    "        \"num_layers\": 1,        # RNN layers\n",
    "        \"output_size\": 1,       # One-step prediction\n",
    "        \"batch_size\": 4096,     # Training batch size\n",
    "        \"epochs\": 100,            # Training epochs\n",
    "        \"learning_rate\": 0.001  # Optimizer learning rate\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "m12_hp_table = pd.DataFrame(m12_hp_table)\n",
    "m12_hp_table.set_index('hp_no', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 13. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m13_hp_table = [\n",
    "    {\n",
    "        \"hp_no\": \"hp1\", #this is only for testing, with epochs = 2 just to ensure code is working.\n",
    "        \"seed\": 1,              # Seed for reproducibility\n",
    "        \"input_size\": 1,        # One feature per timestep (e.g., lag)\n",
    "        \"hidden_size\": 64,      # LSTM units\n",
    "        \"num_layers\": 1,        # LSTM layers\n",
    "        \"output_size\": 1,       # One-step prediction\n",
    "        \"batch_size\": 4096,     # Training batch size\n",
    "        \"epochs\": 2,            # Training epochs\n",
    "        \"learning_rate\": 0.001  # Optimizer learning rate\n",
    "    },\n",
    "    {\n",
    "        \"hp_no\": \"hp2\",\n",
    "        \"seed\": 1,              # Seed for reproducibility\n",
    "        \"input_size\": 1,        # One feature per timestep (e.g., lag)\n",
    "        \"hidden_size\": 64,     # LSTM units\n",
    "        \"num_layers\": 1,        # LSTM layers\n",
    "        \"output_size\": 1,       # One-step prediction\n",
    "        \"batch_size\": 4096,     # Training batch size\n",
    "        \"epochs\": 100,            # Training epochs\n",
    "        \"learning_rate\": 0.001  # Optimizer learning rate\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "m13_hp_table = pd.DataFrame(m13_hp_table)\n",
    "m13_hp_table.set_index('hp_no', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 14. GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m14_hp_table = [\n",
    "    {\n",
    "        \"hp_no\": \"hp1\", #this is only for testing, with epochs = 2 just to ensure code is working.\n",
    "        \"seed\": 1,              # Seed for reproducibility\n",
    "        \"input_size\": 1,        # One feature per timestep (e.g., lag)\n",
    "        \"hidden_size\": 64,      # GRU units\n",
    "        \"num_layers\": 1,        # GRU layers\n",
    "        \"output_size\": 1,       # One-step prediction\n",
    "        \"batch_size\": 4096,     # Training batch size\n",
    "        \"epochs\": 2,            # Training epochs\n",
    "        \"learning_rate\": 0.001  # Optimizer learning rate\n",
    "    },\n",
    "    {\n",
    "        \"hp_no\": \"hp2\",\n",
    "        \"seed\": 1,              # Seed for reproducibility\n",
    "        \"input_size\": 1,        # One feature per timestep (e.g., lag)\n",
    "        \"hidden_size\": 64,     # GRU units\n",
    "        \"num_layers\": 1,        # GRU layers\n",
    "        \"output_size\": 1,       # One-step prediction\n",
    "        \"batch_size\": 4096,     # Training batch size\n",
    "        \"epochs\": 100,            # Training epochs\n",
    "        \"learning_rate\": 0.001  # Optimizer learning rate\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "m14_hp_table = pd.DataFrame(m14_hp_table)\n",
    "m14_hp_table.set_index('hp_no', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 15. TRANSFORMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m15_hp_table = [\n",
    "    {\n",
    "        \"hp_no\": \"hp1\", #this is only for testing, with epochs = 2 just to ensure code is working.\n",
    "        \"seed\": 1,              # Seed for reproducibility\n",
    "        \"input_size\": 1,        # One feature per timestep (e.g., lag)\n",
    "        \"hidden_size\": 64,      # transformer units\n",
    "        \"num_layers\": 1,        # transformer layers\n",
    "        \"output_size\": 1,       # One-step prediction\n",
    "        \"batch_size\": 4096,     # Training batch size\n",
    "        \"nhead\" : 4,           # Number of attention heads\n",
    "        \"epochs\": 2,            # Training epochs\n",
    "        \"learning_rate\": 0.001  # Optimizer learning rate\n",
    "    },\n",
    "    {\n",
    "        \"hp_no\": \"hp2\",\n",
    "        \"seed\": 1,              # Seed for reproducibility\n",
    "        \"input_size\": 1,        # One feature per timestep (e.g., lag)\n",
    "        \"hidden_size\": 64,     # transformer units\n",
    "        \"num_layers\": 1,        # transformer layers\n",
    "        \"output_size\": 1,       # One-step prediction\n",
    "        \"batch_size\": 4096,     # Training batch size\n",
    "        \"nhead\" : 4,           # Number of attention heads\n",
    "        \"epochs\": 100,            # Training epochs\n",
    "        \"learning_rate\": 0.001  # Optimizer learning rate\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "m15_hp_table = pd.DataFrame(m15_hp_table)\n",
    "m15_hp_table.set_index('hp_no', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 16. PROPHET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m16_hp_table = [\n",
    "    {\n",
    "        \"hp_no\": \"hp1\",\n",
    "        \"seed\": 1,                      # Seed for reproducibility\n",
    "        \"seasonality_prior_scale\": 10,           # Strength of seasonality (higher = stronger seasonality)\n",
    "        \"seasonality_mode\": \"additive\",         # Type of seasonality: 'additive' or 'multiplicative'\n",
    "        \"weekly_seasonality\": True,             # Enable weekly seasonality\n",
    "        \"daily_seasonality\": True,              # Enable daily seasonality\n",
    "        \"growth\": \"linear\"                      # Model type: 'linear' or 'logistic'\n",
    "    }\n",
    "]\n",
    "\n",
    "m16_hp_table = pd.DataFrame(m16_hp_table)\n",
    "m16_hp_table.set_index('hp_no', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 17. XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m17_hp_table = [\n",
    "    {\n",
    "        \"hp_no\": \"hp1\",\n",
    "        'xgb_seed': 1,             # Seed for reproducibility\n",
    "        'n_estimators': 200,          # Number of trees\n",
    "        'learning_rate': 0.1,         # Step size for gradient descent\n",
    "        'max_depth': 6,               # Maximum depth of each tree\n",
    "        'subsample': 0.8,             # Fraction of training data used for each tree\n",
    "        'colsample_bytree': 0.8       # Fraction of features used for each tree\n",
    "        # this seems to result in overfittting\n",
    "    },\n",
    "    {\n",
    "        \"hp_no\": \"hp2\",\n",
    "        'xgb_seed': 1,             # Seed for reproducibility\n",
    "        'n_estimators': 100,          # Number of trees\n",
    "        'learning_rate': 0.1,         # Step size for gradient descent\n",
    "        'max_depth': 3,               # Maximum depth of each tree\n",
    "        'subsample': 0.8,             # Fraction of training data used for each tree\n",
    "        'colsample_bytree': 0.8       # Fraction of features used for each tree\n",
    "    }\n",
    "]\n",
    "\n",
    "m17_hp_table = pd.DataFrame(m17_hp_table)\n",
    "m17_hp_table.set_index('hp_no', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 18. NBEATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m18_hp_table = [\n",
    "    {\n",
    "        \"hp_no\": \"hp1\", #this is only for testing, with epochs = 2 just to ensure code is working.\n",
    "        \"seed\": 1,              # Seed for reproducibility\n",
    "        \"hidden_size\": 64,      # NBeats units\n",
    "        \"num_layers\": 1,        # NBeats layers\n",
    "        \"num_blocks\": 3,        # number of blocks in N-BEATS\n",
    "        \"output_size\": 1,       # One-step prediction\n",
    "        \"epochs\": 2,            # Training epochs\n",
    "        \"lr\": 0.001  # Optimizer learning rate\n",
    "    },\n",
    "    {\n",
    "        \"hp_no\": \"hp2\",\n",
    "        \"seed\": 1,              # Seed for reproducibility\n",
    "        \"hidden_size\": 64,     # NBeats units\n",
    "        \"num_layers\": 1,        # NBeats layers\n",
    "        \"num_blocks\": 3,        # number of blocks in N-BEATS\n",
    "        \"output_size\": 1,       # One-step prediction\n",
    "        \"epochs\": 100,            # Training epochs\n",
    "        \"lr\": 0.001  # Optimizer learning rate\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "m18_hp_table = pd.DataFrame(m18_hp_table)\n",
    "m18_hp_table.set_index('hp_no', inplace=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
